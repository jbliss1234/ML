{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define common functions\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)    \n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).    \n",
    "def encode_text_index(df,name): \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "                \n",
    "# Encode a numeric column as zscores    \n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "        \n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "        \n",
    "    df[name] = (df[name]-mean)/sd\n",
    "    \n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    print(target_type)\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "# setup exponential decay function\n",
    "def exp_decay(global_step):\n",
    "    return tf.train.exponential_decay(\n",
    "        learning_rate=0.01, global_step=global_step,\n",
    "        decay_steps=100, decay_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read Input CSV file\n",
    "path = \"./data/\"\n",
    "inputFilePath = os.path.join(path, \"oronite.csv\")\n",
    "#df = pd.read_csv(inputFilePath, compression=\"gzip\", header=0, na_values=['NULL'])\n",
    "df = pd.read_csv(inputFilePath, header=0, na_values=['NULL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffOR-F Code' 'D' 'PD' 'B' 'MD' 'F' 'FM' 'ZN' 'DE' 'IN' 'O' 'VI' 'DI'\n",
      " 'LAB_CODE' 'Result_Code' 'TEST_RESULT_VALUE']\n"
     ]
    }
   ],
   "source": [
    "#show headers\n",
    "#headers = list(df.columns.values)\n",
    "#print(headers)\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort dataset\n",
    "#df.sort_values(by=\"SortKey\",ascending=True)\n",
    "#shuffle dataset. Unnecessary in this case because already sorted by guid\n",
    "np.random.seed(42)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('\\ufeffOR-F Code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encode result_code and lab_code as numbers\n",
    "encode_text_dummy(df, 'LAB_CODE')\n",
    "encode_text_dummy(df, 'Result_Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>PD</th>\n",
       "      <th>B</th>\n",
       "      <th>MD</th>\n",
       "      <th>F</th>\n",
       "      <th>FM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>DE</th>\n",
       "      <th>IN</th>\n",
       "      <th>O</th>\n",
       "      <th>VI</th>\n",
       "      <th>DI</th>\n",
       "      <th>TEST_RESULT_VALUE</th>\n",
       "      <th>LAB_CODE-AL</th>\n",
       "      <th>LAB_CODE-EG</th>\n",
       "      <th>LAB_CODE-LT</th>\n",
       "      <th>LAB_CODE-SR</th>\n",
       "      <th>Result_Code-DP1--WDN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>80.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.721</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.580</td>\n",
       "      <td>223.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>87.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.457</td>\n",
       "      <td>4.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.670</td>\n",
       "      <td>445.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.510</td>\n",
       "      <td>0.20</td>\n",
       "      <td>77.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.109</td>\n",
       "      <td>3.442</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>4.068</td>\n",
       "      <td>314.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>82.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.227</td>\n",
       "      <td>4.303</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.676</td>\n",
       "      <td>214.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>84.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.251</td>\n",
       "      <td>3.072</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.20</td>\n",
       "      <td>4.304</td>\n",
       "      <td>215.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.569</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.20</td>\n",
       "      <td>5.000</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.094</td>\n",
       "      <td>5.849</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.329</td>\n",
       "      <td>252.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>83.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.249</td>\n",
       "      <td>3.088</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.925</td>\n",
       "      <td>210.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.094</td>\n",
       "      <td>5.849</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.329</td>\n",
       "      <td>321.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>84.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2.673</td>\n",
       "      <td>477.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>75.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.512</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>79.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.358</td>\n",
       "      <td>5.096</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.800</td>\n",
       "      <td>263.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>78.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.269</td>\n",
       "      <td>3.337</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>7.800</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>4.230</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.879</td>\n",
       "      <td>300.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>76.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.295</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.000</td>\n",
       "      <td>213.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.680</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>8.000</td>\n",
       "      <td>237.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>84.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2.673</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.044</td>\n",
       "      <td>4.150</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.233</td>\n",
       "      <td>181.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.061</td>\n",
       "      <td>5.471</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.20</td>\n",
       "      <td>6.233</td>\n",
       "      <td>207.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.30</td>\n",
       "      <td>79.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386</td>\n",
       "      <td>3.079</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.80</td>\n",
       "      <td>7.325</td>\n",
       "      <td>278.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>4.722</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>7.684</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>75.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.512</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>220.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>72.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.851</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.10</td>\n",
       "      <td>8.865</td>\n",
       "      <td>284.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.569</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.60</td>\n",
       "      <td>8.000</td>\n",
       "      <td>187.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.680</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>8.000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>80.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.751</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>8.500</td>\n",
       "      <td>158.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>74.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.104</td>\n",
       "      <td>5.878</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.20</td>\n",
       "      <td>6.408</td>\n",
       "      <td>187.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.851</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.90</td>\n",
       "      <td>8.300</td>\n",
       "      <td>201.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.104</td>\n",
       "      <td>5.979</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>7.000</td>\n",
       "      <td>236.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>79.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.000</td>\n",
       "      <td>197.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>78.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.398</td>\n",
       "      <td>3.446</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.30</td>\n",
       "      <td>7.879</td>\n",
       "      <td>235.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.549</td>\n",
       "      <td>4.512</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>7.000</td>\n",
       "      <td>219.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.662</td>\n",
       "      <td>0.20</td>\n",
       "      <td>79.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.356</td>\n",
       "      <td>4.595</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.90</td>\n",
       "      <td>4.535</td>\n",
       "      <td>321.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>83.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.249</td>\n",
       "      <td>3.088</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.925</td>\n",
       "      <td>221.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>78.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.721</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>9.580</td>\n",
       "      <td>183.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.585</td>\n",
       "      <td>4.205</td>\n",
       "      <td>1.138</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>9.555</td>\n",
       "      <td>227.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>83.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.249</td>\n",
       "      <td>3.102</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4.925</td>\n",
       "      <td>234.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>4.770</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.30</td>\n",
       "      <td>7.880</td>\n",
       "      <td>275.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>75.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.512</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.00</td>\n",
       "      <td>10.000</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.585</td>\n",
       "      <td>4.205</td>\n",
       "      <td>1.138</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>9.555</td>\n",
       "      <td>202.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>79.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.721</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.30</td>\n",
       "      <td>9.580</td>\n",
       "      <td>231.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.510</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.109</td>\n",
       "      <td>3.442</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>4.068</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.30</td>\n",
       "      <td>79.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>3.034</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.373</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>76.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.545</td>\n",
       "      <td>3.114</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.90</td>\n",
       "      <td>5.300</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.648</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.573</td>\n",
       "      <td>3.446</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.832</td>\n",
       "      <td>276.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>83.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.251</td>\n",
       "      <td>3.126</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.974</td>\n",
       "      <td>182.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.510</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.109</td>\n",
       "      <td>3.442</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>4.068</td>\n",
       "      <td>239.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.094</td>\n",
       "      <td>5.849</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.329</td>\n",
       "      <td>240.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.082</td>\n",
       "      <td>5.829</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.233</td>\n",
       "      <td>199.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.704</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463</td>\n",
       "      <td>4.089</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>7.500</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.915</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>8.000</td>\n",
       "      <td>264.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>75.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.851</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>9.050</td>\n",
       "      <td>244.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>78.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.549</td>\n",
       "      <td>4.346</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.20</td>\n",
       "      <td>4.960</td>\n",
       "      <td>341.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>2.721</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.90</td>\n",
       "      <td>9.250</td>\n",
       "      <td>215.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>76.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.379</td>\n",
       "      <td>4.312</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.10</td>\n",
       "      <td>9.967</td>\n",
       "      <td>214.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>81.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.358</td>\n",
       "      <td>5.096</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.10</td>\n",
       "      <td>4.830</td>\n",
       "      <td>242.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>78.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>4.852</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>4.500</td>\n",
       "      <td>245.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>81.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250</td>\n",
       "      <td>4.289</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.771</td>\n",
       "      <td>222.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>74.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.696</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.80</td>\n",
       "      <td>9.000</td>\n",
       "      <td>193.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.082</td>\n",
       "      <td>5.829</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6.233</td>\n",
       "      <td>230.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         D    PD      B   MD      F   FM     ZN     DE     IN     O     VI  \\\n",
       "0    0.000  0.30  80.13  0.0  0.025  0.0  1.377  2.721  1.150  0.00   4.00   \n",
       "1    0.000  0.10  87.02  0.0  0.023  0.0  1.457  4.087  0.000  0.00   0.00   \n",
       "2    2.510  0.20  77.31  0.0  0.020  0.0  1.109  3.442  0.214  0.00  12.50   \n",
       "3    0.000  0.30  82.10  0.0  0.010  0.0  1.227  4.303  0.170  0.00   4.90   \n",
       "4    0.000  0.20  84.75  0.0  0.025  0.0  1.251  3.072  0.202  0.00   6.20   \n",
       "5    0.000  0.00  77.13  0.0  0.025  0.0  1.377  3.569  0.700  0.00  12.20   \n",
       "6    0.000  0.00  74.80  0.0  0.020  0.0  2.094  5.849  0.150  0.00   9.00   \n",
       "7    0.000  0.20  83.20  0.0  0.025  0.0  1.249  3.088  0.199  0.00   7.12   \n",
       "8    0.000  0.00  74.80  0.0  0.020  0.0  2.094  5.849  0.150  0.00   9.00   \n",
       "9    0.000  0.20  84.04  0.0  0.025  0.0  0.697  1.980  0.100  0.00  10.00   \n",
       "10   0.000  0.30  75.45  0.0  0.020  0.0  1.377  4.512  0.450  0.03   7.00   \n",
       "11   0.000  0.15  79.84  0.0  0.050  0.0  1.358  5.096  0.200  0.00   6.50   \n",
       "12   0.000  0.20  78.77  0.0  0.025  0.0  1.269  3.337  0.200  0.00   8.40   \n",
       "13   0.000  0.00  80.18  0.0  0.025  0.0  1.485  4.230  0.304  0.00   5.90   \n",
       "14   0.000  0.00  76.92  0.0  0.025  0.0  1.377  4.295  0.200  0.00   9.50   \n",
       "15   0.000  0.00  78.64  0.0  0.025  0.0  1.377  3.680  0.200  0.00   7.50   \n",
       "16   0.000  0.20  84.04  0.0  0.025  0.0  0.697  1.980  0.100  0.00  10.00   \n",
       "17   0.000  0.00  78.53  0.0  0.010  0.0  1.044  4.150  0.202  0.00   8.30   \n",
       "18   0.809  0.00  75.10  0.0  0.020  0.0  2.061  5.471  0.154  0.00   9.20   \n",
       "19   0.590  0.30  79.61  0.0  0.050  0.0  1.386  3.079  0.862  0.00   6.80   \n",
       "20   0.780  0.00  78.99  0.0  0.025  0.0  1.485  4.722  0.813  0.00   5.50   \n",
       "21   0.000  0.30  75.45  0.0  0.020  0.0  1.377  4.512  0.450  0.03   7.00   \n",
       "22   0.000  0.15  72.79  0.0  0.025  0.0  1.377  3.851  0.950  0.00  11.10   \n",
       "23   1.200  0.00  77.53  0.0  0.025  0.0  1.377  3.569  0.700  0.00   7.60   \n",
       "24   0.000  0.00  78.64  0.0  0.025  0.0  1.377  3.680  0.200  0.00   7.50   \n",
       "25   0.000  0.30  80.04  0.0  0.025  0.0  1.377  2.751  0.800  0.00   5.50   \n",
       "26   0.000  0.20  74.40  0.0  0.020  0.0  2.104  5.878  0.150  0.00   9.20   \n",
       "27   0.000  0.00  77.86  0.0  0.050  0.0  1.377  3.851  0.950  0.00   6.90   \n",
       "28   0.000  0.00  55.80  0.0  0.020  0.0  2.104  5.979  0.153  0.00  28.00   \n",
       "29   0.000  0.20  79.25  0.0  0.000  0.0  0.000  0.000  0.000  0.00   8.95   \n",
       "..     ...   ...    ...  ...    ...  ...    ...    ...    ...   ...    ...   \n",
       "132  0.000  0.10  78.16  0.0  0.025  0.0  1.398  3.446  0.699  0.00   8.30   \n",
       "133  0.000  0.00  78.67  0.0  0.025  0.0  1.549  4.512  1.000  0.00   6.50   \n",
       "134  0.662  0.20  79.55  0.0  0.010  0.0  1.356  4.595  0.201  0.00   8.90   \n",
       "135  0.000  0.20  83.20  0.0  0.025  0.0  1.249  3.088  0.199  0.00   7.12   \n",
       "136  0.000  0.30  78.92  0.0  0.025  0.0  1.377  2.721  1.150  0.00   5.20   \n",
       "137  0.000  0.00  78.75  0.0  0.057  0.0  1.585  4.205  1.138  0.00   4.70   \n",
       "138  0.000  0.20  83.18  0.0  0.025  0.0  1.249  3.102  0.199  0.00   7.12   \n",
       "139  0.790  0.00  79.33  0.0  0.020  0.0  1.400  4.770  1.000  0.00   5.30   \n",
       "140  0.000  0.30  75.45  0.0  0.020  0.0  1.377  4.512  0.450  0.03   7.00   \n",
       "141  0.000  0.00  79.06  0.0  0.057  0.0  1.585  4.205  1.138  0.00   4.40   \n",
       "142  0.000  0.30  79.82  0.0  0.025  0.0  1.377  2.721  1.150  0.00   4.30   \n",
       "143  0.510  0.00  81.20  0.0  0.020  0.0  1.109  3.442  0.214  0.00   8.80   \n",
       "144  0.830  0.30  79.07  0.0  0.050  0.0  1.485  3.034  0.861  0.00   6.00   \n",
       "145  0.000  0.30  76.13  0.0  0.025  0.0  1.545  3.114  0.150  0.00  12.90   \n",
       "146  0.648  0.00  81.78  0.0  0.025  0.0  1.573  3.446  0.699  0.00   5.00   \n",
       "147  0.000  0.20  83.72  0.0  0.025  0.0  1.251  3.126  0.202  0.00   6.50   \n",
       "148  0.510  0.00  81.20  0.0  0.020  0.0  1.109  3.442  0.214  0.00   8.80   \n",
       "149  0.944  0.00  75.00  0.0  0.020  0.0  2.094  5.849  0.150  0.00   8.80   \n",
       "150  0.000  0.00  74.80  0.0  0.020  0.0  2.082  5.829  0.142  0.00   9.00   \n",
       "151  0.704  0.00  81.32  0.0  0.025  0.0  1.463  4.089  0.200  0.00   4.70   \n",
       "152  0.000  0.00  78.90  0.0  0.025  0.0  1.377  4.915  0.500  0.00   5.50   \n",
       "153  0.000  0.10  75.51  0.0  0.025  0.0  1.377  3.851  0.950  0.00   8.40   \n",
       "154  0.000  0.20  78.15  0.0  0.025  0.0  1.549  4.346  0.000  0.00  10.20   \n",
       "155  0.000  0.50  69.63  0.0  0.025  0.0  1.377  2.721  2.750  0.00  12.90   \n",
       "156  0.000  0.30  76.52  0.0  0.020  0.0  1.379  4.312  0.402  0.00   7.10   \n",
       "157  0.000  0.15  81.22  0.0  0.050  0.0  1.358  5.096  0.200  0.00   7.10   \n",
       "158  0.000  0.20  78.90  0.0  0.010  0.0  1.377  4.852  0.200  0.00   9.30   \n",
       "159  0.000  0.30  81.10  0.0  0.010  0.0  1.250  4.289  0.181  0.00   5.90   \n",
       "160  0.000  0.20  74.69  0.0  0.025  0.0  1.377  3.696  0.450  0.00   9.80   \n",
       "161  0.944  0.00  75.00  0.0  0.020  0.0  2.082  5.829  0.142  0.00   8.80   \n",
       "\n",
       "         DI  TEST_RESULT_VALUE  LAB_CODE-AL  LAB_CODE-EG  LAB_CODE-LT  \\\n",
       "0     9.580              223.2          0.0          0.0          0.0   \n",
       "1     6.670              445.7          0.0          0.0          1.0   \n",
       "2     4.068              314.1          0.0          0.0          0.0   \n",
       "3     5.676              214.3          0.0          0.0          1.0   \n",
       "4     4.304              215.3          0.0          0.0          1.0   \n",
       "5     5.000              207.0          0.0          1.0          0.0   \n",
       "6     6.329              252.6          0.0          0.0          0.0   \n",
       "7     4.925              210.5          0.0          0.0          1.0   \n",
       "8     6.329              321.1          0.0          0.0          0.0   \n",
       "9     2.673              477.6          0.0          1.0          0.0   \n",
       "10   10.000              250.0          0.0          0.0          1.0   \n",
       "11    6.800              263.1          0.0          0.0          1.0   \n",
       "12    7.800              200.0          0.0          0.0          1.0   \n",
       "13    7.879              300.2          0.0          0.0          1.0   \n",
       "14    7.000              213.3          0.0          0.0          1.0   \n",
       "15    8.000              237.6          0.0          0.0          1.0   \n",
       "16    2.673              432.0          0.0          1.0          0.0   \n",
       "17    6.233              181.3          0.0          0.0          1.0   \n",
       "18    6.233              207.1          0.0          0.0          1.0   \n",
       "19    7.325              278.7          0.0          0.0          1.0   \n",
       "20    7.684              259.0          0.0          0.0          1.0   \n",
       "21   10.000              220.9          0.0          0.0          1.0   \n",
       "22    8.865              284.7          0.0          0.0          1.0   \n",
       "23    8.000              187.8          0.0          0.0          1.0   \n",
       "24    8.000              228.0          0.0          0.0          1.0   \n",
       "25    8.500              158.3          0.0          0.0          0.0   \n",
       "26    6.408              187.2          0.0          0.0          1.0   \n",
       "27    8.300              201.5          0.0          0.0          1.0   \n",
       "28    7.000              236.3          0.0          1.0          0.0   \n",
       "29    0.000              197.7          0.0          0.0          1.0   \n",
       "..      ...                ...          ...          ...          ...   \n",
       "132   7.879              235.6          0.0          0.0          1.0   \n",
       "133   7.000              219.7          0.0          0.0          1.0   \n",
       "134   4.535              321.1          0.0          0.0          0.0   \n",
       "135   4.925              221.9          0.0          0.0          1.0   \n",
       "136   9.580              183.9          0.0          0.0          0.0   \n",
       "137   9.555              227.8          0.0          0.0          1.0   \n",
       "138   4.925              234.7          0.0          0.0          1.0   \n",
       "139   7.880              275.8          0.0          0.0          1.0   \n",
       "140  10.000              206.0          0.0          0.0          1.0   \n",
       "141   9.555              202.9          0.0          0.0          1.0   \n",
       "142   9.580              231.5          0.0          0.0          0.0   \n",
       "143   4.068              340.0          0.0          0.0          0.0   \n",
       "144   8.373              324.0          0.0          0.0          0.0   \n",
       "145   5.300              432.7          0.0          1.0          0.0   \n",
       "146   6.832              276.8          0.0          0.0          1.0   \n",
       "147   4.974              182.6          0.0          0.0          1.0   \n",
       "148   4.068              239.2          0.0          0.0          0.0   \n",
       "149   6.329              240.1          0.0          0.0          1.0   \n",
       "150   6.233              199.9          0.0          0.0          1.0   \n",
       "151   7.500              198.0          0.0          0.0          1.0   \n",
       "152   8.000              264.7          0.0          0.0          1.0   \n",
       "153   9.050              244.7          0.0          0.0          1.0   \n",
       "154   4.960              341.7          0.0          0.0          1.0   \n",
       "155   9.250              215.3          0.0          0.0          0.0   \n",
       "156   9.967              214.5          0.0          0.0          1.0   \n",
       "157   4.830              242.3          0.0          0.0          1.0   \n",
       "158   4.500              245.5          0.0          0.0          1.0   \n",
       "159   5.771              222.6          0.0          0.0          1.0   \n",
       "160   9.000              193.9          0.0          0.0          1.0   \n",
       "161   6.233              230.7          0.0          0.0          1.0   \n",
       "\n",
       "     LAB_CODE-SR  Result_Code-DP1--WDN  \n",
       "0            1.0                   1.0  \n",
       "1            0.0                   1.0  \n",
       "2            1.0                   1.0  \n",
       "3            0.0                   1.0  \n",
       "4            0.0                   1.0  \n",
       "5            0.0                   1.0  \n",
       "6            1.0                   1.0  \n",
       "7            0.0                   1.0  \n",
       "8            1.0                   1.0  \n",
       "9            0.0                   1.0  \n",
       "10           0.0                   1.0  \n",
       "11           0.0                   1.0  \n",
       "12           0.0                   1.0  \n",
       "13           0.0                   1.0  \n",
       "14           0.0                   1.0  \n",
       "15           0.0                   1.0  \n",
       "16           0.0                   1.0  \n",
       "17           0.0                   1.0  \n",
       "18           0.0                   1.0  \n",
       "19           0.0                   1.0  \n",
       "20           0.0                   1.0  \n",
       "21           0.0                   1.0  \n",
       "22           0.0                   1.0  \n",
       "23           0.0                   1.0  \n",
       "24           0.0                   1.0  \n",
       "25           1.0                   1.0  \n",
       "26           0.0                   1.0  \n",
       "27           0.0                   1.0  \n",
       "28           0.0                   1.0  \n",
       "29           0.0                   1.0  \n",
       "..           ...                   ...  \n",
       "132          0.0                   1.0  \n",
       "133          0.0                   1.0  \n",
       "134          1.0                   1.0  \n",
       "135          0.0                   1.0  \n",
       "136          1.0                   1.0  \n",
       "137          0.0                   1.0  \n",
       "138          0.0                   1.0  \n",
       "139          0.0                   1.0  \n",
       "140          0.0                   1.0  \n",
       "141          0.0                   1.0  \n",
       "142          1.0                   1.0  \n",
       "143          1.0                   1.0  \n",
       "144          1.0                   1.0  \n",
       "145          0.0                   1.0  \n",
       "146          0.0                   1.0  \n",
       "147          0.0                   1.0  \n",
       "148          1.0                   1.0  \n",
       "149          0.0                   1.0  \n",
       "150          0.0                   1.0  \n",
       "151          0.0                   1.0  \n",
       "152          0.0                   1.0  \n",
       "153          0.0                   1.0  \n",
       "154          0.0                   1.0  \n",
       "155          1.0                   1.0  \n",
       "156          0.0                   1.0  \n",
       "157          0.0                   1.0  \n",
       "158          0.0                   1.0  \n",
       "159          0.0                   1.0  \n",
       "160          0.0                   1.0  \n",
       "161          0.0                   1.0  \n",
       "\n",
       "[162 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D' 'PD' 'B' 'MD' 'F' 'FM' 'ZN' 'DE' 'IN' 'O' 'VI' 'DI'\n",
      " 'TEST_RESULT_VALUE' 'LAB_CODE-AL' 'LAB_CODE-EG' 'LAB_CODE-LT'\n",
      " 'LAB_CODE-SR' 'Result_Code-DP1--WDN']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encode all numeric values to zscored values\n",
    "#encode_numeric_zscore(df, 'AW')\n",
    "encode_numeric_zscore(df, 'D')\n",
    "encode_numeric_zscore(df, 'PD')\n",
    "encode_numeric_zscore(df, 'B')\n",
    "encode_numeric_zscore(df, 'MD')\n",
    "encode_numeric_zscore(df, 'F')\n",
    "encode_numeric_zscore(df, 'FM')\n",
    "encode_numeric_zscore(df, 'ZN')\n",
    "encode_numeric_zscore(df, 'DE')\n",
    "encode_numeric_zscore(df, 'IN')\n",
    "encode_numeric_zscore(df, 'O')\n",
    "encode_numeric_zscore(df, 'VI')\n",
    "encode_numeric_zscore(df, 'DI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>PD</th>\n",
       "      <th>B</th>\n",
       "      <th>MD</th>\n",
       "      <th>F</th>\n",
       "      <th>FM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>DE</th>\n",
       "      <th>IN</th>\n",
       "      <th>O</th>\n",
       "      <th>VI</th>\n",
       "      <th>DI</th>\n",
       "      <th>TEST_RESULT_VALUE</th>\n",
       "      <th>LAB_CODE-AL</th>\n",
       "      <th>LAB_CODE-EG</th>\n",
       "      <th>LAB_CODE-LT</th>\n",
       "      <th>LAB_CODE-SR</th>\n",
       "      <th>Result_Code-DP1--WDN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.166940</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-1.151937</td>\n",
       "      <td>1.474092</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.987497</td>\n",
       "      <td>1.455932</td>\n",
       "      <td>223.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-0.287943</td>\n",
       "      <td>1.604487</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.414869</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.253734</td>\n",
       "      <td>0.305958</td>\n",
       "      <td>-1.152751</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-2.077384</td>\n",
       "      <td>0.053231</td>\n",
       "      <td>445.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.530478</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.421432</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.981330</td>\n",
       "      <td>-0.382433</td>\n",
       "      <td>-0.663930</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>1.328513</td>\n",
       "      <td>-1.201005</td>\n",
       "      <td>314.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.577965</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-1.552516</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.562544</td>\n",
       "      <td>0.536489</td>\n",
       "      <td>-0.764435</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.742272</td>\n",
       "      <td>-0.425904</td>\n",
       "      <td>214.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>1.130868</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.477367</td>\n",
       "      <td>-0.777324</td>\n",
       "      <td>-0.691340</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.388059</td>\n",
       "      <td>-1.087246</td>\n",
       "      <td>215.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.458988</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.246889</td>\n",
       "      <td>0.446197</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>1.246772</td>\n",
       "      <td>-0.751755</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.945125</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.514468</td>\n",
       "      <td>2.186493</td>\n",
       "      <td>-0.810119</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.374862</td>\n",
       "      <td>-0.111140</td>\n",
       "      <td>252.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.807472</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.484465</td>\n",
       "      <td>-0.760248</td>\n",
       "      <td>-0.698193</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.137385</td>\n",
       "      <td>-0.787907</td>\n",
       "      <td>210.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.945125</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.514468</td>\n",
       "      <td>2.186493</td>\n",
       "      <td>-0.810119</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.374862</td>\n",
       "      <td>-0.111140</td>\n",
       "      <td>321.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.982732</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-2.443532</td>\n",
       "      <td>-1.942786</td>\n",
       "      <td>-0.924330</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.647334</td>\n",
       "      <td>-1.873433</td>\n",
       "      <td>477.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.809507</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>-0.124856</td>\n",
       "      <td>5.586249</td>\n",
       "      <td>-0.170081</td>\n",
       "      <td>1.658384</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.095665</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>1.947938</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.097620</td>\n",
       "      <td>1.382837</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.306317</td>\n",
       "      <td>0.115895</td>\n",
       "      <td>263.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.116814</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.413484</td>\n",
       "      <td>-0.494497</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.211379</td>\n",
       "      <td>0.597923</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.177372</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.353107</td>\n",
       "      <td>0.458578</td>\n",
       "      <td>-0.458351</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.469800</td>\n",
       "      <td>0.636003</td>\n",
       "      <td>300.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.502803</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.527951</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.511098</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>213.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.143938</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.128422</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.033845</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>237.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.982732</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-2.443532</td>\n",
       "      <td>-1.942786</td>\n",
       "      <td>-0.924330</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.647334</td>\n",
       "      <td>-1.873433</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.166888</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-1.552516</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-1.212017</td>\n",
       "      <td>0.373196</td>\n",
       "      <td>-0.691340</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.184132</td>\n",
       "      <td>-0.157415</td>\n",
       "      <td>181.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.754940</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.882532</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.397350</td>\n",
       "      <td>1.783064</td>\n",
       "      <td>-0.800983</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>-0.157415</td>\n",
       "      <td>207.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.140100</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.058446</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>1.947938</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.769853</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.224576</td>\n",
       "      <td>0.368959</td>\n",
       "      <td>278.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.673523</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.070913</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.353107</td>\n",
       "      <td>0.983677</td>\n",
       "      <td>0.704313</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.578789</td>\n",
       "      <td>0.542007</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.809507</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>-0.124856</td>\n",
       "      <td>5.586249</td>\n",
       "      <td>-0.170081</td>\n",
       "      <td>1.658384</td>\n",
       "      <td>220.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.095665</td>\n",
       "      <td>-1.364496</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.054082</td>\n",
       "      <td>1.017249</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.947053</td>\n",
       "      <td>1.111282</td>\n",
       "      <td>284.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.852668</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.375531</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.246889</td>\n",
       "      <td>0.446197</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.006598</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>187.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.143938</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.128422</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.033845</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.148162</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-1.119919</td>\n",
       "      <td>0.674618</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.578789</td>\n",
       "      <td>0.935342</td>\n",
       "      <td>158.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-1.028582</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.549959</td>\n",
       "      <td>2.217444</td>\n",
       "      <td>-0.810119</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>-0.073060</td>\n",
       "      <td>187.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.306679</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>1.947938</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.054082</td>\n",
       "      <td>1.017249</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.197328</td>\n",
       "      <td>0.838936</td>\n",
       "      <td>201.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-4.909332</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.549959</td>\n",
       "      <td>2.325239</td>\n",
       "      <td>-0.803267</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>5.551826</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>236.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.016666</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-2.427630</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-4.917208</td>\n",
       "      <td>-4.055987</td>\n",
       "      <td>-1.152751</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.361239</td>\n",
       "      <td>-3.161893</td>\n",
       "      <td>197.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-0.287943</td>\n",
       "      <td>-0.244086</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.044341</td>\n",
       "      <td>-0.378164</td>\n",
       "      <td>0.443912</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.184132</td>\n",
       "      <td>0.636003</td>\n",
       "      <td>235.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.137678</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.580245</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>1.131460</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.306317</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>219.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.342239</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-1.552516</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.104718</td>\n",
       "      <td>0.848133</td>\n",
       "      <td>-0.693625</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.347615</td>\n",
       "      <td>-0.975898</td>\n",
       "      <td>321.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.807472</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.484465</td>\n",
       "      <td>-0.760248</td>\n",
       "      <td>-0.698193</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.137385</td>\n",
       "      <td>-0.787907</td>\n",
       "      <td>221.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.085518</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-1.151937</td>\n",
       "      <td>1.474092</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.660530</td>\n",
       "      <td>1.455932</td>\n",
       "      <td>183.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.120987</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>2.560518</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.708010</td>\n",
       "      <td>0.431896</td>\n",
       "      <td>1.446681</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.796766</td>\n",
       "      <td>1.443881</td>\n",
       "      <td>227.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.803299</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.484465</td>\n",
       "      <td>-0.745306</td>\n",
       "      <td>-0.698193</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.137385</td>\n",
       "      <td>-0.787907</td>\n",
       "      <td>234.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.701598</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.051439</td>\n",
       "      <td>1.034906</td>\n",
       "      <td>1.131460</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.633283</td>\n",
       "      <td>0.636485</td>\n",
       "      <td>275.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.809507</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>-0.124856</td>\n",
       "      <td>5.586249</td>\n",
       "      <td>-0.170081</td>\n",
       "      <td>1.658384</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.056308</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>2.560518</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.708010</td>\n",
       "      <td>0.431896</td>\n",
       "      <td>1.446681</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.878508</td>\n",
       "      <td>1.443881</td>\n",
       "      <td>202.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.102261</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-1.151937</td>\n",
       "      <td>1.474092</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.905755</td>\n",
       "      <td>1.455932</td>\n",
       "      <td>231.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.915501</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.390187</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.981330</td>\n",
       "      <td>-0.382433</td>\n",
       "      <td>-0.663930</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.320368</td>\n",
       "      <td>-1.201005</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.813897</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.054221</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>1.947938</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.353107</td>\n",
       "      <td>-0.817880</td>\n",
       "      <td>0.813955</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.442553</td>\n",
       "      <td>0.874124</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.667630</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.566049</td>\n",
       "      <td>-0.732498</td>\n",
       "      <td>-0.810119</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>1.437502</td>\n",
       "      <td>-0.607147</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.302935</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.665422</td>\n",
       "      <td>-0.378164</td>\n",
       "      <td>0.443912</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.715025</td>\n",
       "      <td>0.131320</td>\n",
       "      <td>276.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.915966</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.477367</td>\n",
       "      <td>-0.719691</td>\n",
       "      <td>-0.691340</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.306317</td>\n",
       "      <td>-0.764288</td>\n",
       "      <td>182.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.915501</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.390187</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.981330</td>\n",
       "      <td>-0.382433</td>\n",
       "      <td>-0.663930</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.320368</td>\n",
       "      <td>-1.201005</td>\n",
       "      <td>239.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2.133951</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.903396</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.514468</td>\n",
       "      <td>2.186493</td>\n",
       "      <td>-0.810119</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.320368</td>\n",
       "      <td>-0.111140</td>\n",
       "      <td>240.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.945125</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.471880</td>\n",
       "      <td>2.165148</td>\n",
       "      <td>-0.828393</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.374862</td>\n",
       "      <td>-0.157415</td>\n",
       "      <td>199.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.460154</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>0.415224</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.275028</td>\n",
       "      <td>0.308093</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.796766</td>\n",
       "      <td>0.453314</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.089691</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>1.189660</td>\n",
       "      <td>-0.010646</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.578789</td>\n",
       "      <td>0.694328</td>\n",
       "      <td>264.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>-0.287943</td>\n",
       "      <td>-0.796989</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>0.054082</td>\n",
       "      <td>1.017249</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.211379</td>\n",
       "      <td>1.200457</td>\n",
       "      <td>244.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.246172</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>0.580245</td>\n",
       "      <td>0.582382</td>\n",
       "      <td>-1.152751</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.701828</td>\n",
       "      <td>-0.771036</td>\n",
       "      <td>341.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>2.780920</td>\n",
       "      <td>-2.023807</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-1.151937</td>\n",
       "      <td>5.128829</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>1.437502</td>\n",
       "      <td>1.296863</td>\n",
       "      <td>215.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>-0.586260</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>0.546095</td>\n",
       "      <td>-0.234498</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.142834</td>\n",
       "      <td>1.642477</td>\n",
       "      <td>214.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.095665</td>\n",
       "      <td>0.394360</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>1.947938</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.097620</td>\n",
       "      <td>1.382837</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.142834</td>\n",
       "      <td>-0.833700</td>\n",
       "      <td>242.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.089691</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-1.552516</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>1.122422</td>\n",
       "      <td>-0.695909</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.456604</td>\n",
       "      <td>-0.992769</td>\n",
       "      <td>245.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>1.246489</td>\n",
       "      <td>0.369323</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-1.552516</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.480916</td>\n",
       "      <td>0.521547</td>\n",
       "      <td>-0.739309</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>-0.469800</td>\n",
       "      <td>-0.380112</td>\n",
       "      <td>222.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.516318</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>-0.968075</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.239846</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>-0.030189</td>\n",
       "      <td>-0.111346</td>\n",
       "      <td>-0.124856</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.592840</td>\n",
       "      <td>1.176356</td>\n",
       "      <td>193.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2.133951</td>\n",
       "      <td>-1.055158</td>\n",
       "      <td>-0.903396</td>\n",
       "      <td>-0.078567</td>\n",
       "      <td>-0.677403</td>\n",
       "      <td>-0.110968</td>\n",
       "      <td>2.471880</td>\n",
       "      <td>2.165148</td>\n",
       "      <td>-0.828393</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.320368</td>\n",
       "      <td>-0.157415</td>\n",
       "      <td>230.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D        PD         B        MD         F        FM        ZN  \\\n",
       "0   -0.516318  1.246489  0.166940 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "1   -0.516318 -0.287943  1.604487 -0.078567 -0.414869 -0.110968  0.253734   \n",
       "2    6.530478  0.479273 -0.421432 -0.078567 -0.677403 -0.110968 -0.981330   \n",
       "3   -0.516318  1.246489  0.577965 -0.078567 -1.552516 -0.110968 -0.562544   \n",
       "4   -0.516318  0.479273  1.130868 -0.078567 -0.239846 -0.110968 -0.477367   \n",
       "5   -0.516318 -1.055158 -0.458988 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "6   -0.516318 -1.055158 -0.945125 -0.078567 -0.677403 -0.110968  2.514468   \n",
       "7   -0.516318  0.479273  0.807472 -0.078567 -0.239846 -0.110968 -0.484465   \n",
       "8   -0.516318 -1.055158 -0.945125 -0.078567 -0.677403 -0.110968  2.514468   \n",
       "9   -0.516318  0.479273  0.982732 -0.078567 -0.239846 -0.110968 -2.443532   \n",
       "10  -0.516318  1.246489 -0.809507 -0.078567 -0.677403 -0.110968 -0.030189   \n",
       "11  -0.516318  0.095665  0.106433 -0.078567  1.947938 -0.110968 -0.097620   \n",
       "12  -0.516318  0.479273 -0.116814 -0.078567 -0.239846 -0.110968 -0.413484   \n",
       "13  -0.516318 -1.055158  0.177372 -0.078567 -0.239846 -0.110968  0.353107   \n",
       "14  -0.516318 -1.055158 -0.502803 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "15  -0.516318 -1.055158 -0.143938 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "16  -0.516318  0.479273  0.982732 -0.078567 -0.239846 -0.110968 -2.443532   \n",
       "17  -0.516318 -1.055158 -0.166888 -0.078567 -1.552516 -0.110968 -1.212017   \n",
       "18   1.754940 -1.055158 -0.882532 -0.078567 -0.677403 -0.110968  2.397350   \n",
       "19   1.140100  1.246489  0.058446 -0.078567  1.947938 -0.110968  0.001753   \n",
       "20   1.673523 -1.055158 -0.070913 -0.078567 -0.239846 -0.110968  0.353107   \n",
       "21  -0.516318  1.246489 -0.809507 -0.078567 -0.677403 -0.110968 -0.030189   \n",
       "22  -0.516318  0.095665 -1.364496 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "23   2.852668 -1.055158 -0.375531 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "24  -0.516318 -1.055158 -0.143938 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "25  -0.516318  1.246489  0.148162 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "26  -0.516318  0.479273 -1.028582 -0.078567 -0.677403 -0.110968  2.549959   \n",
       "27  -0.516318 -1.055158 -0.306679 -0.078567  1.947938 -0.110968 -0.030189   \n",
       "28  -0.516318 -1.055158 -4.909332 -0.078567 -0.677403 -0.110968  2.549959   \n",
       "29  -0.516318  0.479273 -0.016666 -0.078567 -2.427630 -0.110968 -4.917208   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "132 -0.516318 -0.287943 -0.244086 -0.078567 -0.239846 -0.110968  0.044341   \n",
       "133 -0.516318 -1.055158 -0.137678 -0.078567 -0.239846 -0.110968  0.580245   \n",
       "134  1.342239  0.479273  0.045927 -0.078567 -1.552516 -0.110968 -0.104718   \n",
       "135 -0.516318  0.479273  0.807472 -0.078567 -0.239846 -0.110968 -0.484465   \n",
       "136 -0.516318  1.246489 -0.085518 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "137 -0.516318 -1.055158 -0.120987 -0.078567  2.560518 -0.110968  0.708010   \n",
       "138 -0.516318  0.479273  0.803299 -0.078567 -0.239846 -0.110968 -0.484465   \n",
       "139  1.701598 -1.055158  0.000026 -0.078567 -0.677403 -0.110968  0.051439   \n",
       "140 -0.516318  1.246489 -0.809507 -0.078567 -0.677403 -0.110968 -0.030189   \n",
       "141 -0.516318 -1.055158 -0.056308 -0.078567  2.560518 -0.110968  0.708010   \n",
       "142 -0.516318  1.246489  0.102261 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "143  0.915501 -1.055158  0.390187 -0.078567 -0.677403 -0.110968 -0.981330   \n",
       "144  1.813897  1.246489 -0.054221 -0.078567  1.947938 -0.110968  0.353107   \n",
       "145 -0.516318  1.246489 -0.667630 -0.078567 -0.239846 -0.110968  0.566049   \n",
       "146  1.302935 -1.055158  0.511200 -0.078567 -0.239846 -0.110968  0.665422   \n",
       "147 -0.516318  0.479273  0.915966 -0.078567 -0.239846 -0.110968 -0.477367   \n",
       "148  0.915501 -1.055158  0.390187 -0.078567 -0.677403 -0.110968 -0.981330   \n",
       "149  2.133951 -1.055158 -0.903396 -0.078567 -0.677403 -0.110968  2.514468   \n",
       "150 -0.516318 -1.055158 -0.945125 -0.078567 -0.677403 -0.110968  2.471880   \n",
       "151  1.460154 -1.055158  0.415224 -0.078567 -0.239846 -0.110968  0.275028   \n",
       "152 -0.516318 -1.055158 -0.089691 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "153 -0.516318 -0.287943 -0.796989 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "154 -0.516318  0.479273 -0.246172 -0.078567 -0.239846 -0.110968  0.580245   \n",
       "155 -0.516318  2.780920 -2.023807 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "156 -0.516318  1.246489 -0.586260 -0.078567 -0.677403 -0.110968 -0.023091   \n",
       "157 -0.516318  0.095665  0.394360 -0.078567  1.947938 -0.110968 -0.097620   \n",
       "158 -0.516318  0.479273 -0.089691 -0.078567 -1.552516 -0.110968 -0.030189   \n",
       "159 -0.516318  1.246489  0.369323 -0.078567 -1.552516 -0.110968 -0.480916   \n",
       "160 -0.516318  0.479273 -0.968075 -0.078567 -0.239846 -0.110968 -0.030189   \n",
       "161  2.133951 -1.055158 -0.903396 -0.078567 -0.677403 -0.110968  2.471880   \n",
       "\n",
       "           DE        IN         O        VI        DI  TEST_RESULT_VALUE  \\\n",
       "0   -1.151937  1.474092 -0.177906 -0.987497  1.455932              223.2   \n",
       "1    0.305958 -1.152751 -0.177906 -2.077384  0.053231              445.7   \n",
       "2   -0.382433 -0.663930 -0.177906  1.328513 -1.201005              314.1   \n",
       "3    0.536489 -0.764435 -0.177906 -0.742272 -0.425904              214.3   \n",
       "4   -0.777324 -0.691340 -0.177906 -0.388059 -1.087246              215.3   \n",
       "5   -0.246889  0.446197 -0.177906  1.246772 -0.751755              207.0   \n",
       "6    2.186493 -0.810119 -0.177906  0.374862 -0.111140              252.6   \n",
       "7   -0.760248 -0.698193 -0.177906 -0.137385 -0.787907              210.5   \n",
       "8    2.186493 -0.810119 -0.177906  0.374862 -0.111140              321.1   \n",
       "9   -1.942786 -0.924330 -0.177906  0.647334 -1.873433              477.6   \n",
       "10   0.759549 -0.124856  5.586249 -0.170081  1.658384              250.0   \n",
       "11   1.382837 -0.695909 -0.177906 -0.306317  0.115895              263.1   \n",
       "12  -0.494497 -0.695909 -0.177906  0.211379  0.597923              200.0   \n",
       "13   0.458578 -0.458351 -0.177906 -0.469800  0.636003              300.2   \n",
       "14   0.527951 -0.695909 -0.177906  0.511098  0.212300              213.3   \n",
       "15  -0.128422 -0.695909 -0.177906 -0.033845  0.694328              237.6   \n",
       "16  -1.942786 -0.924330 -0.177906  0.647334 -1.873433              432.0   \n",
       "17   0.373196 -0.691340 -0.177906  0.184132 -0.157415              181.3   \n",
       "18   1.783064 -0.800983 -0.177906  0.429357 -0.157415              207.1   \n",
       "19  -0.769853  0.816239 -0.177906 -0.224576  0.368959              278.7   \n",
       "20   0.983677  0.704313 -0.177906 -0.578789  0.542007              259.0   \n",
       "21   0.759549 -0.124856  5.586249 -0.170081  1.658384              220.9   \n",
       "22   0.054082  1.017249 -0.177906  0.947053  1.111282              284.7   \n",
       "23  -0.246889  0.446197 -0.177906 -0.006598  0.694328              187.8   \n",
       "24  -0.128422 -0.695909 -0.177906 -0.033845  0.694328              228.0   \n",
       "25  -1.119919  0.674618 -0.177906 -0.578789  0.935342              158.3   \n",
       "26   2.217444 -0.810119 -0.177906  0.429357 -0.073060              187.2   \n",
       "27   0.054082  1.017249 -0.177906 -0.197328  0.838936              201.5   \n",
       "28   2.325239 -0.803267 -0.177906  5.551826  0.212300              236.3   \n",
       "29  -4.055987 -1.152751 -0.177906  0.361239 -3.161893              197.7   \n",
       "..        ...       ...       ...       ...       ...                ...   \n",
       "132 -0.378164  0.443912 -0.177906  0.184132  0.636003              235.6   \n",
       "133  0.759549  1.131460 -0.177906 -0.306317  0.212300              219.7   \n",
       "134  0.848133 -0.693625 -0.177906  0.347615 -0.975898              321.1   \n",
       "135 -0.760248 -0.698193 -0.177906 -0.137385 -0.787907              221.9   \n",
       "136 -1.151937  1.474092 -0.177906 -0.660530  1.455932              183.9   \n",
       "137  0.431896  1.446681 -0.177906 -0.796766  1.443881              227.8   \n",
       "138 -0.745306 -0.698193 -0.177906 -0.137385 -0.787907              234.7   \n",
       "139  1.034906  1.131460 -0.177906 -0.633283  0.636485              275.8   \n",
       "140  0.759549 -0.124856  5.586249 -0.170081  1.658384              206.0   \n",
       "141  0.431896  1.446681 -0.177906 -0.878508  1.443881              202.9   \n",
       "142 -1.151937  1.474092 -0.177906 -0.905755  1.455932              231.5   \n",
       "143 -0.382433 -0.663930 -0.177906  0.320368 -1.201005              340.0   \n",
       "144 -0.817880  0.813955 -0.177906 -0.442553  0.874124              324.0   \n",
       "145 -0.732498 -0.810119 -0.177906  1.437502 -0.607147              432.7   \n",
       "146 -0.378164  0.443912 -0.177906 -0.715025  0.131320              276.8   \n",
       "147 -0.719691 -0.691340 -0.177906 -0.306317 -0.764288              182.6   \n",
       "148 -0.382433 -0.663930 -0.177906  0.320368 -1.201005              239.2   \n",
       "149  2.186493 -0.810119 -0.177906  0.320368 -0.111140              240.1   \n",
       "150  2.165148 -0.828393 -0.177906  0.374862 -0.157415              199.9   \n",
       "151  0.308093 -0.695909 -0.177906 -0.796766  0.453314              198.0   \n",
       "152  1.189660 -0.010646 -0.177906 -0.578789  0.694328              264.7   \n",
       "153  0.054082  1.017249 -0.177906  0.211379  1.200457              244.7   \n",
       "154  0.582382 -1.152751 -0.177906  0.701828 -0.771036              341.7   \n",
       "155 -1.151937  5.128829 -0.177906  1.437502  1.296863              215.3   \n",
       "156  0.546095 -0.234498 -0.177906 -0.142834  1.642477              214.5   \n",
       "157  1.382837 -0.695909 -0.177906 -0.142834 -0.833700              242.3   \n",
       "158  1.122422 -0.695909 -0.177906  0.456604 -0.992769              245.5   \n",
       "159  0.521547 -0.739309 -0.177906 -0.469800 -0.380112              222.6   \n",
       "160 -0.111346 -0.124856 -0.177906  0.592840  1.176356              193.9   \n",
       "161  2.165148 -0.828393 -0.177906  0.320368 -0.157415              230.7   \n",
       "\n",
       "     LAB_CODE-AL  LAB_CODE-EG  LAB_CODE-LT  LAB_CODE-SR  Result_Code-DP1--WDN  \n",
       "0            0.0          0.0          0.0          1.0                   1.0  \n",
       "1            0.0          0.0          1.0          0.0                   1.0  \n",
       "2            0.0          0.0          0.0          1.0                   1.0  \n",
       "3            0.0          0.0          1.0          0.0                   1.0  \n",
       "4            0.0          0.0          1.0          0.0                   1.0  \n",
       "5            0.0          1.0          0.0          0.0                   1.0  \n",
       "6            0.0          0.0          0.0          1.0                   1.0  \n",
       "7            0.0          0.0          1.0          0.0                   1.0  \n",
       "8            0.0          0.0          0.0          1.0                   1.0  \n",
       "9            0.0          1.0          0.0          0.0                   1.0  \n",
       "10           0.0          0.0          1.0          0.0                   1.0  \n",
       "11           0.0          0.0          1.0          0.0                   1.0  \n",
       "12           0.0          0.0          1.0          0.0                   1.0  \n",
       "13           0.0          0.0          1.0          0.0                   1.0  \n",
       "14           0.0          0.0          1.0          0.0                   1.0  \n",
       "15           0.0          0.0          1.0          0.0                   1.0  \n",
       "16           0.0          1.0          0.0          0.0                   1.0  \n",
       "17           0.0          0.0          1.0          0.0                   1.0  \n",
       "18           0.0          0.0          1.0          0.0                   1.0  \n",
       "19           0.0          0.0          1.0          0.0                   1.0  \n",
       "20           0.0          0.0          1.0          0.0                   1.0  \n",
       "21           0.0          0.0          1.0          0.0                   1.0  \n",
       "22           0.0          0.0          1.0          0.0                   1.0  \n",
       "23           0.0          0.0          1.0          0.0                   1.0  \n",
       "24           0.0          0.0          1.0          0.0                   1.0  \n",
       "25           0.0          0.0          0.0          1.0                   1.0  \n",
       "26           0.0          0.0          1.0          0.0                   1.0  \n",
       "27           0.0          0.0          1.0          0.0                   1.0  \n",
       "28           0.0          1.0          0.0          0.0                   1.0  \n",
       "29           0.0          0.0          1.0          0.0                   1.0  \n",
       "..           ...          ...          ...          ...                   ...  \n",
       "132          0.0          0.0          1.0          0.0                   1.0  \n",
       "133          0.0          0.0          1.0          0.0                   1.0  \n",
       "134          0.0          0.0          0.0          1.0                   1.0  \n",
       "135          0.0          0.0          1.0          0.0                   1.0  \n",
       "136          0.0          0.0          0.0          1.0                   1.0  \n",
       "137          0.0          0.0          1.0          0.0                   1.0  \n",
       "138          0.0          0.0          1.0          0.0                   1.0  \n",
       "139          0.0          0.0          1.0          0.0                   1.0  \n",
       "140          0.0          0.0          1.0          0.0                   1.0  \n",
       "141          0.0          0.0          1.0          0.0                   1.0  \n",
       "142          0.0          0.0          0.0          1.0                   1.0  \n",
       "143          0.0          0.0          0.0          1.0                   1.0  \n",
       "144          0.0          0.0          0.0          1.0                   1.0  \n",
       "145          0.0          1.0          0.0          0.0                   1.0  \n",
       "146          0.0          0.0          1.0          0.0                   1.0  \n",
       "147          0.0          0.0          1.0          0.0                   1.0  \n",
       "148          0.0          0.0          0.0          1.0                   1.0  \n",
       "149          0.0          0.0          1.0          0.0                   1.0  \n",
       "150          0.0          0.0          1.0          0.0                   1.0  \n",
       "151          0.0          0.0          1.0          0.0                   1.0  \n",
       "152          0.0          0.0          1.0          0.0                   1.0  \n",
       "153          0.0          0.0          1.0          0.0                   1.0  \n",
       "154          0.0          0.0          1.0          0.0                   1.0  \n",
       "155          0.0          0.0          0.0          1.0                   1.0  \n",
       "156          0.0          0.0          1.0          0.0                   1.0  \n",
       "157          0.0          0.0          1.0          0.0                   1.0  \n",
       "158          0.0          0.0          1.0          0.0                   1.0  \n",
       "159          0.0          0.0          1.0          0.0                   1.0  \n",
       "160          0.0          0.0          1.0          0.0                   1.0  \n",
       "161          0.0          0.0          1.0          0.0                   1.0  \n",
       "\n",
       "[162 rows x 18 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#discard rows where z-score > 2\n",
    "df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# Create x(predictors) and y (expected outcome)\n",
    "x,y = to_xy(df, \"TEST_RESULT_VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into test/train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a deep neural network with 3 hidden layers of 105,50,25,12,5\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[85, 40, 20, 10], learning_rate=0.01, steps=300000)\n",
    "\n",
    "#early_stop = skflow.monitors.ValidationMonitor(x_test,y_test,early_stopping_rounds=2000, print_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #25, avg. train loss: 26694.48438\n",
      "Step #200, epoch #50, avg. train loss: 4071.89160\n",
      "Step #300, epoch #75, avg. train loss: 1306.95129\n",
      "Step #400, epoch #100, avg. train loss: 1132.84021\n",
      "Step #500, epoch #125, avg. train loss: 1031.47119\n",
      "Step #600, epoch #150, avg. train loss: 953.54272\n",
      "Step #700, epoch #175, avg. train loss: 891.61108\n",
      "Step #800, epoch #200, avg. train loss: 835.11084\n",
      "Step #900, epoch #225, avg. train loss: 773.07153\n",
      "Step #1000, epoch #250, avg. train loss: 722.12128\n",
      "Step #1100, epoch #275, avg. train loss: 681.34680\n",
      "Step #1200, epoch #300, avg. train loss: 646.50403\n",
      "Step #1300, epoch #325, avg. train loss: 606.13226\n",
      "Step #1400, epoch #350, avg. train loss: 576.56317\n",
      "Step #1500, epoch #375, avg. train loss: 548.33429\n",
      "Step #1600, epoch #400, avg. train loss: 525.23633\n",
      "Step #1700, epoch #425, avg. train loss: 506.14227\n",
      "Step #1800, epoch #450, avg. train loss: 487.63281\n",
      "Step #1900, epoch #475, avg. train loss: 470.48450\n",
      "Step #2000, epoch #500, avg. train loss: 454.15811\n",
      "Step #2100, epoch #525, avg. train loss: 440.59433\n",
      "Step #2200, epoch #550, avg. train loss: 429.48279\n",
      "Step #2300, epoch #575, avg. train loss: 421.10309\n",
      "Step #2400, epoch #600, avg. train loss: 410.55722\n",
      "Step #2500, epoch #625, avg. train loss: 398.36374\n",
      "Step #2600, epoch #650, avg. train loss: 389.23547\n",
      "Step #2700, epoch #675, avg. train loss: 385.52637\n",
      "Step #2800, epoch #700, avg. train loss: 376.32657\n",
      "Step #2900, epoch #725, avg. train loss: 370.16949\n",
      "Step #3000, epoch #750, avg. train loss: 364.08875\n",
      "Step #3100, epoch #775, avg. train loss: 356.13617\n",
      "Step #3200, epoch #800, avg. train loss: 351.47778\n",
      "Step #3300, epoch #825, avg. train loss: 345.58661\n",
      "Step #3400, epoch #850, avg. train loss: 337.73590\n",
      "Step #3500, epoch #875, avg. train loss: 333.38220\n",
      "Step #3600, epoch #900, avg. train loss: 328.88583\n",
      "Step #3700, epoch #925, avg. train loss: 323.23370\n",
      "Step #3800, epoch #950, avg. train loss: 318.93274\n",
      "Step #3900, epoch #975, avg. train loss: 312.50589\n",
      "Step #4000, epoch #1000, avg. train loss: 308.44302\n",
      "Step #4100, epoch #1025, avg. train loss: 303.04950\n",
      "Step #4200, epoch #1050, avg. train loss: 298.57355\n",
      "Step #4300, epoch #1075, avg. train loss: 295.76697\n",
      "Step #4400, epoch #1100, avg. train loss: 289.37103\n",
      "Step #4500, epoch #1125, avg. train loss: 285.23029\n",
      "Step #4600, epoch #1150, avg. train loss: 283.82635\n",
      "Step #4700, epoch #1175, avg. train loss: 277.74814\n",
      "Step #4800, epoch #1200, avg. train loss: 275.18201\n",
      "Step #4900, epoch #1225, avg. train loss: 271.51669\n",
      "Step #5000, epoch #1250, avg. train loss: 270.27216\n",
      "Step #5100, epoch #1275, avg. train loss: 266.40561\n",
      "Step #5200, epoch #1300, avg. train loss: 262.32098\n",
      "Step #5300, epoch #1325, avg. train loss: 259.08154\n",
      "Step #5400, epoch #1350, avg. train loss: 255.80202\n",
      "Step #5500, epoch #1375, avg. train loss: 253.09753\n",
      "Step #5600, epoch #1400, avg. train loss: 250.42802\n",
      "Step #5700, epoch #1425, avg. train loss: 247.49364\n",
      "Step #5800, epoch #1450, avg. train loss: 244.37033\n",
      "Step #5900, epoch #1475, avg. train loss: 242.28511\n",
      "Step #6000, epoch #1500, avg. train loss: 239.62694\n",
      "Step #6100, epoch #1525, avg. train loss: 239.41681\n",
      "Step #6200, epoch #1550, avg. train loss: 235.21297\n",
      "Step #6300, epoch #1575, avg. train loss: 233.37270\n",
      "Step #6400, epoch #1600, avg. train loss: 231.91666\n",
      "Step #6500, epoch #1625, avg. train loss: 229.04385\n",
      "Step #6600, epoch #1650, avg. train loss: 226.95807\n",
      "Step #6700, epoch #1675, avg. train loss: 225.16080\n",
      "Step #6800, epoch #1700, avg. train loss: 224.06752\n",
      "Step #6900, epoch #1725, avg. train loss: 222.88643\n",
      "Step #7000, epoch #1750, avg. train loss: 222.68958\n",
      "Step #7100, epoch #1775, avg. train loss: 218.80415\n",
      "Step #7200, epoch #1800, avg. train loss: 217.69476\n",
      "Step #7300, epoch #1825, avg. train loss: 215.99521\n",
      "Step #7400, epoch #1850, avg. train loss: 215.40031\n",
      "Step #7500, epoch #1875, avg. train loss: 213.95398\n",
      "Step #7600, epoch #1900, avg. train loss: 211.21507\n",
      "Step #7700, epoch #1925, avg. train loss: 210.96933\n",
      "Step #7800, epoch #1950, avg. train loss: 210.04848\n",
      "Step #7900, epoch #1975, avg. train loss: 209.31573\n",
      "Step #8000, epoch #2000, avg. train loss: 206.95108\n",
      "Step #8100, epoch #2025, avg. train loss: 205.77814\n",
      "Step #8200, epoch #2050, avg. train loss: 206.25299\n",
      "Step #8300, epoch #2075, avg. train loss: 203.49556\n",
      "Step #8400, epoch #2100, avg. train loss: 203.72905\n",
      "Step #8500, epoch #2125, avg. train loss: 202.31081\n",
      "Step #8600, epoch #2150, avg. train loss: 200.49898\n",
      "Step #8700, epoch #2175, avg. train loss: 201.96010\n",
      "Step #8800, epoch #2200, avg. train loss: 198.83539\n",
      "Step #8900, epoch #2225, avg. train loss: 198.92339\n",
      "Step #9000, epoch #2250, avg. train loss: 197.12045\n",
      "Step #9100, epoch #2275, avg. train loss: 195.10085\n",
      "Step #9200, epoch #2300, avg. train loss: 197.46477\n",
      "Step #9300, epoch #2325, avg. train loss: 193.84659\n",
      "Step #9400, epoch #2350, avg. train loss: 194.40692\n",
      "Step #9500, epoch #2375, avg. train loss: 194.40584\n",
      "Step #9600, epoch #2400, avg. train loss: 193.55266\n",
      "Step #9700, epoch #2425, avg. train loss: 191.71379\n",
      "Step #9800, epoch #2450, avg. train loss: 192.89362\n",
      "Step #9900, epoch #2475, avg. train loss: 191.62624\n",
      "Step #10000, epoch #2500, avg. train loss: 189.63858\n",
      "Step #10100, epoch #2525, avg. train loss: 187.97034\n",
      "Step #10200, epoch #2550, avg. train loss: 190.27638\n",
      "Step #10300, epoch #2575, avg. train loss: 188.30620\n",
      "Step #10400, epoch #2600, avg. train loss: 187.69882\n",
      "Step #10500, epoch #2625, avg. train loss: 186.48442\n",
      "Step #10600, epoch #2650, avg. train loss: 184.28299\n",
      "Step #10700, epoch #2675, avg. train loss: 185.69208\n",
      "Step #10800, epoch #2700, avg. train loss: 183.79051\n",
      "Step #10900, epoch #2725, avg. train loss: 183.32011\n",
      "Step #11000, epoch #2750, avg. train loss: 183.46161\n",
      "Step #11100, epoch #2775, avg. train loss: 183.03767\n",
      "Step #11200, epoch #2800, avg. train loss: 182.39606\n",
      "Step #11300, epoch #2825, avg. train loss: 181.42120\n",
      "Step #11400, epoch #2850, avg. train loss: 180.63055\n",
      "Step #11500, epoch #2875, avg. train loss: 180.64172\n",
      "Step #11600, epoch #2900, avg. train loss: 178.50110\n",
      "Step #11700, epoch #2925, avg. train loss: 179.16684\n",
      "Step #11800, epoch #2950, avg. train loss: 178.56026\n",
      "Step #11900, epoch #2975, avg. train loss: 177.03084\n",
      "Step #12000, epoch #3000, avg. train loss: 177.72481\n",
      "Step #12100, epoch #3025, avg. train loss: 176.41176\n",
      "Step #12200, epoch #3050, avg. train loss: 177.39510\n",
      "Step #12300, epoch #3075, avg. train loss: 175.62868\n",
      "Step #12400, epoch #3100, avg. train loss: 174.46597\n",
      "Step #12500, epoch #3125, avg. train loss: 173.00809\n",
      "Step #12600, epoch #3150, avg. train loss: 174.36264\n",
      "Step #12700, epoch #3175, avg. train loss: 172.53311\n",
      "Step #12800, epoch #3200, avg. train loss: 173.01158\n",
      "Step #12900, epoch #3225, avg. train loss: 172.63176\n",
      "Step #13000, epoch #3250, avg. train loss: 171.35834\n",
      "Step #13100, epoch #3275, avg. train loss: 171.13045\n",
      "Step #13200, epoch #3300, avg. train loss: 171.10400\n",
      "Step #13300, epoch #3325, avg. train loss: 169.10524\n",
      "Step #13400, epoch #3350, avg. train loss: 169.69022\n",
      "Step #13500, epoch #3375, avg. train loss: 168.18408\n",
      "Step #13600, epoch #3400, avg. train loss: 168.46898\n",
      "Step #13700, epoch #3425, avg. train loss: 168.20369\n",
      "Step #13800, epoch #3450, avg. train loss: 167.92747\n",
      "Step #13900, epoch #3475, avg. train loss: 168.32800\n",
      "Step #14000, epoch #3500, avg. train loss: 165.75172\n",
      "Step #14100, epoch #3525, avg. train loss: 166.70650\n",
      "Step #14200, epoch #3550, avg. train loss: 165.50116\n",
      "Step #14300, epoch #3575, avg. train loss: 164.87198\n",
      "Step #14400, epoch #3600, avg. train loss: 164.17857\n",
      "Step #14500, epoch #3625, avg. train loss: 164.76285\n",
      "Step #14600, epoch #3650, avg. train loss: 163.55789\n",
      "Step #14700, epoch #3675, avg. train loss: 163.28006\n",
      "Step #14800, epoch #3700, avg. train loss: 161.76683\n",
      "Step #14900, epoch #3725, avg. train loss: 162.39783\n",
      "Step #15000, epoch #3750, avg. train loss: 161.36249\n",
      "Step #15100, epoch #3775, avg. train loss: 161.80318\n",
      "Step #15200, epoch #3800, avg. train loss: 160.94194\n",
      "Step #15300, epoch #3825, avg. train loss: 160.28822\n",
      "Step #15400, epoch #3850, avg. train loss: 159.91589\n",
      "Step #15500, epoch #3875, avg. train loss: 161.53114\n",
      "Step #15600, epoch #3900, avg. train loss: 158.60478\n",
      "Step #15700, epoch #3925, avg. train loss: 158.77423\n",
      "Step #15800, epoch #3950, avg. train loss: 156.85931\n",
      "Step #15900, epoch #3975, avg. train loss: 157.99931\n",
      "Step #16000, epoch #4000, avg. train loss: 158.28604\n",
      "Step #16100, epoch #4025, avg. train loss: 155.70898\n",
      "Step #16200, epoch #4050, avg. train loss: 157.15881\n",
      "Step #16300, epoch #4075, avg. train loss: 155.93619\n",
      "Step #16400, epoch #4100, avg. train loss: 156.83986\n",
      "Step #16500, epoch #4125, avg. train loss: 153.88130\n",
      "Step #16600, epoch #4150, avg. train loss: 153.99982\n",
      "Step #16700, epoch #4175, avg. train loss: 155.09485\n",
      "Step #16800, epoch #4200, avg. train loss: 153.69727\n",
      "Step #16900, epoch #4225, avg. train loss: 153.95079\n",
      "Step #17000, epoch #4250, avg. train loss: 153.77484\n",
      "Step #17100, epoch #4275, avg. train loss: 153.16365\n",
      "Step #17200, epoch #4300, avg. train loss: 151.80595\n",
      "Step #17300, epoch #4325, avg. train loss: 151.30820\n",
      "Step #17400, epoch #4350, avg. train loss: 152.51698\n",
      "Step #17500, epoch #4375, avg. train loss: 151.32201\n",
      "Step #17600, epoch #4400, avg. train loss: 151.02007\n",
      "Step #17700, epoch #4425, avg. train loss: 150.98364\n",
      "Step #17800, epoch #4450, avg. train loss: 150.73138\n",
      "Step #17900, epoch #4475, avg. train loss: 149.51189\n",
      "Step #18000, epoch #4500, avg. train loss: 150.02583\n",
      "Step #18100, epoch #4525, avg. train loss: 149.09694\n",
      "Step #18200, epoch #4550, avg. train loss: 149.12701\n",
      "Step #18300, epoch #4575, avg. train loss: 147.92294\n",
      "Step #18400, epoch #4600, avg. train loss: 149.03528\n",
      "Step #18500, epoch #4625, avg. train loss: 148.97569\n",
      "Step #18600, epoch #4650, avg. train loss: 147.79637\n",
      "Step #18700, epoch #4675, avg. train loss: 147.01640\n",
      "Step #18800, epoch #4700, avg. train loss: 148.09634\n",
      "Step #18900, epoch #4725, avg. train loss: 148.29010\n",
      "Step #19000, epoch #4750, avg. train loss: 146.11212\n",
      "Step #19100, epoch #4775, avg. train loss: 146.01869\n",
      "Step #19200, epoch #4800, avg. train loss: 145.52609\n",
      "Step #19300, epoch #4825, avg. train loss: 147.65005\n",
      "Step #19400, epoch #4850, avg. train loss: 145.95348\n",
      "Step #19500, epoch #4875, avg. train loss: 146.13866\n",
      "Step #19600, epoch #4900, avg. train loss: 144.57492\n",
      "Step #19700, epoch #4925, avg. train loss: 143.92554\n",
      "Step #19800, epoch #4950, avg. train loss: 143.89482\n",
      "Step #19900, epoch #4975, avg. train loss: 143.82628\n",
      "Step #20000, epoch #5000, avg. train loss: 143.60280\n",
      "Step #20100, epoch #5025, avg. train loss: 143.03882\n",
      "Step #20200, epoch #5050, avg. train loss: 142.90767\n",
      "Step #20300, epoch #5075, avg. train loss: 143.75580\n",
      "Step #20400, epoch #5100, avg. train loss: 143.67865\n",
      "Step #20500, epoch #5125, avg. train loss: 143.29277\n",
      "Step #20600, epoch #5150, avg. train loss: 141.57285\n",
      "Step #20700, epoch #5175, avg. train loss: 141.22145\n",
      "Step #20800, epoch #5200, avg. train loss: 142.66718\n",
      "Step #20900, epoch #5225, avg. train loss: 139.35527\n",
      "Step #21000, epoch #5250, avg. train loss: 139.57681\n",
      "Step #21100, epoch #5275, avg. train loss: 139.56793\n",
      "Step #21200, epoch #5300, avg. train loss: 139.60277\n",
      "Step #21300, epoch #5325, avg. train loss: 138.71713\n",
      "Step #21400, epoch #5350, avg. train loss: 139.97226\n",
      "Step #21500, epoch #5375, avg. train loss: 140.13625\n",
      "Step #21600, epoch #5400, avg. train loss: 139.01071\n",
      "Step #21700, epoch #5425, avg. train loss: 138.11191\n",
      "Step #21800, epoch #5450, avg. train loss: 138.44998\n",
      "Step #21900, epoch #5475, avg. train loss: 137.30785\n",
      "Step #22000, epoch #5500, avg. train loss: 137.32028\n",
      "Step #22100, epoch #5525, avg. train loss: 136.57439\n",
      "Step #22200, epoch #5550, avg. train loss: 137.34541\n",
      "Step #22300, epoch #5575, avg. train loss: 136.32762\n",
      "Step #22400, epoch #5600, avg. train loss: 136.44293\n",
      "Step #22500, epoch #5625, avg. train loss: 136.93274\n",
      "Step #22600, epoch #5650, avg. train loss: 134.75186\n",
      "Step #22700, epoch #5675, avg. train loss: 136.33302\n",
      "Step #22800, epoch #5700, avg. train loss: 135.76968\n",
      "Step #22900, epoch #5725, avg. train loss: 135.71767\n",
      "Step #23000, epoch #5750, avg. train loss: 135.67952\n",
      "Step #23100, epoch #5775, avg. train loss: 135.82558\n",
      "Step #23200, epoch #5800, avg. train loss: 134.51550\n",
      "Step #23300, epoch #5825, avg. train loss: 134.53477\n",
      "Step #23400, epoch #5850, avg. train loss: 133.93391\n",
      "Step #23500, epoch #5875, avg. train loss: 133.26740\n",
      "Step #23600, epoch #5900, avg. train loss: 132.50044\n",
      "Step #23700, epoch #5925, avg. train loss: 132.11018\n",
      "Step #23800, epoch #5950, avg. train loss: 132.28650\n",
      "Step #23900, epoch #5975, avg. train loss: 132.66553\n",
      "Step #24000, epoch #6000, avg. train loss: 131.88857\n",
      "Step #24100, epoch #6025, avg. train loss: 132.02419\n",
      "Step #24200, epoch #6050, avg. train loss: 131.94281\n",
      "Step #24300, epoch #6075, avg. train loss: 131.32428\n",
      "Step #24400, epoch #6100, avg. train loss: 130.68442\n",
      "Step #24500, epoch #6125, avg. train loss: 130.79036\n",
      "Step #24600, epoch #6150, avg. train loss: 130.66777\n",
      "Step #24700, epoch #6175, avg. train loss: 130.56821\n",
      "Step #24800, epoch #6200, avg. train loss: 129.06111\n",
      "Step #24900, epoch #6225, avg. train loss: 128.60971\n",
      "Step #25000, epoch #6250, avg. train loss: 127.90011\n",
      "Step #25100, epoch #6275, avg. train loss: 128.23477\n",
      "Step #25200, epoch #6300, avg. train loss: 126.99606\n",
      "Step #25300, epoch #6325, avg. train loss: 126.50196\n",
      "Step #25400, epoch #6350, avg. train loss: 127.08691\n",
      "Step #25500, epoch #6375, avg. train loss: 126.89670\n",
      "Step #25600, epoch #6400, avg. train loss: 126.87180\n",
      "Step #25700, epoch #6425, avg. train loss: 125.67732\n",
      "Step #25800, epoch #6450, avg. train loss: 125.99989\n",
      "Step #25900, epoch #6475, avg. train loss: 126.94444\n",
      "Step #26000, epoch #6500, avg. train loss: 125.16441\n",
      "Step #26100, epoch #6525, avg. train loss: 125.69783\n",
      "Step #26200, epoch #6550, avg. train loss: 123.56291\n",
      "Step #26300, epoch #6575, avg. train loss: 123.40298\n",
      "Step #26400, epoch #6600, avg. train loss: 123.58801\n",
      "Step #26500, epoch #6625, avg. train loss: 122.69287\n",
      "Step #26600, epoch #6650, avg. train loss: 123.72319\n",
      "Step #26700, epoch #6675, avg. train loss: 123.05116\n",
      "Step #26800, epoch #6700, avg. train loss: 122.35065\n",
      "Step #26900, epoch #6725, avg. train loss: 124.04962\n",
      "Step #27000, epoch #6750, avg. train loss: 122.07202\n",
      "Step #27100, epoch #6775, avg. train loss: 122.23064\n",
      "Step #27200, epoch #6800, avg. train loss: 122.62276\n",
      "Step #27300, epoch #6825, avg. train loss: 121.18580\n",
      "Step #27400, epoch #6850, avg. train loss: 121.75433\n",
      "Step #27500, epoch #6875, avg. train loss: 121.63410\n",
      "Step #27600, epoch #6900, avg. train loss: 120.97070\n",
      "Step #27700, epoch #6925, avg. train loss: 120.33315\n",
      "Step #27800, epoch #6950, avg. train loss: 120.21546\n",
      "Step #27900, epoch #6975, avg. train loss: 121.63285\n",
      "Step #28000, epoch #7000, avg. train loss: 120.76559\n",
      "Step #28100, epoch #7025, avg. train loss: 119.90044\n",
      "Step #28200, epoch #7050, avg. train loss: 121.08485\n",
      "Step #28300, epoch #7075, avg. train loss: 119.26590\n",
      "Step #28400, epoch #7100, avg. train loss: 119.76900\n",
      "Step #28500, epoch #7125, avg. train loss: 118.72301\n",
      "Step #28600, epoch #7150, avg. train loss: 119.73759\n",
      "Step #28700, epoch #7175, avg. train loss: 119.16592\n",
      "Step #28800, epoch #7200, avg. train loss: 119.18204\n",
      "Step #28900, epoch #7225, avg. train loss: 119.11251\n",
      "Step #29000, epoch #7250, avg. train loss: 119.80444\n",
      "Step #29100, epoch #7275, avg. train loss: 118.59129\n",
      "Step #29200, epoch #7300, avg. train loss: 117.20927\n",
      "Step #29300, epoch #7325, avg. train loss: 117.43545\n",
      "Step #29400, epoch #7350, avg. train loss: 118.26366\n",
      "Step #29500, epoch #7375, avg. train loss: 116.26242\n",
      "Step #29600, epoch #7400, avg. train loss: 117.27544\n",
      "Step #29700, epoch #7425, avg. train loss: 117.73640\n",
      "Step #29800, epoch #7450, avg. train loss: 117.00244\n",
      "Step #29900, epoch #7475, avg. train loss: 117.29578\n",
      "Step #30000, epoch #7500, avg. train loss: 117.58694\n",
      "Step #30100, epoch #7525, avg. train loss: 116.98193\n",
      "Step #30200, epoch #7550, avg. train loss: 115.89481\n",
      "Step #30300, epoch #7575, avg. train loss: 116.33801\n",
      "Step #30400, epoch #7600, avg. train loss: 116.83910\n",
      "Step #30500, epoch #7625, avg. train loss: 115.74123\n",
      "Step #30600, epoch #7650, avg. train loss: 114.74637\n",
      "Step #30700, epoch #7675, avg. train loss: 114.64705\n",
      "Step #30800, epoch #7700, avg. train loss: 115.02351\n",
      "Step #30900, epoch #7725, avg. train loss: 114.71961\n",
      "Step #31000, epoch #7750, avg. train loss: 115.24413\n",
      "Step #31100, epoch #7775, avg. train loss: 115.32450\n",
      "Step #31200, epoch #7800, avg. train loss: 114.47176\n",
      "Step #31300, epoch #7825, avg. train loss: 113.96276\n",
      "Step #31400, epoch #7850, avg. train loss: 113.60617\n",
      "Step #31500, epoch #7875, avg. train loss: 114.30585\n",
      "Step #31600, epoch #7900, avg. train loss: 113.78590\n",
      "Step #31700, epoch #7925, avg. train loss: 113.35143\n",
      "Step #31800, epoch #7950, avg. train loss: 113.61594\n",
      "Step #31900, epoch #7975, avg. train loss: 113.36810\n",
      "Step #32000, epoch #8000, avg. train loss: 114.02179\n",
      "Step #32100, epoch #8025, avg. train loss: 113.08671\n",
      "Step #32200, epoch #8050, avg. train loss: 113.40765\n",
      "Step #32300, epoch #8075, avg. train loss: 112.45787\n",
      "Step #32400, epoch #8100, avg. train loss: 112.21423\n",
      "Step #32500, epoch #8125, avg. train loss: 112.08694\n",
      "Step #32600, epoch #8150, avg. train loss: 112.84388\n",
      "Step #32700, epoch #8175, avg. train loss: 112.35041\n",
      "Step #32800, epoch #8200, avg. train loss: 112.83656\n",
      "Step #32900, epoch #8225, avg. train loss: 112.18546\n",
      "Step #33000, epoch #8250, avg. train loss: 112.37656\n",
      "Step #33100, epoch #8275, avg. train loss: 111.85793\n",
      "Step #33200, epoch #8300, avg. train loss: 112.45348\n",
      "Step #33300, epoch #8325, avg. train loss: 111.95971\n",
      "Step #33400, epoch #8350, avg. train loss: 112.03705\n",
      "Step #33500, epoch #8375, avg. train loss: 111.43039\n",
      "Step #33600, epoch #8400, avg. train loss: 111.53356\n",
      "Step #33700, epoch #8425, avg. train loss: 110.47317\n",
      "Step #33800, epoch #8450, avg. train loss: 111.37928\n",
      "Step #33900, epoch #8475, avg. train loss: 110.56107\n",
      "Step #34000, epoch #8500, avg. train loss: 110.25726\n",
      "Step #34100, epoch #8525, avg. train loss: 110.38678\n",
      "Step #34200, epoch #8550, avg. train loss: 110.63829\n",
      "Step #34300, epoch #8575, avg. train loss: 109.97068\n",
      "Step #34400, epoch #8600, avg. train loss: 110.91617\n",
      "Step #34500, epoch #8625, avg. train loss: 110.30782\n",
      "Step #34600, epoch #8650, avg. train loss: 110.17867\n",
      "Step #34700, epoch #8675, avg. train loss: 109.68415\n",
      "Step #34800, epoch #8700, avg. train loss: 109.25402\n",
      "Step #34900, epoch #8725, avg. train loss: 109.91939\n",
      "Step #35000, epoch #8750, avg. train loss: 108.38168\n",
      "Step #35100, epoch #8775, avg. train loss: 108.93024\n",
      "Step #35200, epoch #8800, avg. train loss: 109.51620\n",
      "Step #35300, epoch #8825, avg. train loss: 109.02205\n",
      "Step #35400, epoch #8850, avg. train loss: 108.91599\n",
      "Step #35500, epoch #8875, avg. train loss: 108.62505\n",
      "Step #35600, epoch #8900, avg. train loss: 109.88200\n",
      "Step #35700, epoch #8925, avg. train loss: 109.61555\n",
      "Step #35800, epoch #8950, avg. train loss: 108.52662\n",
      "Step #35900, epoch #8975, avg. train loss: 108.76253\n",
      "Step #36000, epoch #9000, avg. train loss: 108.14119\n",
      "Step #36100, epoch #9025, avg. train loss: 108.34032\n",
      "Step #36200, epoch #9050, avg. train loss: 107.95235\n",
      "Step #36300, epoch #9075, avg. train loss: 107.81795\n",
      "Step #36400, epoch #9100, avg. train loss: 107.40094\n",
      "Step #36500, epoch #9125, avg. train loss: 108.01363\n",
      "Step #36600, epoch #9150, avg. train loss: 107.63937\n",
      "Step #36700, epoch #9175, avg. train loss: 107.87326\n",
      "Step #36800, epoch #9200, avg. train loss: 107.50186\n",
      "Step #36900, epoch #9225, avg. train loss: 108.68342\n",
      "Step #37000, epoch #9250, avg. train loss: 108.17242\n",
      "Step #37100, epoch #9275, avg. train loss: 106.72423\n",
      "Step #37200, epoch #9300, avg. train loss: 107.04308\n",
      "Step #37300, epoch #9325, avg. train loss: 106.18624\n",
      "Step #37400, epoch #9350, avg. train loss: 107.57010\n",
      "Step #37500, epoch #9375, avg. train loss: 107.13644\n",
      "Step #37600, epoch #9400, avg. train loss: 106.51333\n",
      "Step #37700, epoch #9425, avg. train loss: 107.04919\n",
      "Step #37800, epoch #9450, avg. train loss: 107.83661\n",
      "Step #37900, epoch #9475, avg. train loss: 106.21429\n",
      "Step #38000, epoch #9500, avg. train loss: 105.89809\n",
      "Step #38100, epoch #9525, avg. train loss: 107.21922\n",
      "Step #38200, epoch #9550, avg. train loss: 106.02399\n",
      "Step #38300, epoch #9575, avg. train loss: 106.40563\n",
      "Step #38400, epoch #9600, avg. train loss: 106.09521\n",
      "Step #38500, epoch #9625, avg. train loss: 104.66814\n",
      "Step #38600, epoch #9650, avg. train loss: 105.59141\n",
      "Step #38700, epoch #9675, avg. train loss: 104.46255\n",
      "Step #38800, epoch #9700, avg. train loss: 105.19933\n",
      "Step #38900, epoch #9725, avg. train loss: 105.22510\n",
      "Step #39000, epoch #9750, avg. train loss: 105.06892\n",
      "Step #39100, epoch #9775, avg. train loss: 104.96674\n",
      "Step #39200, epoch #9800, avg. train loss: 104.90218\n",
      "Step #39300, epoch #9825, avg. train loss: 104.77261\n",
      "Step #39400, epoch #9850, avg. train loss: 104.66743\n",
      "Step #39500, epoch #9875, avg. train loss: 104.93759\n",
      "Step #39600, epoch #9900, avg. train loss: 104.43670\n",
      "Step #39700, epoch #9925, avg. train loss: 104.60664\n",
      "Step #39800, epoch #9950, avg. train loss: 104.22980\n",
      "Step #39900, epoch #9975, avg. train loss: 105.08838\n",
      "Step #40000, epoch #10000, avg. train loss: 103.31798\n",
      "Step #40100, epoch #10025, avg. train loss: 104.30099\n",
      "Step #40200, epoch #10050, avg. train loss: 103.93258\n",
      "Step #40300, epoch #10075, avg. train loss: 104.89422\n",
      "Step #40400, epoch #10100, avg. train loss: 103.59658\n",
      "Step #40500, epoch #10125, avg. train loss: 104.32747\n",
      "Step #40600, epoch #10150, avg. train loss: 103.10488\n",
      "Step #40700, epoch #10175, avg. train loss: 103.57010\n",
      "Step #40800, epoch #10200, avg. train loss: 103.08695\n",
      "Step #40900, epoch #10225, avg. train loss: 103.27852\n",
      "Step #41000, epoch #10250, avg. train loss: 103.98364\n",
      "Step #41100, epoch #10275, avg. train loss: 103.47513\n",
      "Step #41200, epoch #10300, avg. train loss: 102.60506\n",
      "Step #41300, epoch #10325, avg. train loss: 104.20268\n",
      "Step #41400, epoch #10350, avg. train loss: 103.22636\n",
      "Step #41500, epoch #10375, avg. train loss: 102.83073\n",
      "Step #41600, epoch #10400, avg. train loss: 103.35209\n",
      "Step #41700, epoch #10425, avg. train loss: 103.20190\n",
      "Step #41800, epoch #10450, avg. train loss: 103.19441\n",
      "Step #41900, epoch #10475, avg. train loss: 103.72624\n",
      "Step #42000, epoch #10500, avg. train loss: 102.07475\n",
      "Step #42100, epoch #10525, avg. train loss: 102.70590\n",
      "Step #42200, epoch #10550, avg. train loss: 103.67241\n",
      "Step #42300, epoch #10575, avg. train loss: 102.59718\n",
      "Step #42400, epoch #10600, avg. train loss: 101.88747\n",
      "Step #42500, epoch #10625, avg. train loss: 101.70645\n",
      "Step #42600, epoch #10650, avg. train loss: 102.22070\n",
      "Step #42700, epoch #10675, avg. train loss: 102.72733\n",
      "Step #42800, epoch #10700, avg. train loss: 101.81205\n",
      "Step #42900, epoch #10725, avg. train loss: 101.72329\n",
      "Step #43000, epoch #10750, avg. train loss: 101.40014\n",
      "Step #43100, epoch #10775, avg. train loss: 101.60911\n",
      "Step #43200, epoch #10800, avg. train loss: 101.67024\n",
      "Step #43300, epoch #10825, avg. train loss: 102.36997\n",
      "Step #43400, epoch #10850, avg. train loss: 101.13986\n",
      "Step #43500, epoch #10875, avg. train loss: 101.69986\n",
      "Step #43600, epoch #10900, avg. train loss: 100.84488\n",
      "Step #43700, epoch #10925, avg. train loss: 101.35703\n",
      "Step #43800, epoch #10950, avg. train loss: 101.31504\n",
      "Step #43900, epoch #10975, avg. train loss: 101.73969\n",
      "Step #44000, epoch #11000, avg. train loss: 101.32209\n",
      "Step #44100, epoch #11025, avg. train loss: 101.32495\n",
      "Step #44200, epoch #11050, avg. train loss: 100.52298\n",
      "Step #44300, epoch #11075, avg. train loss: 101.22343\n",
      "Step #44400, epoch #11100, avg. train loss: 101.83582\n",
      "Step #44500, epoch #11125, avg. train loss: 100.85041\n",
      "Step #44600, epoch #11150, avg. train loss: 102.63437\n",
      "Step #44700, epoch #11175, avg. train loss: 100.42900\n",
      "Step #44800, epoch #11200, avg. train loss: 100.45180\n",
      "Step #44900, epoch #11225, avg. train loss: 101.23797\n",
      "Step #45000, epoch #11250, avg. train loss: 101.25843\n",
      "Step #45100, epoch #11275, avg. train loss: 100.79151\n",
      "Step #45200, epoch #11300, avg. train loss: 101.01425\n",
      "Step #45300, epoch #11325, avg. train loss: 100.32787\n",
      "Step #45400, epoch #11350, avg. train loss: 100.79747\n",
      "Step #45500, epoch #11375, avg. train loss: 100.94985\n",
      "Step #45600, epoch #11400, avg. train loss: 100.71490\n",
      "Step #45700, epoch #11425, avg. train loss: 99.17588\n",
      "Step #45800, epoch #11450, avg. train loss: 99.78642\n",
      "Step #45900, epoch #11475, avg. train loss: 100.45322\n",
      "Step #46000, epoch #11500, avg. train loss: 100.33839\n",
      "Step #46100, epoch #11525, avg. train loss: 101.56233\n",
      "Step #46200, epoch #11550, avg. train loss: 100.33099\n",
      "Step #46300, epoch #11575, avg. train loss: 100.20605\n",
      "Step #46400, epoch #11600, avg. train loss: 99.90045\n",
      "Step #46500, epoch #11625, avg. train loss: 99.94360\n",
      "Step #46600, epoch #11650, avg. train loss: 100.17143\n",
      "Step #46700, epoch #11675, avg. train loss: 99.85763\n",
      "Step #46800, epoch #11700, avg. train loss: 98.94346\n",
      "Step #46900, epoch #11725, avg. train loss: 99.12387\n",
      "Step #47000, epoch #11750, avg. train loss: 98.59631\n",
      "Step #47100, epoch #11775, avg. train loss: 99.60547\n",
      "Step #47200, epoch #11800, avg. train loss: 98.86517\n",
      "Step #47300, epoch #11825, avg. train loss: 99.66671\n",
      "Step #47400, epoch #11850, avg. train loss: 99.74767\n",
      "Step #47500, epoch #11875, avg. train loss: 99.39695\n",
      "Step #47600, epoch #11900, avg. train loss: 99.54726\n",
      "Step #47700, epoch #11925, avg. train loss: 99.19661\n",
      "Step #47800, epoch #11950, avg. train loss: 98.91961\n",
      "Step #47900, epoch #11975, avg. train loss: 99.01354\n",
      "Step #48000, epoch #12000, avg. train loss: 99.26183\n",
      "Step #48100, epoch #12025, avg. train loss: 99.65548\n",
      "Step #48200, epoch #12050, avg. train loss: 99.16788\n",
      "Step #48300, epoch #12075, avg. train loss: 99.66449\n",
      "Step #48400, epoch #12100, avg. train loss: 98.80768\n",
      "Step #48500, epoch #12125, avg. train loss: 98.90768\n",
      "Step #48600, epoch #12150, avg. train loss: 98.84883\n",
      "Step #48700, epoch #12175, avg. train loss: 99.13609\n",
      "Step #48800, epoch #12200, avg. train loss: 98.58719\n",
      "Step #48900, epoch #12225, avg. train loss: 98.96067\n",
      "Step #49000, epoch #12250, avg. train loss: 98.73061\n",
      "Step #49100, epoch #12275, avg. train loss: 98.53304\n",
      "Step #49200, epoch #12300, avg. train loss: 97.93489\n",
      "Step #49300, epoch #12325, avg. train loss: 98.35491\n",
      "Step #49400, epoch #12350, avg. train loss: 98.16897\n",
      "Step #49500, epoch #12375, avg. train loss: 98.02085\n",
      "Step #49600, epoch #12400, avg. train loss: 98.51538\n",
      "Step #49700, epoch #12425, avg. train loss: 98.76161\n",
      "Step #49800, epoch #12450, avg. train loss: 98.21274\n",
      "Step #49900, epoch #12475, avg. train loss: 98.17931\n",
      "Step #50000, epoch #12500, avg. train loss: 97.71651\n",
      "Step #50100, epoch #12525, avg. train loss: 97.68137\n",
      "Step #50200, epoch #12550, avg. train loss: 98.10030\n",
      "Step #50300, epoch #12575, avg. train loss: 97.94169\n",
      "Step #50400, epoch #12600, avg. train loss: 97.77425\n",
      "Step #50500, epoch #12625, avg. train loss: 96.78813\n",
      "Step #50600, epoch #12650, avg. train loss: 97.48827\n",
      "Step #50700, epoch #12675, avg. train loss: 98.26260\n",
      "Step #50800, epoch #12700, avg. train loss: 97.99799\n",
      "Step #50900, epoch #12725, avg. train loss: 98.28168\n",
      "Step #51000, epoch #12750, avg. train loss: 98.23298\n",
      "Step #51100, epoch #12775, avg. train loss: 98.01143\n",
      "Step #51200, epoch #12800, avg. train loss: 97.65903\n",
      "Step #51300, epoch #12825, avg. train loss: 97.88254\n",
      "Step #51400, epoch #12850, avg. train loss: 97.10213\n",
      "Step #51500, epoch #12875, avg. train loss: 97.14243\n",
      "Step #51600, epoch #12900, avg. train loss: 97.57459\n",
      "Step #51700, epoch #12925, avg. train loss: 97.37663\n",
      "Step #51800, epoch #12950, avg. train loss: 98.04880\n",
      "Step #51900, epoch #12975, avg. train loss: 96.51437\n",
      "Step #52000, epoch #13000, avg. train loss: 97.00764\n",
      "Step #52100, epoch #13025, avg. train loss: 97.13445\n",
      "Step #52200, epoch #13050, avg. train loss: 97.41512\n",
      "Step #52300, epoch #13075, avg. train loss: 96.89007\n",
      "Step #52400, epoch #13100, avg. train loss: 96.38750\n",
      "Step #52500, epoch #13125, avg. train loss: 97.73563\n",
      "Step #52600, epoch #13150, avg. train loss: 97.45479\n",
      "Step #52700, epoch #13175, avg. train loss: 96.76556\n",
      "Step #52800, epoch #13200, avg. train loss: 97.44203\n",
      "Step #52900, epoch #13225, avg. train loss: 96.53038\n",
      "Step #53000, epoch #13250, avg. train loss: 96.24986\n",
      "Step #53100, epoch #13275, avg. train loss: 95.80859\n",
      "Step #53200, epoch #13300, avg. train loss: 95.28251\n",
      "Step #53300, epoch #13325, avg. train loss: 96.61530\n",
      "Step #53400, epoch #13350, avg. train loss: 97.49048\n",
      "Step #53500, epoch #13375, avg. train loss: 96.32969\n",
      "Step #53600, epoch #13400, avg. train loss: 96.46069\n",
      "Step #53700, epoch #13425, avg. train loss: 96.34445\n",
      "Step #53800, epoch #13450, avg. train loss: 97.70205\n",
      "Step #53900, epoch #13475, avg. train loss: 97.11845\n",
      "Step #54000, epoch #13500, avg. train loss: 97.09615\n",
      "Step #54100, epoch #13525, avg. train loss: 96.62775\n",
      "Step #54200, epoch #13550, avg. train loss: 96.26649\n",
      "Step #54300, epoch #13575, avg. train loss: 96.35649\n",
      "Step #54400, epoch #13600, avg. train loss: 96.62189\n",
      "Step #54500, epoch #13625, avg. train loss: 96.27890\n",
      "Step #54600, epoch #13650, avg. train loss: 95.68429\n",
      "Step #54700, epoch #13675, avg. train loss: 95.82271\n",
      "Step #54800, epoch #13700, avg. train loss: 95.90629\n",
      "Step #54900, epoch #13725, avg. train loss: 96.53713\n",
      "Step #55000, epoch #13750, avg. train loss: 96.31919\n",
      "Step #55100, epoch #13775, avg. train loss: 96.36838\n",
      "Step #55200, epoch #13800, avg. train loss: 96.08858\n",
      "Step #55300, epoch #13825, avg. train loss: 96.52329\n",
      "Step #55400, epoch #13850, avg. train loss: 95.97392\n",
      "Step #55500, epoch #13875, avg. train loss: 95.66323\n",
      "Step #55600, epoch #13900, avg. train loss: 95.27256\n",
      "Step #55700, epoch #13925, avg. train loss: 94.94456\n",
      "Step #55800, epoch #13950, avg. train loss: 95.11704\n",
      "Step #55900, epoch #13975, avg. train loss: 95.26492\n",
      "Step #56000, epoch #14000, avg. train loss: 95.52286\n",
      "Step #56100, epoch #14025, avg. train loss: 95.91525\n",
      "Step #56200, epoch #14050, avg. train loss: 95.66176\n",
      "Step #56300, epoch #14075, avg. train loss: 95.60860\n",
      "Step #56400, epoch #14100, avg. train loss: 95.54021\n",
      "Step #56500, epoch #14125, avg. train loss: 95.91805\n",
      "Step #56600, epoch #14150, avg. train loss: 94.95529\n",
      "Step #56700, epoch #14175, avg. train loss: 95.95090\n",
      "Step #56800, epoch #14200, avg. train loss: 96.42371\n",
      "Step #56900, epoch #14225, avg. train loss: 95.42512\n",
      "Step #57000, epoch #14250, avg. train loss: 94.93269\n",
      "Step #57100, epoch #14275, avg. train loss: 94.90570\n",
      "Step #57200, epoch #14300, avg. train loss: 95.28149\n",
      "Step #57300, epoch #14325, avg. train loss: 95.38898\n",
      "Step #57400, epoch #14350, avg. train loss: 96.28336\n",
      "Step #57500, epoch #14375, avg. train loss: 95.94756\n",
      "Step #57600, epoch #14400, avg. train loss: 94.50118\n",
      "Step #57700, epoch #14425, avg. train loss: 95.72680\n",
      "Step #57800, epoch #14450, avg. train loss: 95.45971\n",
      "Step #57900, epoch #14475, avg. train loss: 95.09677\n",
      "Step #58000, epoch #14500, avg. train loss: 94.53027\n",
      "Step #58100, epoch #14525, avg. train loss: 95.39497\n",
      "Step #58200, epoch #14550, avg. train loss: 95.20710\n",
      "Step #58300, epoch #14575, avg. train loss: 95.55693\n",
      "Step #58400, epoch #14600, avg. train loss: 94.88685\n",
      "Step #58500, epoch #14625, avg. train loss: 94.75918\n",
      "Step #58600, epoch #14650, avg. train loss: 93.89707\n",
      "Step #58700, epoch #14675, avg. train loss: 95.23049\n",
      "Step #58800, epoch #14700, avg. train loss: 95.89545\n",
      "Step #58900, epoch #14725, avg. train loss: 94.95191\n",
      "Step #59000, epoch #14750, avg. train loss: 93.97137\n",
      "Step #59100, epoch #14775, avg. train loss: 93.79411\n",
      "Step #59200, epoch #14800, avg. train loss: 95.32621\n",
      "Step #59300, epoch #14825, avg. train loss: 95.55630\n",
      "Step #59400, epoch #14850, avg. train loss: 95.12753\n",
      "Step #59500, epoch #14875, avg. train loss: 94.65585\n",
      "Step #59600, epoch #14900, avg. train loss: 95.22115\n",
      "Step #59700, epoch #14925, avg. train loss: 95.53090\n",
      "Step #59800, epoch #14950, avg. train loss: 94.53089\n",
      "Step #59900, epoch #14975, avg. train loss: 93.58102\n",
      "Step #60000, epoch #15000, avg. train loss: 93.72334\n",
      "Step #60100, epoch #15025, avg. train loss: 94.73219\n",
      "Step #60200, epoch #15050, avg. train loss: 95.30740\n",
      "Step #60300, epoch #15075, avg. train loss: 93.64524\n",
      "Step #60400, epoch #15100, avg. train loss: 94.16339\n",
      "Step #60500, epoch #15125, avg. train loss: 94.25311\n",
      "Step #60600, epoch #15150, avg. train loss: 94.62975\n",
      "Step #60700, epoch #15175, avg. train loss: 95.00949\n",
      "Step #60800, epoch #15200, avg. train loss: 94.73206\n",
      "Step #60900, epoch #15225, avg. train loss: 94.80518\n",
      "Step #61000, epoch #15250, avg. train loss: 94.59164\n",
      "Step #61100, epoch #15275, avg. train loss: 95.21254\n",
      "Step #61200, epoch #15300, avg. train loss: 94.20821\n",
      "Step #61300, epoch #15325, avg. train loss: 94.26628\n",
      "Step #61400, epoch #15350, avg. train loss: 94.83890\n",
      "Step #61500, epoch #15375, avg. train loss: 95.01678\n",
      "Step #61600, epoch #15400, avg. train loss: 94.05806\n",
      "Step #61700, epoch #15425, avg. train loss: 93.52034\n",
      "Step #61800, epoch #15450, avg. train loss: 93.47269\n",
      "Step #61900, epoch #15475, avg. train loss: 94.30793\n",
      "Step #62000, epoch #15500, avg. train loss: 93.88989\n",
      "Step #62100, epoch #15525, avg. train loss: 95.14366\n",
      "Step #62200, epoch #15550, avg. train loss: 94.42654\n",
      "Step #62300, epoch #15575, avg. train loss: 93.61308\n",
      "Step #62400, epoch #15600, avg. train loss: 93.73852\n",
      "Step #62500, epoch #15625, avg. train loss: 94.38374\n",
      "Step #62600, epoch #15650, avg. train loss: 93.27359\n",
      "Step #62700, epoch #15675, avg. train loss: 93.79729\n",
      "Step #62800, epoch #15700, avg. train loss: 93.58015\n",
      "Step #62900, epoch #15725, avg. train loss: 94.13909\n",
      "Step #63000, epoch #15750, avg. train loss: 94.68160\n",
      "Step #63100, epoch #15775, avg. train loss: 94.54990\n",
      "Step #63200, epoch #15800, avg. train loss: 94.14746\n",
      "Step #63300, epoch #15825, avg. train loss: 94.37996\n",
      "Step #63400, epoch #15850, avg. train loss: 94.99232\n",
      "Step #63500, epoch #15875, avg. train loss: 93.54015\n",
      "Step #63600, epoch #15900, avg. train loss: 94.63552\n",
      "Step #63700, epoch #15925, avg. train loss: 93.30782\n",
      "Step #63800, epoch #15950, avg. train loss: 94.28452\n",
      "Step #63900, epoch #15975, avg. train loss: 94.69982\n",
      "Step #64000, epoch #16000, avg. train loss: 93.78365\n",
      "Step #64100, epoch #16025, avg. train loss: 94.04983\n",
      "Step #64200, epoch #16050, avg. train loss: 93.30235\n",
      "Step #64300, epoch #16075, avg. train loss: 93.76367\n",
      "Step #64400, epoch #16100, avg. train loss: 93.02200\n",
      "Step #64500, epoch #16125, avg. train loss: 94.15622\n",
      "Step #64600, epoch #16150, avg. train loss: 93.51939\n",
      "Step #64700, epoch #16175, avg. train loss: 93.54314\n",
      "Step #64800, epoch #16200, avg. train loss: 93.68906\n",
      "Step #64900, epoch #16225, avg. train loss: 93.36369\n",
      "Step #65000, epoch #16250, avg. train loss: 94.04771\n",
      "Step #65100, epoch #16275, avg. train loss: 93.49258\n",
      "Step #65200, epoch #16300, avg. train loss: 93.53327\n",
      "Step #65300, epoch #16325, avg. train loss: 93.83154\n",
      "Step #65400, epoch #16350, avg. train loss: 93.58798\n",
      "Step #65500, epoch #16375, avg. train loss: 94.12886\n",
      "Step #65600, epoch #16400, avg. train loss: 93.17934\n",
      "Step #65700, epoch #16425, avg. train loss: 92.65539\n",
      "Step #65800, epoch #16450, avg. train loss: 93.32568\n",
      "Step #65900, epoch #16475, avg. train loss: 93.44831\n",
      "Step #66000, epoch #16500, avg. train loss: 92.94030\n",
      "Step #66100, epoch #16525, avg. train loss: 94.03131\n",
      "Step #66200, epoch #16550, avg. train loss: 93.69615\n",
      "Step #66300, epoch #16575, avg. train loss: 93.28749\n",
      "Step #66400, epoch #16600, avg. train loss: 94.05126\n",
      "Step #66500, epoch #16625, avg. train loss: 92.07038\n",
      "Step #66600, epoch #16650, avg. train loss: 93.00171\n",
      "Step #66700, epoch #16675, avg. train loss: 93.45361\n",
      "Step #66800, epoch #16700, avg. train loss: 94.42341\n",
      "Step #66900, epoch #16725, avg. train loss: 93.97778\n",
      "Step #67000, epoch #16750, avg. train loss: 93.31244\n",
      "Step #67100, epoch #16775, avg. train loss: 92.81854\n",
      "Step #67200, epoch #16800, avg. train loss: 93.16869\n",
      "Step #67300, epoch #16825, avg. train loss: 92.85242\n",
      "Step #67400, epoch #16850, avg. train loss: 92.29190\n",
      "Step #67500, epoch #16875, avg. train loss: 92.87891\n",
      "Step #67600, epoch #16900, avg. train loss: 92.34402\n",
      "Step #67700, epoch #16925, avg. train loss: 92.67912\n",
      "Step #67800, epoch #16950, avg. train loss: 92.98386\n",
      "Step #67900, epoch #16975, avg. train loss: 93.37320\n",
      "Step #68000, epoch #17000, avg. train loss: 93.09255\n",
      "Step #68100, epoch #17025, avg. train loss: 92.88013\n",
      "Step #68200, epoch #17050, avg. train loss: 92.88258\n",
      "Step #68300, epoch #17075, avg. train loss: 92.28204\n",
      "Step #68400, epoch #17100, avg. train loss: 92.06218\n",
      "Step #68500, epoch #17125, avg. train loss: 91.75594\n",
      "Step #68600, epoch #17150, avg. train loss: 92.72105\n",
      "Step #68700, epoch #17175, avg. train loss: 93.02833\n",
      "Step #68800, epoch #17200, avg. train loss: 93.33872\n",
      "Step #68900, epoch #17225, avg. train loss: 92.80595\n",
      "Step #69000, epoch #17250, avg. train loss: 92.73449\n",
      "Step #69100, epoch #17275, avg. train loss: 94.01877\n",
      "Step #69200, epoch #17300, avg. train loss: 93.28185\n",
      "Step #69300, epoch #17325, avg. train loss: 92.37305\n",
      "Step #69400, epoch #17350, avg. train loss: 92.61454\n",
      "Step #69500, epoch #17375, avg. train loss: 92.93128\n",
      "Step #69600, epoch #17400, avg. train loss: 92.58036\n",
      "Step #69700, epoch #17425, avg. train loss: 93.42310\n",
      "Step #69800, epoch #17450, avg. train loss: 93.30463\n",
      "Step #69900, epoch #17475, avg. train loss: 93.29562\n",
      "Step #70000, epoch #17500, avg. train loss: 92.94651\n",
      "Step #70100, epoch #17525, avg. train loss: 92.02180\n",
      "Step #70200, epoch #17550, avg. train loss: 92.65136\n",
      "Step #70300, epoch #17575, avg. train loss: 92.51378\n",
      "Step #70400, epoch #17600, avg. train loss: 92.62779\n",
      "Step #70500, epoch #17625, avg. train loss: 92.43363\n",
      "Step #70600, epoch #17650, avg. train loss: 93.54450\n",
      "Step #70700, epoch #17675, avg. train loss: 92.28474\n",
      "Step #70800, epoch #17700, avg. train loss: 93.23374\n",
      "Step #70900, epoch #17725, avg. train loss: 92.20615\n",
      "Step #71000, epoch #17750, avg. train loss: 91.95100\n",
      "Step #71100, epoch #17775, avg. train loss: 91.21565\n",
      "Step #71200, epoch #17800, avg. train loss: 92.59760\n",
      "Step #71300, epoch #17825, avg. train loss: 92.67152\n",
      "Step #71400, epoch #17850, avg. train loss: 93.16388\n",
      "Step #71500, epoch #17875, avg. train loss: 92.49742\n",
      "Step #71600, epoch #17900, avg. train loss: 93.43303\n",
      "Step #71700, epoch #17925, avg. train loss: 92.83462\n",
      "Step #71800, epoch #17950, avg. train loss: 91.88572\n",
      "Step #71900, epoch #17975, avg. train loss: 92.54957\n",
      "Step #72000, epoch #18000, avg. train loss: 91.69451\n",
      "Step #72100, epoch #18025, avg. train loss: 92.86513\n",
      "Step #72200, epoch #18050, avg. train loss: 91.34280\n",
      "Step #72300, epoch #18075, avg. train loss: 91.08169\n",
      "Step #72400, epoch #18100, avg. train loss: 92.93761\n",
      "Step #72500, epoch #18125, avg. train loss: 90.91196\n",
      "Step #72600, epoch #18150, avg. train loss: 92.21703\n",
      "Step #72700, epoch #18175, avg. train loss: 92.07195\n",
      "Step #72800, epoch #18200, avg. train loss: 92.31619\n",
      "Step #72900, epoch #18225, avg. train loss: 91.38110\n",
      "Step #73000, epoch #18250, avg. train loss: 91.45066\n",
      "Step #73100, epoch #18275, avg. train loss: 91.55476\n",
      "Step #73200, epoch #18300, avg. train loss: 92.29209\n",
      "Step #73300, epoch #18325, avg. train loss: 91.29552\n",
      "Step #73400, epoch #18350, avg. train loss: 91.49010\n",
      "Step #73500, epoch #18375, avg. train loss: 91.81075\n",
      "Step #73600, epoch #18400, avg. train loss: 92.54784\n",
      "Step #73700, epoch #18425, avg. train loss: 91.25189\n",
      "Step #73800, epoch #18450, avg. train loss: 92.68177\n",
      "Step #73900, epoch #18475, avg. train loss: 92.17694\n",
      "Step #74000, epoch #18500, avg. train loss: 91.49453\n",
      "Step #74100, epoch #18525, avg. train loss: 92.47179\n",
      "Step #74200, epoch #18550, avg. train loss: 91.72868\n",
      "Step #74300, epoch #18575, avg. train loss: 91.86451\n",
      "Step #74400, epoch #18600, avg. train loss: 91.99490\n",
      "Step #74500, epoch #18625, avg. train loss: 91.45390\n",
      "Step #74600, epoch #18650, avg. train loss: 91.32366\n",
      "Step #74700, epoch #18675, avg. train loss: 92.77245\n",
      "Step #74800, epoch #18700, avg. train loss: 91.47162\n",
      "Step #74900, epoch #18725, avg. train loss: 92.05240\n",
      "Step #75000, epoch #18750, avg. train loss: 91.74991\n",
      "Step #75100, epoch #18775, avg. train loss: 92.14846\n",
      "Step #75200, epoch #18800, avg. train loss: 91.89520\n",
      "Step #75300, epoch #18825, avg. train loss: 90.66762\n",
      "Step #75400, epoch #18850, avg. train loss: 91.76991\n",
      "Step #75500, epoch #18875, avg. train loss: 91.11777\n",
      "Step #75600, epoch #18900, avg. train loss: 92.25903\n",
      "Step #75700, epoch #18925, avg. train loss: 91.55905\n",
      "Step #75800, epoch #18950, avg. train loss: 91.93594\n",
      "Step #75900, epoch #18975, avg. train loss: 92.07432\n",
      "Step #76000, epoch #19000, avg. train loss: 91.61848\n",
      "Step #76100, epoch #19025, avg. train loss: 91.54385\n",
      "Step #76200, epoch #19050, avg. train loss: 91.76266\n",
      "Step #76300, epoch #19075, avg. train loss: 91.16635\n",
      "Step #76400, epoch #19100, avg. train loss: 91.74595\n",
      "Step #76500, epoch #19125, avg. train loss: 91.89037\n",
      "Step #76600, epoch #19150, avg. train loss: 91.94586\n",
      "Step #76700, epoch #19175, avg. train loss: 90.73855\n",
      "Step #76800, epoch #19200, avg. train loss: 91.43950\n",
      "Step #76900, epoch #19225, avg. train loss: 91.16179\n",
      "Step #77000, epoch #19250, avg. train loss: 91.13490\n",
      "Step #77100, epoch #19275, avg. train loss: 90.70071\n",
      "Step #77200, epoch #19300, avg. train loss: 91.02396\n",
      "Step #77300, epoch #19325, avg. train loss: 92.70779\n",
      "Step #77400, epoch #19350, avg. train loss: 92.23625\n",
      "Step #77500, epoch #19375, avg. train loss: 91.37665\n",
      "Step #77600, epoch #19400, avg. train loss: 91.63415\n",
      "Step #77700, epoch #19425, avg. train loss: 91.12979\n",
      "Step #77800, epoch #19450, avg. train loss: 91.38263\n",
      "Step #77900, epoch #19475, avg. train loss: 91.53226\n",
      "Step #78000, epoch #19500, avg. train loss: 91.98975\n",
      "Step #78100, epoch #19525, avg. train loss: 91.55232\n",
      "Step #78200, epoch #19550, avg. train loss: 91.22116\n",
      "Step #78300, epoch #19575, avg. train loss: 91.52026\n",
      "Step #78400, epoch #19600, avg. train loss: 91.09608\n",
      "Step #78500, epoch #19625, avg. train loss: 91.34309\n",
      "Step #78600, epoch #19650, avg. train loss: 91.38903\n",
      "Step #78700, epoch #19675, avg. train loss: 91.23843\n",
      "Step #78800, epoch #19700, avg. train loss: 90.17938\n",
      "Step #78900, epoch #19725, avg. train loss: 91.36533\n",
      "Step #79000, epoch #19750, avg. train loss: 90.66113\n",
      "Step #79100, epoch #19775, avg. train loss: 90.70274\n",
      "Step #79200, epoch #19800, avg. train loss: 91.65577\n",
      "Step #79300, epoch #19825, avg. train loss: 90.48012\n",
      "Step #79400, epoch #19850, avg. train loss: 90.95672\n",
      "Step #79500, epoch #19875, avg. train loss: 90.96899\n",
      "Step #79600, epoch #19900, avg. train loss: 91.05925\n",
      "Step #79700, epoch #19925, avg. train loss: 90.94162\n",
      "Step #79800, epoch #19950, avg. train loss: 91.52651\n",
      "Step #79900, epoch #19975, avg. train loss: 91.42800\n",
      "Step #80000, epoch #20000, avg. train loss: 91.48646\n",
      "Step #80100, epoch #20025, avg. train loss: 91.04397\n",
      "Step #80200, epoch #20050, avg. train loss: 90.89324\n",
      "Step #80300, epoch #20075, avg. train loss: 90.72886\n",
      "Step #80400, epoch #20100, avg. train loss: 91.30737\n",
      "Step #80500, epoch #20125, avg. train loss: 90.04807\n",
      "Step #80600, epoch #20150, avg. train loss: 91.23148\n",
      "Step #80700, epoch #20175, avg. train loss: 91.33427\n",
      "Step #80800, epoch #20200, avg. train loss: 90.66149\n",
      "Step #80900, epoch #20225, avg. train loss: 90.47382\n",
      "Step #81000, epoch #20250, avg. train loss: 91.12290\n",
      "Step #81100, epoch #20275, avg. train loss: 90.74599\n",
      "Step #81200, epoch #20300, avg. train loss: 90.90304\n",
      "Step #81300, epoch #20325, avg. train loss: 90.29397\n",
      "Step #81400, epoch #20350, avg. train loss: 90.02686\n",
      "Step #81500, epoch #20375, avg. train loss: 90.81723\n",
      "Step #81600, epoch #20400, avg. train loss: 90.78751\n",
      "Step #81700, epoch #20425, avg. train loss: 90.98001\n",
      "Step #81800, epoch #20450, avg. train loss: 90.46780\n",
      "Step #81900, epoch #20475, avg. train loss: 90.42017\n",
      "Step #82000, epoch #20500, avg. train loss: 91.15493\n",
      "Step #82100, epoch #20525, avg. train loss: 90.32590\n",
      "Step #82200, epoch #20550, avg. train loss: 90.67127\n",
      "Step #82300, epoch #20575, avg. train loss: 90.86879\n",
      "Step #82400, epoch #20600, avg. train loss: 91.12123\n",
      "Step #82500, epoch #20625, avg. train loss: 90.49728\n",
      "Step #82600, epoch #20650, avg. train loss: 90.89125\n",
      "Step #82700, epoch #20675, avg. train loss: 90.26731\n",
      "Step #82800, epoch #20700, avg. train loss: 90.38184\n",
      "Step #82900, epoch #20725, avg. train loss: 91.81851\n",
      "Step #83000, epoch #20750, avg. train loss: 90.13037\n",
      "Step #83100, epoch #20775, avg. train loss: 90.93934\n",
      "Step #83200, epoch #20800, avg. train loss: 90.17393\n",
      "Step #83300, epoch #20825, avg. train loss: 90.31919\n",
      "Step #83400, epoch #20850, avg. train loss: 90.89680\n",
      "Step #83500, epoch #20875, avg. train loss: 91.23731\n",
      "Step #83600, epoch #20900, avg. train loss: 91.24005\n",
      "Step #83700, epoch #20925, avg. train loss: 90.65064\n",
      "Step #83800, epoch #20950, avg. train loss: 90.15576\n",
      "Step #83900, epoch #20975, avg. train loss: 90.40329\n",
      "Step #84000, epoch #21000, avg. train loss: 90.46882\n",
      "Step #84100, epoch #21025, avg. train loss: 90.66068\n",
      "Step #84200, epoch #21050, avg. train loss: 90.05362\n",
      "Step #84300, epoch #21075, avg. train loss: 92.02126\n",
      "Step #84400, epoch #21100, avg. train loss: 90.33131\n",
      "Step #84500, epoch #21125, avg. train loss: 90.62840\n",
      "Step #84600, epoch #21150, avg. train loss: 90.33192\n",
      "Step #84700, epoch #21175, avg. train loss: 90.60720\n",
      "Step #84800, epoch #21200, avg. train loss: 90.11341\n",
      "Step #84900, epoch #21225, avg. train loss: 89.57168\n",
      "Step #85000, epoch #21250, avg. train loss: 90.45399\n",
      "Step #85100, epoch #21275, avg. train loss: 89.43838\n",
      "Step #85200, epoch #21300, avg. train loss: 90.35035\n",
      "Step #85300, epoch #21325, avg. train loss: 90.49363\n",
      "Step #85400, epoch #21350, avg. train loss: 89.85754\n",
      "Step #85500, epoch #21375, avg. train loss: 90.12631\n",
      "Step #85600, epoch #21400, avg. train loss: 90.37663\n",
      "Step #85700, epoch #21425, avg. train loss: 90.68579\n",
      "Step #85800, epoch #21450, avg. train loss: 89.69003\n",
      "Step #85900, epoch #21475, avg. train loss: 89.80856\n",
      "Step #86000, epoch #21500, avg. train loss: 90.03781\n",
      "Step #86100, epoch #21525, avg. train loss: 90.37859\n",
      "Step #86200, epoch #21550, avg. train loss: 89.48032\n",
      "Step #86300, epoch #21575, avg. train loss: 90.15932\n",
      "Step #86400, epoch #21600, avg. train loss: 91.17636\n",
      "Step #86500, epoch #21625, avg. train loss: 89.75422\n",
      "Step #86600, epoch #21650, avg. train loss: 90.03072\n",
      "Step #86700, epoch #21675, avg. train loss: 90.30166\n",
      "Step #86800, epoch #21700, avg. train loss: 90.83122\n",
      "Step #86900, epoch #21725, avg. train loss: 89.88264\n",
      "Step #87000, epoch #21750, avg. train loss: 90.49328\n",
      "Step #87100, epoch #21775, avg. train loss: 89.75883\n",
      "Step #87200, epoch #21800, avg. train loss: 90.09233\n",
      "Step #87300, epoch #21825, avg. train loss: 90.65475\n",
      "Step #87400, epoch #21850, avg. train loss: 89.11709\n",
      "Step #87500, epoch #21875, avg. train loss: 90.94790\n",
      "Step #87600, epoch #21900, avg. train loss: 90.37225\n",
      "Step #87700, epoch #21925, avg. train loss: 89.94734\n",
      "Step #87800, epoch #21950, avg. train loss: 90.65785\n",
      "Step #87900, epoch #21975, avg. train loss: 90.37367\n",
      "Step #88000, epoch #22000, avg. train loss: 90.13028\n",
      "Step #88100, epoch #22025, avg. train loss: 90.06045\n",
      "Step #88200, epoch #22050, avg. train loss: 90.50704\n",
      "Step #88300, epoch #22075, avg. train loss: 90.82735\n",
      "Step #88400, epoch #22100, avg. train loss: 90.57878\n",
      "Step #88500, epoch #22125, avg. train loss: 90.35525\n",
      "Step #88600, epoch #22150, avg. train loss: 90.30529\n",
      "Step #88700, epoch #22175, avg. train loss: 90.33305\n",
      "Step #88800, epoch #22200, avg. train loss: 90.16743\n",
      "Step #88900, epoch #22225, avg. train loss: 89.17265\n",
      "Step #89000, epoch #22250, avg. train loss: 89.46677\n",
      "Step #89100, epoch #22275, avg. train loss: 90.44096\n",
      "Step #89200, epoch #22300, avg. train loss: 89.51872\n",
      "Step #89300, epoch #22325, avg. train loss: 89.16644\n",
      "Step #89400, epoch #22350, avg. train loss: 89.75296\n",
      "Step #89500, epoch #22375, avg. train loss: 90.02976\n",
      "Step #89600, epoch #22400, avg. train loss: 89.30875\n",
      "Step #89700, epoch #22425, avg. train loss: 89.40527\n",
      "Step #89800, epoch #22450, avg. train loss: 89.71464\n",
      "Step #89900, epoch #22475, avg. train loss: 90.43730\n",
      "Step #90000, epoch #22500, avg. train loss: 89.79043\n",
      "Step #90100, epoch #22525, avg. train loss: 89.16822\n",
      "Step #90200, epoch #22550, avg. train loss: 88.89227\n",
      "Step #90300, epoch #22575, avg. train loss: 89.29692\n",
      "Step #90400, epoch #22600, avg. train loss: 90.58867\n",
      "Step #90500, epoch #22625, avg. train loss: 90.23208\n",
      "Step #90600, epoch #22650, avg. train loss: 89.55168\n",
      "Step #90700, epoch #22675, avg. train loss: 89.66254\n",
      "Step #90800, epoch #22700, avg. train loss: 89.77956\n",
      "Step #90900, epoch #22725, avg. train loss: 89.95129\n",
      "Step #91000, epoch #22750, avg. train loss: 88.85899\n",
      "Step #91100, epoch #22775, avg. train loss: 89.99647\n",
      "Step #91200, epoch #22800, avg. train loss: 89.68498\n",
      "Step #91300, epoch #22825, avg. train loss: 90.27274\n",
      "Step #91400, epoch #22850, avg. train loss: 89.49337\n",
      "Step #91500, epoch #22875, avg. train loss: 89.49745\n",
      "Step #91600, epoch #22900, avg. train loss: 89.13336\n",
      "Step #91700, epoch #22925, avg. train loss: 89.18603\n",
      "Step #91800, epoch #22950, avg. train loss: 89.66338\n",
      "Step #91900, epoch #22975, avg. train loss: 89.61761\n",
      "Step #92000, epoch #23000, avg. train loss: 89.25998\n",
      "Step #92100, epoch #23025, avg. train loss: 89.37531\n",
      "Step #92200, epoch #23050, avg. train loss: 89.73215\n",
      "Step #92300, epoch #23075, avg. train loss: 89.08196\n",
      "Step #92400, epoch #23100, avg. train loss: 89.89471\n",
      "Step #92500, epoch #23125, avg. train loss: 90.05014\n",
      "Step #92600, epoch #23150, avg. train loss: 90.01375\n",
      "Step #92700, epoch #23175, avg. train loss: 88.60469\n",
      "Step #92800, epoch #23200, avg. train loss: 89.49055\n",
      "Step #92900, epoch #23225, avg. train loss: 89.25041\n",
      "Step #93000, epoch #23250, avg. train loss: 89.95992\n",
      "Step #93100, epoch #23275, avg. train loss: 89.42606\n",
      "Step #93200, epoch #23300, avg. train loss: 89.91041\n",
      "Step #93300, epoch #23325, avg. train loss: 89.78490\n",
      "Step #93400, epoch #23350, avg. train loss: 90.21676\n",
      "Step #93500, epoch #23375, avg. train loss: 89.25020\n",
      "Step #93600, epoch #23400, avg. train loss: 89.54708\n",
      "Step #93700, epoch #23425, avg. train loss: 90.18597\n",
      "Step #93800, epoch #23450, avg. train loss: 90.33696\n",
      "Step #93900, epoch #23475, avg. train loss: 89.20563\n",
      "Step #94000, epoch #23500, avg. train loss: 89.97756\n",
      "Step #94100, epoch #23525, avg. train loss: 89.17400\n",
      "Step #94200, epoch #23550, avg. train loss: 89.96030\n",
      "Step #94300, epoch #23575, avg. train loss: 88.85086\n",
      "Step #94400, epoch #23600, avg. train loss: 89.38022\n",
      "Step #94500, epoch #23625, avg. train loss: 89.52464\n",
      "Step #94600, epoch #23650, avg. train loss: 90.03325\n",
      "Step #94700, epoch #23675, avg. train loss: 88.64748\n",
      "Step #94800, epoch #23700, avg. train loss: 89.45696\n",
      "Step #94900, epoch #23725, avg. train loss: 88.96732\n",
      "Step #95000, epoch #23750, avg. train loss: 89.23309\n",
      "Step #95100, epoch #23775, avg. train loss: 89.47929\n",
      "Step #95200, epoch #23800, avg. train loss: 88.44527\n",
      "Step #95300, epoch #23825, avg. train loss: 89.75143\n",
      "Step #95400, epoch #23850, avg. train loss: 89.73163\n",
      "Step #95500, epoch #23875, avg. train loss: 89.74473\n",
      "Step #95600, epoch #23900, avg. train loss: 89.11561\n",
      "Step #95700, epoch #23925, avg. train loss: 89.82552\n",
      "Step #95800, epoch #23950, avg. train loss: 89.11211\n",
      "Step #95900, epoch #23975, avg. train loss: 88.93981\n",
      "Step #96000, epoch #24000, avg. train loss: 90.53604\n",
      "Step #96100, epoch #24025, avg. train loss: 89.25397\n",
      "Step #96200, epoch #24050, avg. train loss: 90.35583\n",
      "Step #96300, epoch #24075, avg. train loss: 89.25250\n",
      "Step #96400, epoch #24100, avg. train loss: 89.22559\n",
      "Step #96500, epoch #24125, avg. train loss: 88.98322\n",
      "Step #96600, epoch #24150, avg. train loss: 89.15417\n",
      "Step #96700, epoch #24175, avg. train loss: 89.46304\n",
      "Step #96800, epoch #24200, avg. train loss: 88.91103\n",
      "Step #96900, epoch #24225, avg. train loss: 88.70592\n",
      "Step #97000, epoch #24250, avg. train loss: 90.04321\n",
      "Step #97100, epoch #24275, avg. train loss: 89.55989\n",
      "Step #97200, epoch #24300, avg. train loss: 88.43668\n",
      "Step #97300, epoch #24325, avg. train loss: 88.84643\n",
      "Step #97400, epoch #24350, avg. train loss: 89.03501\n",
      "Step #97500, epoch #24375, avg. train loss: 88.82042\n",
      "Step #97600, epoch #24400, avg. train loss: 89.53537\n",
      "Step #97700, epoch #24425, avg. train loss: 89.37415\n",
      "Step #97800, epoch #24450, avg. train loss: 89.16958\n",
      "Step #97900, epoch #24475, avg. train loss: 88.69037\n",
      "Step #98000, epoch #24500, avg. train loss: 88.92139\n",
      "Step #98100, epoch #24525, avg. train loss: 89.53637\n",
      "Step #98200, epoch #24550, avg. train loss: 88.75857\n",
      "Step #98300, epoch #24575, avg. train loss: 88.24907\n",
      "Step #98400, epoch #24600, avg. train loss: 88.55241\n",
      "Step #98500, epoch #24625, avg. train loss: 88.92274\n",
      "Step #98600, epoch #24650, avg. train loss: 89.65628\n",
      "Step #98700, epoch #24675, avg. train loss: 88.82646\n",
      "Step #98800, epoch #24700, avg. train loss: 88.47216\n",
      "Step #98900, epoch #24725, avg. train loss: 88.25572\n",
      "Step #99000, epoch #24750, avg. train loss: 88.79336\n",
      "Step #99100, epoch #24775, avg. train loss: 88.71722\n",
      "Step #99200, epoch #24800, avg. train loss: 88.51884\n",
      "Step #99300, epoch #24825, avg. train loss: 89.54639\n",
      "Step #99400, epoch #24850, avg. train loss: 89.14561\n",
      "Step #99500, epoch #24875, avg. train loss: 89.81443\n",
      "Step #99600, epoch #24900, avg. train loss: 88.99536\n",
      "Step #99700, epoch #24925, avg. train loss: 89.02435\n",
      "Step #99800, epoch #24950, avg. train loss: 88.96719\n",
      "Step #99900, epoch #24975, avg. train loss: 88.40991\n",
      "Step #100000, epoch #25000, avg. train loss: 89.22095\n",
      "Step #100100, epoch #25025, avg. train loss: 88.77738\n",
      "Step #100200, epoch #25050, avg. train loss: 88.00149\n",
      "Step #100300, epoch #25075, avg. train loss: 89.08823\n",
      "Step #100400, epoch #25100, avg. train loss: 89.23742\n",
      "Step #100500, epoch #25125, avg. train loss: 89.59287\n",
      "Step #100600, epoch #25150, avg. train loss: 89.03011\n",
      "Step #100700, epoch #25175, avg. train loss: 89.54781\n",
      "Step #100800, epoch #25200, avg. train loss: 88.81034\n",
      "Step #100900, epoch #25225, avg. train loss: 89.67648\n",
      "Step #101000, epoch #25250, avg. train loss: 89.33609\n",
      "Step #101100, epoch #25275, avg. train loss: 89.07181\n",
      "Step #101200, epoch #25300, avg. train loss: 89.19131\n",
      "Step #101300, epoch #25325, avg. train loss: 88.07949\n",
      "Step #101400, epoch #25350, avg. train loss: 88.33050\n",
      "Step #101500, epoch #25375, avg. train loss: 88.80839\n",
      "Step #101600, epoch #25400, avg. train loss: 87.96955\n",
      "Step #101700, epoch #25425, avg. train loss: 89.11014\n",
      "Step #101800, epoch #25450, avg. train loss: 89.42570\n",
      "Step #101900, epoch #25475, avg. train loss: 88.37192\n",
      "Step #102000, epoch #25500, avg. train loss: 87.99073\n",
      "Step #102100, epoch #25525, avg. train loss: 88.42161\n",
      "Step #102200, epoch #25550, avg. train loss: 88.31294\n",
      "Step #102300, epoch #25575, avg. train loss: 88.76649\n",
      "Step #102400, epoch #25600, avg. train loss: 88.46903\n",
      "Step #102500, epoch #25625, avg. train loss: 89.53351\n",
      "Step #102600, epoch #25650, avg. train loss: 88.74979\n",
      "Step #102700, epoch #25675, avg. train loss: 88.38724\n",
      "Step #102800, epoch #25700, avg. train loss: 89.07461\n",
      "Step #102900, epoch #25725, avg. train loss: 88.59030\n",
      "Step #103000, epoch #25750, avg. train loss: 89.41019\n",
      "Step #103100, epoch #25775, avg. train loss: 89.08269\n",
      "Step #103200, epoch #25800, avg. train loss: 88.46939\n",
      "Step #103300, epoch #25825, avg. train loss: 87.90510\n",
      "Step #103400, epoch #25850, avg. train loss: 88.59013\n",
      "Step #103500, epoch #25875, avg. train loss: 89.33017\n",
      "Step #103600, epoch #25900, avg. train loss: 88.20296\n",
      "Step #103700, epoch #25925, avg. train loss: 89.10497\n",
      "Step #103800, epoch #25950, avg. train loss: 87.43992\n",
      "Step #103900, epoch #25975, avg. train loss: 88.80280\n",
      "Step #104000, epoch #26000, avg. train loss: 88.17812\n",
      "Step #104100, epoch #26025, avg. train loss: 87.39946\n",
      "Step #104200, epoch #26050, avg. train loss: 88.52798\n",
      "Step #104300, epoch #26075, avg. train loss: 88.81829\n",
      "Step #104400, epoch #26100, avg. train loss: 88.33501\n",
      "Step #104500, epoch #26125, avg. train loss: 88.25129\n",
      "Step #104600, epoch #26150, avg. train loss: 89.09880\n",
      "Step #104700, epoch #26175, avg. train loss: 88.09908\n",
      "Step #104800, epoch #26200, avg. train loss: 87.68234\n",
      "Step #104900, epoch #26225, avg. train loss: 88.16266\n",
      "Step #105000, epoch #26250, avg. train loss: 88.16173\n",
      "Step #105100, epoch #26275, avg. train loss: 88.08210\n",
      "Step #105200, epoch #26300, avg. train loss: 88.66437\n",
      "Step #105300, epoch #26325, avg. train loss: 88.34878\n",
      "Step #105400, epoch #26350, avg. train loss: 88.41448\n",
      "Step #105500, epoch #26375, avg. train loss: 88.64773\n",
      "Step #105600, epoch #26400, avg. train loss: 89.01072\n",
      "Step #105700, epoch #26425, avg. train loss: 88.77833\n",
      "Step #105800, epoch #26450, avg. train loss: 87.84217\n",
      "Step #105900, epoch #26475, avg. train loss: 87.79202\n",
      "Step #106000, epoch #26500, avg. train loss: 89.41792\n",
      "Step #106100, epoch #26525, avg. train loss: 87.78241\n",
      "Step #106200, epoch #26550, avg. train loss: 88.13852\n",
      "Step #106300, epoch #26575, avg. train loss: 88.89840\n",
      "Step #106400, epoch #26600, avg. train loss: 88.01453\n",
      "Step #106500, epoch #26625, avg. train loss: 87.96363\n",
      "Step #106600, epoch #26650, avg. train loss: 88.43555\n",
      "Step #106700, epoch #26675, avg. train loss: 88.47966\n",
      "Step #106800, epoch #26700, avg. train loss: 87.85117\n",
      "Step #106900, epoch #26725, avg. train loss: 88.52287\n",
      "Step #107000, epoch #26750, avg. train loss: 88.68017\n",
      "Step #107100, epoch #26775, avg. train loss: 89.24902\n",
      "Step #107200, epoch #26800, avg. train loss: 87.99819\n",
      "Step #107300, epoch #26825, avg. train loss: 87.74635\n",
      "Step #107400, epoch #26850, avg. train loss: 88.57217\n",
      "Step #107500, epoch #26875, avg. train loss: 88.66035\n",
      "Step #107600, epoch #26900, avg. train loss: 87.61631\n",
      "Step #107700, epoch #26925, avg. train loss: 87.61751\n",
      "Step #107800, epoch #26950, avg. train loss: 88.62514\n",
      "Step #107900, epoch #26975, avg. train loss: 88.28661\n",
      "Step #108000, epoch #27000, avg. train loss: 88.97817\n",
      "Step #108100, epoch #27025, avg. train loss: 87.47910\n",
      "Step #108200, epoch #27050, avg. train loss: 87.94947\n",
      "Step #108300, epoch #27075, avg. train loss: 86.81519\n",
      "Step #108400, epoch #27100, avg. train loss: 88.78885\n",
      "Step #108500, epoch #27125, avg. train loss: 88.17629\n",
      "Step #108600, epoch #27150, avg. train loss: 89.12804\n",
      "Step #108700, epoch #27175, avg. train loss: 87.66622\n",
      "Step #108800, epoch #27200, avg. train loss: 88.04266\n",
      "Step #108900, epoch #27225, avg. train loss: 88.10991\n",
      "Step #109000, epoch #27250, avg. train loss: 87.42323\n",
      "Step #109100, epoch #27275, avg. train loss: 87.20012\n",
      "Step #109200, epoch #27300, avg. train loss: 87.93735\n",
      "Step #109300, epoch #27325, avg. train loss: 87.22814\n",
      "Step #109400, epoch #27350, avg. train loss: 88.46853\n",
      "Step #109500, epoch #27375, avg. train loss: 87.67009\n",
      "Step #109600, epoch #27400, avg. train loss: 87.50816\n",
      "Step #109700, epoch #27425, avg. train loss: 87.31898\n",
      "Step #109800, epoch #27450, avg. train loss: 88.04501\n",
      "Step #109900, epoch #27475, avg. train loss: 88.15614\n",
      "Step #110000, epoch #27500, avg. train loss: 88.64133\n",
      "Step #110100, epoch #27525, avg. train loss: 88.38097\n",
      "Step #110200, epoch #27550, avg. train loss: 87.53378\n",
      "Step #110300, epoch #27575, avg. train loss: 87.90237\n",
      "Step #110400, epoch #27600, avg. train loss: 87.97621\n",
      "Step #110500, epoch #27625, avg. train loss: 87.24784\n",
      "Step #110600, epoch #27650, avg. train loss: 88.28718\n",
      "Step #110700, epoch #27675, avg. train loss: 87.95046\n",
      "Step #110800, epoch #27700, avg. train loss: 88.29803\n",
      "Step #110900, epoch #27725, avg. train loss: 88.40331\n",
      "Step #111000, epoch #27750, avg. train loss: 87.77145\n",
      "Step #111100, epoch #27775, avg. train loss: 87.15074\n",
      "Step #111200, epoch #27800, avg. train loss: 88.81687\n",
      "Step #111300, epoch #27825, avg. train loss: 88.01098\n",
      "Step #111400, epoch #27850, avg. train loss: 87.51405\n",
      "Step #111500, epoch #27875, avg. train loss: 88.02435\n",
      "Step #111600, epoch #27900, avg. train loss: 87.94753\n",
      "Step #111700, epoch #27925, avg. train loss: 88.36112\n",
      "Step #111800, epoch #27950, avg. train loss: 87.63347\n",
      "Step #111900, epoch #27975, avg. train loss: 87.91417\n",
      "Step #112000, epoch #28000, avg. train loss: 89.67134\n",
      "Step #112100, epoch #28025, avg. train loss: 87.51406\n",
      "Step #112200, epoch #28050, avg. train loss: 87.85564\n",
      "Step #112300, epoch #28075, avg. train loss: 88.14956\n",
      "Step #112400, epoch #28100, avg. train loss: 87.42204\n",
      "Step #112500, epoch #28125, avg. train loss: 87.21893\n",
      "Step #112600, epoch #28150, avg. train loss: 88.35800\n",
      "Step #112700, epoch #28175, avg. train loss: 87.12583\n",
      "Step #112800, epoch #28200, avg. train loss: 86.76406\n",
      "Step #112900, epoch #28225, avg. train loss: 88.19053\n",
      "Step #113000, epoch #28250, avg. train loss: 87.72230\n",
      "Step #113100, epoch #28275, avg. train loss: 88.47037\n",
      "Step #113200, epoch #28300, avg. train loss: 87.18498\n",
      "Step #113300, epoch #28325, avg. train loss: 87.51680\n",
      "Step #113400, epoch #28350, avg. train loss: 87.60745\n",
      "Step #113500, epoch #28375, avg. train loss: 87.46693\n",
      "Step #113600, epoch #28400, avg. train loss: 88.03693\n",
      "Step #113700, epoch #28425, avg. train loss: 86.96127\n",
      "Step #113800, epoch #28450, avg. train loss: 87.76004\n",
      "Step #113900, epoch #28475, avg. train loss: 87.42221\n",
      "Step #114000, epoch #28500, avg. train loss: 87.65859\n",
      "Step #114100, epoch #28525, avg. train loss: 87.56976\n",
      "Step #114200, epoch #28550, avg. train loss: 87.16396\n",
      "Step #114300, epoch #28575, avg. train loss: 88.39000\n",
      "Step #114400, epoch #28600, avg. train loss: 87.22829\n",
      "Step #114500, epoch #28625, avg. train loss: 87.68378\n",
      "Step #114600, epoch #28650, avg. train loss: 87.15974\n",
      "Step #114700, epoch #28675, avg. train loss: 87.81551\n",
      "Step #114800, epoch #28700, avg. train loss: 87.86458\n",
      "Step #114900, epoch #28725, avg. train loss: 87.54683\n",
      "Step #115000, epoch #28750, avg. train loss: 87.85269\n",
      "Step #115100, epoch #28775, avg. train loss: 88.18143\n",
      "Step #115200, epoch #28800, avg. train loss: 88.33105\n",
      "Step #115300, epoch #28825, avg. train loss: 87.40188\n",
      "Step #115400, epoch #28850, avg. train loss: 87.27686\n",
      "Step #115500, epoch #28875, avg. train loss: 87.94476\n",
      "Step #115600, epoch #28900, avg. train loss: 87.12124\n",
      "Step #115700, epoch #28925, avg. train loss: 87.14002\n",
      "Step #115800, epoch #28950, avg. train loss: 86.80428\n",
      "Step #115900, epoch #28975, avg. train loss: 87.91798\n",
      "Step #116000, epoch #29000, avg. train loss: 87.60839\n",
      "Step #116100, epoch #29025, avg. train loss: 87.43931\n",
      "Step #116200, epoch #29050, avg. train loss: 87.44499\n",
      "Step #116300, epoch #29075, avg. train loss: 87.53869\n",
      "Step #116400, epoch #29100, avg. train loss: 88.25642\n",
      "Step #116500, epoch #29125, avg. train loss: 88.00405\n",
      "Step #116600, epoch #29150, avg. train loss: 86.65830\n",
      "Step #116700, epoch #29175, avg. train loss: 87.85110\n",
      "Step #116800, epoch #29200, avg. train loss: 86.95679\n",
      "Step #116900, epoch #29225, avg. train loss: 87.50500\n",
      "Step #117000, epoch #29250, avg. train loss: 88.19489\n",
      "Step #117100, epoch #29275, avg. train loss: 86.97598\n",
      "Step #117200, epoch #29300, avg. train loss: 88.10229\n",
      "Step #117300, epoch #29325, avg. train loss: 87.62585\n",
      "Step #117400, epoch #29350, avg. train loss: 87.01912\n",
      "Step #117500, epoch #29375, avg. train loss: 87.53815\n",
      "Step #117600, epoch #29400, avg. train loss: 87.76493\n",
      "Step #117700, epoch #29425, avg. train loss: 87.81556\n",
      "Step #117800, epoch #29450, avg. train loss: 87.41312\n",
      "Step #117900, epoch #29475, avg. train loss: 87.18613\n",
      "Step #118000, epoch #29500, avg. train loss: 87.66064\n",
      "Step #118100, epoch #29525, avg. train loss: 88.03491\n",
      "Step #118200, epoch #29550, avg. train loss: 87.01070\n",
      "Step #118300, epoch #29575, avg. train loss: 86.98763\n",
      "Step #118400, epoch #29600, avg. train loss: 87.85546\n",
      "Step #118500, epoch #29625, avg. train loss: 87.58368\n",
      "Step #118600, epoch #29650, avg. train loss: 86.74602\n",
      "Step #118700, epoch #29675, avg. train loss: 88.37756\n",
      "Step #118800, epoch #29700, avg. train loss: 87.16531\n",
      "Step #118900, epoch #29725, avg. train loss: 86.33098\n",
      "Step #119000, epoch #29750, avg. train loss: 86.99574\n",
      "Step #119100, epoch #29775, avg. train loss: 88.17392\n",
      "Step #119200, epoch #29800, avg. train loss: 86.66636\n",
      "Step #119300, epoch #29825, avg. train loss: 87.50331\n",
      "Step #119400, epoch #29850, avg. train loss: 87.01852\n",
      "Step #119500, epoch #29875, avg. train loss: 87.73332\n",
      "Step #119600, epoch #29900, avg. train loss: 86.63578\n",
      "Step #119700, epoch #29925, avg. train loss: 86.60414\n",
      "Step #119800, epoch #29950, avg. train loss: 87.25123\n",
      "Step #119900, epoch #29975, avg. train loss: 87.13675\n",
      "Step #120000, epoch #30000, avg. train loss: 87.11485\n",
      "Step #120100, epoch #30025, avg. train loss: 87.18135\n",
      "Step #120200, epoch #30050, avg. train loss: 88.27557\n",
      "Step #120300, epoch #30075, avg. train loss: 87.54349\n",
      "Step #120400, epoch #30100, avg. train loss: 86.86158\n",
      "Step #120500, epoch #30125, avg. train loss: 86.60356\n",
      "Step #120600, epoch #30150, avg. train loss: 86.77749\n",
      "Step #120700, epoch #30175, avg. train loss: 86.62785\n",
      "Step #120800, epoch #30200, avg. train loss: 86.86966\n",
      "Step #120900, epoch #30225, avg. train loss: 87.49283\n",
      "Step #121000, epoch #30250, avg. train loss: 87.56062\n",
      "Step #121100, epoch #30275, avg. train loss: 87.94534\n",
      "Step #121200, epoch #30300, avg. train loss: 87.56070\n",
      "Step #121300, epoch #30325, avg. train loss: 87.80202\n",
      "Step #121400, epoch #30350, avg. train loss: 87.09916\n",
      "Step #121500, epoch #30375, avg. train loss: 88.02719\n",
      "Step #121600, epoch #30400, avg. train loss: 86.47800\n",
      "Step #121700, epoch #30425, avg. train loss: 87.48521\n",
      "Step #121800, epoch #30450, avg. train loss: 86.04605\n",
      "Step #121900, epoch #30475, avg. train loss: 86.83712\n",
      "Step #122000, epoch #30500, avg. train loss: 87.26321\n",
      "Step #122100, epoch #30525, avg. train loss: 86.66757\n",
      "Step #122200, epoch #30550, avg. train loss: 86.19638\n",
      "Step #122300, epoch #30575, avg. train loss: 87.29723\n",
      "Step #122400, epoch #30600, avg. train loss: 86.85731\n",
      "Step #122500, epoch #30625, avg. train loss: 86.23121\n",
      "Step #122600, epoch #30650, avg. train loss: 87.43794\n",
      "Step #122700, epoch #30675, avg. train loss: 86.89997\n",
      "Step #122800, epoch #30700, avg. train loss: 86.86980\n",
      "Step #122900, epoch #30725, avg. train loss: 87.11742\n",
      "Step #123000, epoch #30750, avg. train loss: 87.39256\n",
      "Step #123100, epoch #30775, avg. train loss: 86.88131\n",
      "Step #123200, epoch #30800, avg. train loss: 87.58160\n",
      "Step #123300, epoch #30825, avg. train loss: 86.60160\n",
      "Step #123400, epoch #30850, avg. train loss: 87.65147\n",
      "Step #123500, epoch #30875, avg. train loss: 87.16197\n",
      "Step #123600, epoch #30900, avg. train loss: 86.71298\n",
      "Step #123700, epoch #30925, avg. train loss: 86.70595\n",
      "Step #123800, epoch #30950, avg. train loss: 87.92375\n",
      "Step #123900, epoch #30975, avg. train loss: 86.70412\n",
      "Step #124000, epoch #31000, avg. train loss: 87.61788\n",
      "Step #124100, epoch #31025, avg. train loss: 86.56461\n",
      "Step #124200, epoch #31050, avg. train loss: 87.06183\n",
      "Step #124300, epoch #31075, avg. train loss: 87.67670\n",
      "Step #124400, epoch #31100, avg. train loss: 86.17082\n",
      "Step #124500, epoch #31125, avg. train loss: 86.80878\n",
      "Step #124600, epoch #31150, avg. train loss: 87.12383\n",
      "Step #124700, epoch #31175, avg. train loss: 86.20411\n",
      "Step #124800, epoch #31200, avg. train loss: 86.60893\n",
      "Step #124900, epoch #31225, avg. train loss: 86.78239\n",
      "Step #125000, epoch #31250, avg. train loss: 87.09373\n",
      "Step #125100, epoch #31275, avg. train loss: 87.31261\n",
      "Step #125200, epoch #31300, avg. train loss: 87.74872\n",
      "Step #125300, epoch #31325, avg. train loss: 87.17728\n",
      "Step #125400, epoch #31350, avg. train loss: 87.16550\n",
      "Step #125500, epoch #31375, avg. train loss: 85.89289\n",
      "Step #125600, epoch #31400, avg. train loss: 86.97798\n",
      "Step #125700, epoch #31425, avg. train loss: 86.81795\n",
      "Step #125800, epoch #31450, avg. train loss: 86.34041\n",
      "Step #125900, epoch #31475, avg. train loss: 86.56123\n",
      "Step #126000, epoch #31500, avg. train loss: 87.02714\n",
      "Step #126100, epoch #31525, avg. train loss: 86.05601\n",
      "Step #126200, epoch #31550, avg. train loss: 86.68653\n",
      "Step #126300, epoch #31575, avg. train loss: 86.64951\n",
      "Step #126400, epoch #31600, avg. train loss: 87.61667\n",
      "Step #126500, epoch #31625, avg. train loss: 86.37569\n",
      "Step #126600, epoch #31650, avg. train loss: 85.85110\n",
      "Step #126700, epoch #31675, avg. train loss: 87.30613\n",
      "Step #126800, epoch #31700, avg. train loss: 86.79381\n",
      "Step #126900, epoch #31725, avg. train loss: 86.60174\n",
      "Step #127000, epoch #31750, avg. train loss: 85.98994\n",
      "Step #127100, epoch #31775, avg. train loss: 86.44916\n",
      "Step #127200, epoch #31800, avg. train loss: 86.63815\n",
      "Step #127300, epoch #31825, avg. train loss: 86.42980\n",
      "Step #127400, epoch #31850, avg. train loss: 86.71347\n",
      "Step #127500, epoch #31875, avg. train loss: 86.65134\n",
      "Step #127600, epoch #31900, avg. train loss: 86.60603\n",
      "Step #127700, epoch #31925, avg. train loss: 86.51887\n",
      "Step #127800, epoch #31950, avg. train loss: 86.83884\n",
      "Step #127900, epoch #31975, avg. train loss: 86.58263\n",
      "Step #128000, epoch #32000, avg. train loss: 86.46310\n",
      "Step #128100, epoch #32025, avg. train loss: 86.70930\n",
      "Step #128200, epoch #32050, avg. train loss: 86.31643\n",
      "Step #128300, epoch #32075, avg. train loss: 86.28613\n",
      "Step #128400, epoch #32100, avg. train loss: 86.06838\n",
      "Step #128500, epoch #32125, avg. train loss: 85.42155\n",
      "Step #128600, epoch #32150, avg. train loss: 86.15802\n",
      "Step #128700, epoch #32175, avg. train loss: 86.37321\n",
      "Step #128800, epoch #32200, avg. train loss: 86.50063\n",
      "Step #128900, epoch #32225, avg. train loss: 86.58807\n",
      "Step #129000, epoch #32250, avg. train loss: 87.54261\n",
      "Step #129100, epoch #32275, avg. train loss: 87.61908\n",
      "Step #129200, epoch #32300, avg. train loss: 86.95672\n",
      "Step #129300, epoch #32325, avg. train loss: 86.65925\n",
      "Step #129400, epoch #32350, avg. train loss: 86.59375\n",
      "Step #129500, epoch #32375, avg. train loss: 86.95699\n",
      "Step #129600, epoch #32400, avg. train loss: 86.58485\n",
      "Step #129700, epoch #32425, avg. train loss: 86.17319\n",
      "Step #129800, epoch #32450, avg. train loss: 87.03207\n",
      "Step #129900, epoch #32475, avg. train loss: 87.56243\n",
      "Step #130000, epoch #32500, avg. train loss: 87.34751\n",
      "Step #130100, epoch #32525, avg. train loss: 85.36961\n",
      "Step #130200, epoch #32550, avg. train loss: 86.16549\n",
      "Step #130300, epoch #32575, avg. train loss: 86.95683\n",
      "Step #130400, epoch #32600, avg. train loss: 86.73543\n",
      "Step #130500, epoch #32625, avg. train loss: 87.04708\n",
      "Step #130600, epoch #32650, avg. train loss: 86.85149\n",
      "Step #130700, epoch #32675, avg. train loss: 86.45038\n",
      "Step #130800, epoch #32700, avg. train loss: 86.76584\n",
      "Step #130900, epoch #32725, avg. train loss: 86.37349\n",
      "Step #131000, epoch #32750, avg. train loss: 87.06863\n",
      "Step #131100, epoch #32775, avg. train loss: 85.50732\n",
      "Step #131200, epoch #32800, avg. train loss: 86.51431\n",
      "Step #131300, epoch #32825, avg. train loss: 86.16534\n",
      "Step #131400, epoch #32850, avg. train loss: 86.35818\n",
      "Step #131500, epoch #32875, avg. train loss: 86.75632\n",
      "Step #131600, epoch #32900, avg. train loss: 85.71751\n",
      "Step #131700, epoch #32925, avg. train loss: 86.79356\n",
      "Step #131800, epoch #32950, avg. train loss: 86.27817\n",
      "Step #131900, epoch #32975, avg. train loss: 86.22485\n",
      "Step #132000, epoch #33000, avg. train loss: 85.96484\n",
      "Step #132100, epoch #33025, avg. train loss: 86.45615\n",
      "Step #132200, epoch #33050, avg. train loss: 86.86080\n",
      "Step #132300, epoch #33075, avg. train loss: 86.37291\n",
      "Step #132400, epoch #33100, avg. train loss: 85.85780\n",
      "Step #132500, epoch #33125, avg. train loss: 86.81525\n",
      "Step #132600, epoch #33150, avg. train loss: 86.44047\n",
      "Step #132700, epoch #33175, avg. train loss: 86.22070\n",
      "Step #132800, epoch #33200, avg. train loss: 87.24058\n",
      "Step #132900, epoch #33225, avg. train loss: 85.62637\n",
      "Step #133000, epoch #33250, avg. train loss: 86.62749\n",
      "Step #133100, epoch #33275, avg. train loss: 86.50878\n",
      "Step #133200, epoch #33300, avg. train loss: 87.42300\n",
      "Step #133300, epoch #33325, avg. train loss: 87.21176\n",
      "Step #133400, epoch #33350, avg. train loss: 86.64432\n",
      "Step #133500, epoch #33375, avg. train loss: 85.99575\n",
      "Step #133600, epoch #33400, avg. train loss: 85.90284\n",
      "Step #133700, epoch #33425, avg. train loss: 86.34912\n",
      "Step #133800, epoch #33450, avg. train loss: 85.91577\n",
      "Step #133900, epoch #33475, avg. train loss: 86.15715\n",
      "Step #134000, epoch #33500, avg. train loss: 85.77009\n",
      "Step #134100, epoch #33525, avg. train loss: 86.86048\n",
      "Step #134200, epoch #33550, avg. train loss: 85.52620\n",
      "Step #134300, epoch #33575, avg. train loss: 86.72047\n",
      "Step #134400, epoch #33600, avg. train loss: 86.23235\n",
      "Step #134500, epoch #33625, avg. train loss: 86.73633\n",
      "Step #134600, epoch #33650, avg. train loss: 86.16700\n",
      "Step #134700, epoch #33675, avg. train loss: 86.87482\n",
      "Step #134800, epoch #33700, avg. train loss: 86.48332\n",
      "Step #134900, epoch #33725, avg. train loss: 85.97166\n",
      "Step #135000, epoch #33750, avg. train loss: 86.27155\n",
      "Step #135100, epoch #33775, avg. train loss: 86.82444\n",
      "Step #135200, epoch #33800, avg. train loss: 87.15668\n",
      "Step #135300, epoch #33825, avg. train loss: 85.61960\n",
      "Step #135400, epoch #33850, avg. train loss: 85.79730\n",
      "Step #135500, epoch #33875, avg. train loss: 86.09194\n",
      "Step #135600, epoch #33900, avg. train loss: 86.78163\n",
      "Step #135700, epoch #33925, avg. train loss: 86.96313\n",
      "Step #135800, epoch #33950, avg. train loss: 86.06159\n",
      "Step #135900, epoch #33975, avg. train loss: 85.59383\n",
      "Step #136000, epoch #34000, avg. train loss: 86.59130\n",
      "Step #136100, epoch #34025, avg. train loss: 85.77158\n",
      "Step #136200, epoch #34050, avg. train loss: 87.21231\n",
      "Step #136300, epoch #34075, avg. train loss: 85.94521\n",
      "Step #136400, epoch #34100, avg. train loss: 85.59592\n",
      "Step #136500, epoch #34125, avg. train loss: 86.60818\n",
      "Step #136600, epoch #34150, avg. train loss: 85.65486\n",
      "Step #136700, epoch #34175, avg. train loss: 85.93311\n",
      "Step #136800, epoch #34200, avg. train loss: 86.25320\n",
      "Step #136900, epoch #34225, avg. train loss: 86.19912\n",
      "Step #137000, epoch #34250, avg. train loss: 86.65894\n",
      "Step #137100, epoch #34275, avg. train loss: 86.40671\n",
      "Step #137200, epoch #34300, avg. train loss: 85.90932\n",
      "Step #137300, epoch #34325, avg. train loss: 86.74847\n",
      "Step #137400, epoch #34350, avg. train loss: 86.36962\n",
      "Step #137500, epoch #34375, avg. train loss: 86.05386\n",
      "Step #137600, epoch #34400, avg. train loss: 86.27550\n",
      "Step #137700, epoch #34425, avg. train loss: 86.53777\n",
      "Step #137800, epoch #34450, avg. train loss: 85.34111\n",
      "Step #137900, epoch #34475, avg. train loss: 85.81470\n",
      "Step #138000, epoch #34500, avg. train loss: 86.49122\n",
      "Step #138100, epoch #34525, avg. train loss: 86.08879\n",
      "Step #138200, epoch #34550, avg. train loss: 86.71101\n",
      "Step #138300, epoch #34575, avg. train loss: 85.47634\n",
      "Step #138400, epoch #34600, avg. train loss: 85.89899\n",
      "Step #138500, epoch #34625, avg. train loss: 85.10788\n",
      "Step #138600, epoch #34650, avg. train loss: 85.69936\n",
      "Step #138700, epoch #34675, avg. train loss: 86.24927\n",
      "Step #138800, epoch #34700, avg. train loss: 86.25450\n",
      "Step #138900, epoch #34725, avg. train loss: 87.02222\n",
      "Step #139000, epoch #34750, avg. train loss: 86.15625\n",
      "Step #139100, epoch #34775, avg. train loss: 86.37946\n",
      "Step #139200, epoch #34800, avg. train loss: 86.53811\n",
      "Step #139300, epoch #34825, avg. train loss: 86.44622\n",
      "Step #139400, epoch #34850, avg. train loss: 85.27426\n",
      "Step #139500, epoch #34875, avg. train loss: 86.43803\n",
      "Step #139600, epoch #34900, avg. train loss: 85.98038\n",
      "Step #139700, epoch #34925, avg. train loss: 85.56454\n",
      "Step #139800, epoch #34950, avg. train loss: 86.21397\n",
      "Step #139900, epoch #34975, avg. train loss: 85.80451\n",
      "Step #140000, epoch #35000, avg. train loss: 85.91745\n",
      "Step #140100, epoch #35025, avg. train loss: 86.50146\n",
      "Step #140200, epoch #35050, avg. train loss: 85.55423\n",
      "Step #140300, epoch #35075, avg. train loss: 85.10309\n",
      "Step #140400, epoch #35100, avg. train loss: 85.59864\n",
      "Step #140500, epoch #35125, avg. train loss: 85.99770\n",
      "Step #140600, epoch #35150, avg. train loss: 85.89717\n",
      "Step #140700, epoch #35175, avg. train loss: 85.54572\n",
      "Step #140800, epoch #35200, avg. train loss: 86.73305\n",
      "Step #140900, epoch #35225, avg. train loss: 86.78659\n",
      "Step #141000, epoch #35250, avg. train loss: 86.18167\n",
      "Step #141100, epoch #35275, avg. train loss: 86.02266\n",
      "Step #141200, epoch #35300, avg. train loss: 86.39217\n",
      "Step #141300, epoch #35325, avg. train loss: 85.69143\n",
      "Step #141400, epoch #35350, avg. train loss: 86.91776\n",
      "Step #141500, epoch #35375, avg. train loss: 86.54125\n",
      "Step #141600, epoch #35400, avg. train loss: 85.86526\n",
      "Step #141700, epoch #35425, avg. train loss: 85.95075\n",
      "Step #141800, epoch #35450, avg. train loss: 85.14889\n",
      "Step #141900, epoch #35475, avg. train loss: 86.03248\n",
      "Step #142000, epoch #35500, avg. train loss: 86.53742\n",
      "Step #142100, epoch #35525, avg. train loss: 84.44490\n",
      "Step #142200, epoch #35550, avg. train loss: 85.25695\n",
      "Step #142300, epoch #35575, avg. train loss: 85.98296\n",
      "Step #142400, epoch #35600, avg. train loss: 86.15025\n",
      "Step #142500, epoch #35625, avg. train loss: 85.80692\n",
      "Step #142600, epoch #35650, avg. train loss: 86.29905\n",
      "Step #142700, epoch #35675, avg. train loss: 85.48350\n",
      "Step #142800, epoch #35700, avg. train loss: 85.47845\n",
      "Step #142900, epoch #35725, avg. train loss: 85.79617\n",
      "Step #143000, epoch #35750, avg. train loss: 85.35938\n",
      "Step #143100, epoch #35775, avg. train loss: 85.28541\n",
      "Step #143200, epoch #35800, avg. train loss: 86.02167\n",
      "Step #143300, epoch #35825, avg. train loss: 85.75848\n",
      "Step #143400, epoch #35850, avg. train loss: 85.44476\n",
      "Step #143500, epoch #35875, avg. train loss: 85.26807\n",
      "Step #143600, epoch #35900, avg. train loss: 86.50751\n",
      "Step #143700, epoch #35925, avg. train loss: 85.30490\n",
      "Step #143800, epoch #35950, avg. train loss: 84.98019\n",
      "Step #143900, epoch #35975, avg. train loss: 85.98758\n",
      "Step #144000, epoch #36000, avg. train loss: 85.45869\n",
      "Step #144100, epoch #36025, avg. train loss: 86.25100\n",
      "Step #144200, epoch #36050, avg. train loss: 86.66997\n",
      "Step #144300, epoch #36075, avg. train loss: 85.50954\n",
      "Step #144400, epoch #36100, avg. train loss: 85.17432\n",
      "Step #144500, epoch #36125, avg. train loss: 85.15723\n",
      "Step #144600, epoch #36150, avg. train loss: 85.74139\n",
      "Step #144700, epoch #36175, avg. train loss: 86.51901\n",
      "Step #144800, epoch #36200, avg. train loss: 85.17592\n",
      "Step #144900, epoch #36225, avg. train loss: 85.85918\n",
      "Step #145000, epoch #36250, avg. train loss: 85.94810\n",
      "Step #145100, epoch #36275, avg. train loss: 86.25878\n",
      "Step #145200, epoch #36300, avg. train loss: 85.60750\n",
      "Step #145300, epoch #36325, avg. train loss: 86.02267\n",
      "Step #145400, epoch #36350, avg. train loss: 86.15884\n",
      "Step #145500, epoch #36375, avg. train loss: 85.86211\n",
      "Step #145600, epoch #36400, avg. train loss: 85.76635\n",
      "Step #145700, epoch #36425, avg. train loss: 86.76684\n",
      "Step #145800, epoch #36450, avg. train loss: 85.63221\n",
      "Step #145900, epoch #36475, avg. train loss: 86.16099\n",
      "Step #146000, epoch #36500, avg. train loss: 85.88530\n",
      "Step #146100, epoch #36525, avg. train loss: 85.26977\n",
      "Step #146200, epoch #36550, avg. train loss: 85.28374\n",
      "Step #146300, epoch #36575, avg. train loss: 86.17836\n",
      "Step #146400, epoch #36600, avg. train loss: 85.30623\n",
      "Step #146500, epoch #36625, avg. train loss: 86.20495\n",
      "Step #146600, epoch #36650, avg. train loss: 84.92035\n",
      "Step #146700, epoch #36675, avg. train loss: 85.00406\n",
      "Step #146800, epoch #36700, avg. train loss: 85.33353\n",
      "Step #146900, epoch #36725, avg. train loss: 86.48827\n",
      "Step #147000, epoch #36750, avg. train loss: 86.37196\n",
      "Step #147100, epoch #36775, avg. train loss: 85.55460\n",
      "Step #147200, epoch #36800, avg. train loss: 85.48235\n",
      "Step #147300, epoch #36825, avg. train loss: 85.00209\n",
      "Step #147400, epoch #36850, avg. train loss: 85.68676\n",
      "Step #147500, epoch #36875, avg. train loss: 84.97782\n",
      "Step #147600, epoch #36900, avg. train loss: 85.26958\n",
      "Step #147700, epoch #36925, avg. train loss: 85.44408\n",
      "Step #147800, epoch #36950, avg. train loss: 84.89206\n",
      "Step #147900, epoch #36975, avg. train loss: 85.26973\n",
      "Step #148000, epoch #37000, avg. train loss: 85.52866\n",
      "Step #148100, epoch #37025, avg. train loss: 85.62933\n",
      "Step #148200, epoch #37050, avg. train loss: 85.11014\n",
      "Step #148300, epoch #37075, avg. train loss: 86.07677\n",
      "Step #148400, epoch #37100, avg. train loss: 85.62288\n",
      "Step #148500, epoch #37125, avg. train loss: 85.09532\n",
      "Step #148600, epoch #37150, avg. train loss: 86.32903\n",
      "Step #148700, epoch #37175, avg. train loss: 85.71404\n",
      "Step #148800, epoch #37200, avg. train loss: 86.08549\n",
      "Step #148900, epoch #37225, avg. train loss: 85.15961\n",
      "Step #149000, epoch #37250, avg. train loss: 85.03728\n",
      "Step #149100, epoch #37275, avg. train loss: 86.21502\n",
      "Step #149200, epoch #37300, avg. train loss: 85.46468\n",
      "Step #149300, epoch #37325, avg. train loss: 85.88474\n",
      "Step #149400, epoch #37350, avg. train loss: 86.02109\n",
      "Step #149500, epoch #37375, avg. train loss: 85.05435\n",
      "Step #149600, epoch #37400, avg. train loss: 86.86687\n",
      "Step #149700, epoch #37425, avg. train loss: 85.18554\n",
      "Step #149800, epoch #37450, avg. train loss: 85.82585\n",
      "Step #149900, epoch #37475, avg. train loss: 85.43855\n",
      "Step #150000, epoch #37500, avg. train loss: 85.88586\n",
      "Step #150100, epoch #37525, avg. train loss: 85.95716\n",
      "Step #150200, epoch #37550, avg. train loss: 85.78616\n",
      "Step #150300, epoch #37575, avg. train loss: 85.12395\n",
      "Step #150400, epoch #37600, avg. train loss: 86.03223\n",
      "Step #150500, epoch #37625, avg. train loss: 85.15129\n",
      "Step #150600, epoch #37650, avg. train loss: 86.12930\n",
      "Step #150700, epoch #37675, avg. train loss: 85.93890\n",
      "Step #150800, epoch #37700, avg. train loss: 85.08872\n",
      "Step #150900, epoch #37725, avg. train loss: 85.55681\n",
      "Step #151000, epoch #37750, avg. train loss: 86.09567\n",
      "Step #151100, epoch #37775, avg. train loss: 85.75513\n",
      "Step #151200, epoch #37800, avg. train loss: 85.77853\n",
      "Step #151300, epoch #37825, avg. train loss: 85.26789\n",
      "Step #151400, epoch #37850, avg. train loss: 84.60214\n",
      "Step #151500, epoch #37875, avg. train loss: 85.29268\n",
      "Step #151600, epoch #37900, avg. train loss: 84.99277\n",
      "Step #151700, epoch #37925, avg. train loss: 85.63113\n",
      "Step #151800, epoch #37950, avg. train loss: 84.99895\n",
      "Step #151900, epoch #37975, avg. train loss: 85.80593\n",
      "Step #152000, epoch #38000, avg. train loss: 85.16766\n",
      "Step #152100, epoch #38025, avg. train loss: 85.52428\n",
      "Step #152200, epoch #38050, avg. train loss: 84.86599\n",
      "Step #152300, epoch #38075, avg. train loss: 85.22177\n",
      "Step #152400, epoch #38100, avg. train loss: 85.04764\n",
      "Step #152500, epoch #38125, avg. train loss: 84.96342\n",
      "Step #152600, epoch #38150, avg. train loss: 84.75116\n",
      "Step #152700, epoch #38175, avg. train loss: 84.64888\n",
      "Step #152800, epoch #38200, avg. train loss: 85.36428\n",
      "Step #152900, epoch #38225, avg. train loss: 85.45158\n",
      "Step #153000, epoch #38250, avg. train loss: 86.15601\n",
      "Step #153100, epoch #38275, avg. train loss: 85.65586\n",
      "Step #153200, epoch #38300, avg. train loss: 85.70327\n",
      "Step #153300, epoch #38325, avg. train loss: 85.72346\n",
      "Step #153400, epoch #38350, avg. train loss: 85.50520\n",
      "Step #153500, epoch #38375, avg. train loss: 84.83949\n",
      "Step #153600, epoch #38400, avg. train loss: 85.26033\n",
      "Step #153700, epoch #38425, avg. train loss: 85.77570\n",
      "Step #153800, epoch #38450, avg. train loss: 84.84312\n",
      "Step #153900, epoch #38475, avg. train loss: 84.96077\n",
      "Step #154000, epoch #38500, avg. train loss: 84.63938\n",
      "Step #154100, epoch #38525, avg. train loss: 85.08633\n",
      "Step #154200, epoch #38550, avg. train loss: 85.33898\n",
      "Step #154300, epoch #38575, avg. train loss: 85.43461\n",
      "Step #154400, epoch #38600, avg. train loss: 86.08830\n",
      "Step #154500, epoch #38625, avg. train loss: 84.59756\n",
      "Step #154600, epoch #38650, avg. train loss: 84.94431\n",
      "Step #154700, epoch #38675, avg. train loss: 84.97174\n",
      "Step #154800, epoch #38700, avg. train loss: 85.09505\n",
      "Step #154900, epoch #38725, avg. train loss: 84.75433\n",
      "Step #155000, epoch #38750, avg. train loss: 84.47461\n",
      "Step #155100, epoch #38775, avg. train loss: 85.13664\n",
      "Step #155200, epoch #38800, avg. train loss: 85.08045\n",
      "Step #155300, epoch #38825, avg. train loss: 84.53773\n",
      "Step #155400, epoch #38850, avg. train loss: 84.99358\n",
      "Step #155500, epoch #38875, avg. train loss: 84.80859\n",
      "Step #155600, epoch #38900, avg. train loss: 84.83060\n",
      "Step #155700, epoch #38925, avg. train loss: 84.36366\n",
      "Step #155800, epoch #38950, avg. train loss: 85.42619\n",
      "Step #155900, epoch #38975, avg. train loss: 84.38960\n",
      "Step #156000, epoch #39000, avg. train loss: 85.73328\n",
      "Step #156100, epoch #39025, avg. train loss: 85.03990\n",
      "Step #156200, epoch #39050, avg. train loss: 84.91766\n",
      "Step #156300, epoch #39075, avg. train loss: 85.57481\n",
      "Step #156400, epoch #39100, avg. train loss: 85.22612\n",
      "Step #156500, epoch #39125, avg. train loss: 85.15591\n",
      "Step #156600, epoch #39150, avg. train loss: 85.09231\n",
      "Step #156700, epoch #39175, avg. train loss: 85.12061\n",
      "Step #156800, epoch #39200, avg. train loss: 85.00768\n",
      "Step #156900, epoch #39225, avg. train loss: 84.39903\n",
      "Step #157000, epoch #39250, avg. train loss: 85.56413\n",
      "Step #157100, epoch #39275, avg. train loss: 85.00198\n",
      "Step #157200, epoch #39300, avg. train loss: 85.70481\n",
      "Step #157300, epoch #39325, avg. train loss: 84.94560\n",
      "Step #157400, epoch #39350, avg. train loss: 84.98470\n",
      "Step #157500, epoch #39375, avg. train loss: 85.98033\n",
      "Step #157600, epoch #39400, avg. train loss: 84.90779\n",
      "Step #157700, epoch #39425, avg. train loss: 84.20613\n",
      "Step #157800, epoch #39450, avg. train loss: 84.84400\n",
      "Step #157900, epoch #39475, avg. train loss: 83.93974\n",
      "Step #158000, epoch #39500, avg. train loss: 84.77193\n",
      "Step #158100, epoch #39525, avg. train loss: 85.07857\n",
      "Step #158200, epoch #39550, avg. train loss: 84.15904\n",
      "Step #158300, epoch #39575, avg. train loss: 85.67979\n",
      "Step #158400, epoch #39600, avg. train loss: 85.65083\n",
      "Step #158500, epoch #39625, avg. train loss: 85.82393\n",
      "Step #158600, epoch #39650, avg. train loss: 85.39073\n",
      "Step #158700, epoch #39675, avg. train loss: 85.11684\n",
      "Step #158800, epoch #39700, avg. train loss: 84.73189\n",
      "Step #158900, epoch #39725, avg. train loss: 85.18739\n",
      "Step #159000, epoch #39750, avg. train loss: 84.58416\n",
      "Step #159100, epoch #39775, avg. train loss: 85.79224\n",
      "Step #159200, epoch #39800, avg. train loss: 84.87752\n",
      "Step #159300, epoch #39825, avg. train loss: 86.09824\n",
      "Step #159400, epoch #39850, avg. train loss: 84.44396\n",
      "Step #159500, epoch #39875, avg. train loss: 85.30159\n",
      "Step #159600, epoch #39900, avg. train loss: 84.72971\n",
      "Step #159700, epoch #39925, avg. train loss: 84.99797\n",
      "Step #159800, epoch #39950, avg. train loss: 85.97528\n",
      "Step #159900, epoch #39975, avg. train loss: 84.67873\n",
      "Step #160000, epoch #40000, avg. train loss: 85.09088\n",
      "Step #160100, epoch #40025, avg. train loss: 85.14581\n",
      "Step #160200, epoch #40050, avg. train loss: 84.86265\n",
      "Step #160300, epoch #40075, avg. train loss: 84.85595\n",
      "Step #160400, epoch #40100, avg. train loss: 84.47028\n",
      "Step #160500, epoch #40125, avg. train loss: 84.43684\n",
      "Step #160600, epoch #40150, avg. train loss: 85.04984\n",
      "Step #160700, epoch #40175, avg. train loss: 84.73050\n",
      "Step #160800, epoch #40200, avg. train loss: 85.50070\n",
      "Step #160900, epoch #40225, avg. train loss: 84.72855\n",
      "Step #161000, epoch #40250, avg. train loss: 84.76244\n",
      "Step #161100, epoch #40275, avg. train loss: 85.20898\n",
      "Step #161200, epoch #40300, avg. train loss: 85.29050\n",
      "Step #161300, epoch #40325, avg. train loss: 85.57002\n",
      "Step #161400, epoch #40350, avg. train loss: 85.55780\n",
      "Step #161500, epoch #40375, avg. train loss: 85.37562\n",
      "Step #161600, epoch #40400, avg. train loss: 84.76651\n",
      "Step #161700, epoch #40425, avg. train loss: 85.12984\n",
      "Step #161800, epoch #40450, avg. train loss: 84.32455\n",
      "Step #161900, epoch #40475, avg. train loss: 83.77597\n",
      "Step #162000, epoch #40500, avg. train loss: 84.39710\n",
      "Step #162100, epoch #40525, avg. train loss: 86.16222\n",
      "Step #162200, epoch #40550, avg. train loss: 84.78667\n",
      "Step #162300, epoch #40575, avg. train loss: 84.98215\n",
      "Step #162400, epoch #40600, avg. train loss: 85.73415\n",
      "Step #162500, epoch #40625, avg. train loss: 84.86803\n",
      "Step #162600, epoch #40650, avg. train loss: 85.22635\n",
      "Step #162700, epoch #40675, avg. train loss: 85.36792\n",
      "Step #162800, epoch #40700, avg. train loss: 85.05005\n",
      "Step #162900, epoch #40725, avg. train loss: 84.58727\n",
      "Step #163000, epoch #40750, avg. train loss: 84.12578\n",
      "Step #163100, epoch #40775, avg. train loss: 84.19736\n",
      "Step #163200, epoch #40800, avg. train loss: 84.79635\n",
      "Step #163300, epoch #40825, avg. train loss: 85.23203\n",
      "Step #163400, epoch #40850, avg. train loss: 84.52883\n",
      "Step #163500, epoch #40875, avg. train loss: 85.24049\n",
      "Step #163600, epoch #40900, avg. train loss: 85.61177\n",
      "Step #163700, epoch #40925, avg. train loss: 85.03778\n",
      "Step #163800, epoch #40950, avg. train loss: 84.46819\n",
      "Step #163900, epoch #40975, avg. train loss: 85.32798\n",
      "Step #164000, epoch #41000, avg. train loss: 85.43485\n",
      "Step #164100, epoch #41025, avg. train loss: 84.50229\n",
      "Step #164200, epoch #41050, avg. train loss: 85.17766\n",
      "Step #164300, epoch #41075, avg. train loss: 84.75904\n",
      "Step #164400, epoch #41100, avg. train loss: 85.49708\n",
      "Step #164500, epoch #41125, avg. train loss: 84.86324\n",
      "Step #164600, epoch #41150, avg. train loss: 84.94859\n",
      "Step #164700, epoch #41175, avg. train loss: 84.76614\n",
      "Step #164800, epoch #41200, avg. train loss: 84.19241\n",
      "Step #164900, epoch #41225, avg. train loss: 84.41129\n",
      "Step #165000, epoch #41250, avg. train loss: 84.53689\n",
      "Step #165100, epoch #41275, avg. train loss: 85.14423\n",
      "Step #165200, epoch #41300, avg. train loss: 84.10740\n",
      "Step #165300, epoch #41325, avg. train loss: 84.30913\n",
      "Step #165400, epoch #41350, avg. train loss: 85.02588\n",
      "Step #165500, epoch #41375, avg. train loss: 85.04612\n",
      "Step #165600, epoch #41400, avg. train loss: 84.92870\n",
      "Step #165700, epoch #41425, avg. train loss: 84.83708\n",
      "Step #165800, epoch #41450, avg. train loss: 84.99986\n",
      "Step #165900, epoch #41475, avg. train loss: 84.25928\n",
      "Step #166000, epoch #41500, avg. train loss: 85.48123\n",
      "Step #166100, epoch #41525, avg. train loss: 84.67777\n",
      "Step #166200, epoch #41550, avg. train loss: 84.21915\n",
      "Step #166300, epoch #41575, avg. train loss: 85.65790\n",
      "Step #166400, epoch #41600, avg. train loss: 84.69410\n",
      "Step #166500, epoch #41625, avg. train loss: 84.62271\n",
      "Step #166600, epoch #41650, avg. train loss: 84.56271\n",
      "Step #166700, epoch #41675, avg. train loss: 85.40169\n",
      "Step #166800, epoch #41700, avg. train loss: 85.68674\n",
      "Step #166900, epoch #41725, avg. train loss: 84.51740\n",
      "Step #167000, epoch #41750, avg. train loss: 83.99644\n",
      "Step #167100, epoch #41775, avg. train loss: 84.69479\n",
      "Step #167200, epoch #41800, avg. train loss: 84.50356\n",
      "Step #167300, epoch #41825, avg. train loss: 84.72359\n",
      "Step #167400, epoch #41850, avg. train loss: 84.10938\n",
      "Step #167500, epoch #41875, avg. train loss: 86.15164\n",
      "Step #167600, epoch #41900, avg. train loss: 84.93277\n",
      "Step #167700, epoch #41925, avg. train loss: 85.63776\n",
      "Step #167800, epoch #41950, avg. train loss: 84.32590\n",
      "Step #167900, epoch #41975, avg. train loss: 84.96225\n",
      "Step #168000, epoch #42000, avg. train loss: 84.22960\n",
      "Step #168100, epoch #42025, avg. train loss: 84.87691\n",
      "Step #168200, epoch #42050, avg. train loss: 85.06702\n",
      "Step #168300, epoch #42075, avg. train loss: 84.41018\n",
      "Step #168400, epoch #42100, avg. train loss: 83.74421\n",
      "Step #168500, epoch #42125, avg. train loss: 84.74344\n",
      "Step #168600, epoch #42150, avg. train loss: 85.42362\n",
      "Step #168700, epoch #42175, avg. train loss: 84.60523\n",
      "Step #168800, epoch #42200, avg. train loss: 84.75209\n",
      "Step #168900, epoch #42225, avg. train loss: 84.75001\n",
      "Step #169000, epoch #42250, avg. train loss: 84.48689\n",
      "Step #169100, epoch #42275, avg. train loss: 84.34882\n",
      "Step #169200, epoch #42300, avg. train loss: 86.42752\n",
      "Step #169300, epoch #42325, avg. train loss: 85.27340\n",
      "Step #169400, epoch #42350, avg. train loss: 84.56926\n",
      "Step #169500, epoch #42375, avg. train loss: 84.60584\n",
      "Step #169600, epoch #42400, avg. train loss: 83.91707\n",
      "Step #169700, epoch #42425, avg. train loss: 84.37661\n",
      "Step #169800, epoch #42450, avg. train loss: 84.45627\n",
      "Step #169900, epoch #42475, avg. train loss: 84.48043\n",
      "Step #170000, epoch #42500, avg. train loss: 84.76818\n",
      "Step #170100, epoch #42525, avg. train loss: 84.27364\n",
      "Step #170200, epoch #42550, avg. train loss: 84.56881\n",
      "Step #170300, epoch #42575, avg. train loss: 84.78502\n",
      "Step #170400, epoch #42600, avg. train loss: 84.62780\n",
      "Step #170500, epoch #42625, avg. train loss: 83.89841\n",
      "Step #170600, epoch #42650, avg. train loss: 84.70078\n",
      "Step #170700, epoch #42675, avg. train loss: 84.41508\n",
      "Step #170800, epoch #42700, avg. train loss: 84.92995\n",
      "Step #170900, epoch #42725, avg. train loss: 83.87901\n",
      "Step #171000, epoch #42750, avg. train loss: 84.28540\n",
      "Step #171100, epoch #42775, avg. train loss: 84.87137\n",
      "Step #171200, epoch #42800, avg. train loss: 84.50710\n",
      "Step #171300, epoch #42825, avg. train loss: 84.50371\n",
      "Step #171400, epoch #42850, avg. train loss: 83.67247\n",
      "Step #171500, epoch #42875, avg. train loss: 83.54203\n",
      "Step #171600, epoch #42900, avg. train loss: 84.75108\n",
      "Step #171700, epoch #42925, avg. train loss: 84.66739\n",
      "Step #171800, epoch #42950, avg. train loss: 85.28886\n",
      "Step #171900, epoch #42975, avg. train loss: 84.50653\n",
      "Step #172000, epoch #43000, avg. train loss: 84.38560\n",
      "Step #172100, epoch #43025, avg. train loss: 84.43925\n",
      "Step #172200, epoch #43050, avg. train loss: 84.66006\n",
      "Step #172300, epoch #43075, avg. train loss: 85.62103\n",
      "Step #172400, epoch #43100, avg. train loss: 84.53292\n",
      "Step #172500, epoch #43125, avg. train loss: 84.65273\n",
      "Step #172600, epoch #43150, avg. train loss: 84.71658\n",
      "Step #172700, epoch #43175, avg. train loss: 84.69733\n",
      "Step #172800, epoch #43200, avg. train loss: 84.01876\n",
      "Step #172900, epoch #43225, avg. train loss: 84.79533\n",
      "Step #173000, epoch #43250, avg. train loss: 83.47932\n",
      "Step #173100, epoch #43275, avg. train loss: 84.48389\n",
      "Step #173200, epoch #43300, avg. train loss: 84.67690\n",
      "Step #173300, epoch #43325, avg. train loss: 84.03662\n",
      "Step #173400, epoch #43350, avg. train loss: 84.60786\n",
      "Step #173500, epoch #43375, avg. train loss: 84.88986\n",
      "Step #173600, epoch #43400, avg. train loss: 84.01416\n",
      "Step #173700, epoch #43425, avg. train loss: 85.24830\n",
      "Step #173800, epoch #43450, avg. train loss: 84.97787\n",
      "Step #173900, epoch #43475, avg. train loss: 84.60562\n",
      "Step #174000, epoch #43500, avg. train loss: 85.25411\n",
      "Step #174100, epoch #43525, avg. train loss: 84.29695\n",
      "Step #174200, epoch #43550, avg. train loss: 84.58940\n",
      "Step #174300, epoch #43575, avg. train loss: 84.14039\n",
      "Step #174400, epoch #43600, avg. train loss: 84.73035\n",
      "Step #174500, epoch #43625, avg. train loss: 85.13837\n",
      "Step #174600, epoch #43650, avg. train loss: 85.38877\n",
      "Step #174700, epoch #43675, avg. train loss: 84.25800\n",
      "Step #174800, epoch #43700, avg. train loss: 84.62078\n",
      "Step #174900, epoch #43725, avg. train loss: 84.32575\n",
      "Step #175000, epoch #43750, avg. train loss: 84.79060\n",
      "Step #175100, epoch #43775, avg. train loss: 84.38498\n",
      "Step #175200, epoch #43800, avg. train loss: 84.21467\n",
      "Step #175300, epoch #43825, avg. train loss: 83.98537\n",
      "Step #175400, epoch #43850, avg. train loss: 84.42265\n",
      "Step #175500, epoch #43875, avg. train loss: 84.85498\n",
      "Step #175600, epoch #43900, avg. train loss: 85.03088\n",
      "Step #175700, epoch #43925, avg. train loss: 84.28168\n",
      "Step #175800, epoch #43950, avg. train loss: 84.63309\n",
      "Step #175900, epoch #43975, avg. train loss: 84.01663\n",
      "Step #176000, epoch #44000, avg. train loss: 84.56272\n",
      "Step #176100, epoch #44025, avg. train loss: 84.21813\n",
      "Step #176200, epoch #44050, avg. train loss: 83.83176\n",
      "Step #176300, epoch #44075, avg. train loss: 84.74934\n",
      "Step #176400, epoch #44100, avg. train loss: 84.59740\n",
      "Step #176500, epoch #44125, avg. train loss: 84.11880\n",
      "Step #176600, epoch #44150, avg. train loss: 84.04453\n",
      "Step #176700, epoch #44175, avg. train loss: 84.17236\n",
      "Step #176800, epoch #44200, avg. train loss: 84.66947\n",
      "Step #176900, epoch #44225, avg. train loss: 84.60108\n",
      "Step #177000, epoch #44250, avg. train loss: 84.19311\n",
      "Step #177100, epoch #44275, avg. train loss: 84.17023\n",
      "Step #177200, epoch #44300, avg. train loss: 84.80486\n",
      "Step #177300, epoch #44325, avg. train loss: 84.09165\n",
      "Step #177400, epoch #44350, avg. train loss: 84.88510\n",
      "Step #177500, epoch #44375, avg. train loss: 85.38217\n",
      "Step #177600, epoch #44400, avg. train loss: 84.54987\n",
      "Step #177700, epoch #44425, avg. train loss: 83.72978\n",
      "Step #177800, epoch #44450, avg. train loss: 84.21278\n",
      "Step #177900, epoch #44475, avg. train loss: 84.00478\n",
      "Step #178000, epoch #44500, avg. train loss: 84.66228\n",
      "Step #178100, epoch #44525, avg. train loss: 85.06609\n",
      "Step #178200, epoch #44550, avg. train loss: 84.72028\n",
      "Step #178300, epoch #44575, avg. train loss: 84.67760\n",
      "Step #178400, epoch #44600, avg. train loss: 83.59417\n",
      "Step #178500, epoch #44625, avg. train loss: 84.97056\n",
      "Step #178600, epoch #44650, avg. train loss: 84.42171\n",
      "Step #178700, epoch #44675, avg. train loss: 84.30833\n",
      "Step #178800, epoch #44700, avg. train loss: 84.49454\n",
      "Step #178900, epoch #44725, avg. train loss: 85.21114\n",
      "Step #179000, epoch #44750, avg. train loss: 84.05878\n",
      "Step #179100, epoch #44775, avg. train loss: 84.85170\n",
      "Step #179200, epoch #44800, avg. train loss: 84.02451\n",
      "Step #179300, epoch #44825, avg. train loss: 84.00202\n",
      "Step #179400, epoch #44850, avg. train loss: 84.20509\n",
      "Step #179500, epoch #44875, avg. train loss: 84.49411\n",
      "Step #179600, epoch #44900, avg. train loss: 84.34350\n",
      "Step #179700, epoch #44925, avg. train loss: 84.18496\n",
      "Step #179800, epoch #44950, avg. train loss: 84.52716\n",
      "Step #179900, epoch #44975, avg. train loss: 84.20670\n",
      "Step #180000, epoch #45000, avg. train loss: 84.35288\n",
      "Step #180100, epoch #45025, avg. train loss: 83.87318\n",
      "Step #180200, epoch #45050, avg. train loss: 84.79957\n",
      "Step #180300, epoch #45075, avg. train loss: 84.97903\n",
      "Step #180400, epoch #45100, avg. train loss: 84.88459\n",
      "Step #180500, epoch #45125, avg. train loss: 84.01600\n",
      "Step #180600, epoch #45150, avg. train loss: 83.74345\n",
      "Step #180700, epoch #45175, avg. train loss: 84.83539\n",
      "Step #180800, epoch #45200, avg. train loss: 84.66919\n",
      "Step #180900, epoch #45225, avg. train loss: 84.43957\n",
      "Step #181000, epoch #45250, avg. train loss: 84.65838\n",
      "Step #181100, epoch #45275, avg. train loss: 84.84146\n",
      "Step #181200, epoch #45300, avg. train loss: 83.86841\n",
      "Step #181300, epoch #45325, avg. train loss: 84.78293\n",
      "Step #181400, epoch #45350, avg. train loss: 85.46963\n",
      "Step #181500, epoch #45375, avg. train loss: 84.11221\n",
      "Step #181600, epoch #45400, avg. train loss: 83.99085\n",
      "Step #181700, epoch #45425, avg. train loss: 83.18163\n",
      "Step #181800, epoch #45450, avg. train loss: 84.60297\n",
      "Step #181900, epoch #45475, avg. train loss: 84.73057\n",
      "Step #182000, epoch #45500, avg. train loss: 84.43540\n",
      "Step #182100, epoch #45525, avg. train loss: 84.47639\n",
      "Step #182200, epoch #45550, avg. train loss: 84.16824\n",
      "Step #182300, epoch #45575, avg. train loss: 83.47204\n",
      "Step #182400, epoch #45600, avg. train loss: 84.95885\n",
      "Step #182500, epoch #45625, avg. train loss: 84.64693\n",
      "Step #182600, epoch #45650, avg. train loss: 84.77383\n",
      "Step #182700, epoch #45675, avg. train loss: 84.32365\n",
      "Step #182800, epoch #45700, avg. train loss: 84.34483\n",
      "Step #182900, epoch #45725, avg. train loss: 84.23853\n",
      "Step #183000, epoch #45750, avg. train loss: 83.55574\n",
      "Step #183100, epoch #45775, avg. train loss: 84.73144\n",
      "Step #183200, epoch #45800, avg. train loss: 83.53453\n",
      "Step #183300, epoch #45825, avg. train loss: 84.08580\n",
      "Step #183400, epoch #45850, avg. train loss: 84.57791\n",
      "Step #183500, epoch #45875, avg. train loss: 83.71241\n",
      "Step #183600, epoch #45900, avg. train loss: 83.86878\n",
      "Step #183700, epoch #45925, avg. train loss: 84.23479\n",
      "Step #183800, epoch #45950, avg. train loss: 84.38207\n",
      "Step #183900, epoch #45975, avg. train loss: 85.00097\n",
      "Step #184000, epoch #46000, avg. train loss: 84.19539\n",
      "Step #184100, epoch #46025, avg. train loss: 84.68730\n",
      "Step #184200, epoch #46050, avg. train loss: 83.41231\n",
      "Step #184300, epoch #46075, avg. train loss: 83.35962\n",
      "Step #184400, epoch #46100, avg. train loss: 84.86024\n",
      "Step #184500, epoch #46125, avg. train loss: 83.86697\n",
      "Step #184600, epoch #46150, avg. train loss: 84.58527\n",
      "Step #184700, epoch #46175, avg. train loss: 84.47673\n",
      "Step #184800, epoch #46200, avg. train loss: 84.82535\n",
      "Step #184900, epoch #46225, avg. train loss: 84.18105\n",
      "Step #185000, epoch #46250, avg. train loss: 84.47854\n",
      "Step #185100, epoch #46275, avg. train loss: 83.49797\n",
      "Step #185200, epoch #46300, avg. train loss: 84.21928\n",
      "Step #185300, epoch #46325, avg. train loss: 84.61763\n",
      "Step #185400, epoch #46350, avg. train loss: 84.30067\n",
      "Step #185500, epoch #46375, avg. train loss: 83.83643\n",
      "Step #185600, epoch #46400, avg. train loss: 83.27780\n",
      "Step #185700, epoch #46425, avg. train loss: 83.64747\n",
      "Step #185800, epoch #46450, avg. train loss: 84.21146\n",
      "Step #185900, epoch #46475, avg. train loss: 83.50635\n",
      "Step #186000, epoch #46500, avg. train loss: 83.79614\n",
      "Step #186100, epoch #46525, avg. train loss: 83.59521\n",
      "Step #186200, epoch #46550, avg. train loss: 83.95302\n",
      "Step #186300, epoch #46575, avg. train loss: 83.34286\n",
      "Step #186400, epoch #46600, avg. train loss: 84.10160\n",
      "Step #186500, epoch #46625, avg. train loss: 82.99404\n",
      "Step #186600, epoch #46650, avg. train loss: 84.52158\n",
      "Step #186700, epoch #46675, avg. train loss: 83.43116\n",
      "Step #186800, epoch #46700, avg. train loss: 84.55814\n",
      "Step #186900, epoch #46725, avg. train loss: 83.24571\n",
      "Step #187000, epoch #46750, avg. train loss: 83.83760\n",
      "Step #187100, epoch #46775, avg. train loss: 83.80037\n",
      "Step #187200, epoch #46800, avg. train loss: 85.49428\n",
      "Step #187300, epoch #46825, avg. train loss: 83.51218\n",
      "Step #187400, epoch #46850, avg. train loss: 83.69656\n",
      "Step #187500, epoch #46875, avg. train loss: 83.73839\n",
      "Step #187600, epoch #46900, avg. train loss: 84.03366\n",
      "Step #187700, epoch #46925, avg. train loss: 84.58328\n",
      "Step #187800, epoch #46950, avg. train loss: 83.35809\n",
      "Step #187900, epoch #46975, avg. train loss: 84.59022\n",
      "Step #188000, epoch #47000, avg. train loss: 83.92554\n",
      "Step #188100, epoch #47025, avg. train loss: 83.79681\n",
      "Step #188200, epoch #47050, avg. train loss: 83.93143\n",
      "Step #188300, epoch #47075, avg. train loss: 84.35468\n",
      "Step #188400, epoch #47100, avg. train loss: 84.78767\n",
      "Step #188500, epoch #47125, avg. train loss: 84.82722\n",
      "Step #188600, epoch #47150, avg. train loss: 84.11690\n",
      "Step #188700, epoch #47175, avg. train loss: 83.96774\n",
      "Step #188800, epoch #47200, avg. train loss: 83.57150\n",
      "Step #188900, epoch #47225, avg. train loss: 83.12312\n",
      "Step #189000, epoch #47250, avg. train loss: 84.61842\n",
      "Step #189100, epoch #47275, avg. train loss: 84.01044\n",
      "Step #189200, epoch #47300, avg. train loss: 84.26077\n",
      "Step #189300, epoch #47325, avg. train loss: 83.84658\n",
      "Step #189400, epoch #47350, avg. train loss: 84.39262\n",
      "Step #189500, epoch #47375, avg. train loss: 84.43713\n",
      "Step #189600, epoch #47400, avg. train loss: 83.10046\n",
      "Step #189700, epoch #47425, avg. train loss: 83.73515\n",
      "Step #189800, epoch #47450, avg. train loss: 82.89764\n",
      "Step #189900, epoch #47475, avg. train loss: 84.78549\n",
      "Step #190000, epoch #47500, avg. train loss: 84.32826\n",
      "Step #190100, epoch #47525, avg. train loss: 84.66518\n",
      "Step #190200, epoch #47550, avg. train loss: 83.84011\n",
      "Step #190300, epoch #47575, avg. train loss: 84.09407\n",
      "Step #190400, epoch #47600, avg. train loss: 83.44546\n",
      "Step #190500, epoch #47625, avg. train loss: 84.37434\n",
      "Step #190600, epoch #47650, avg. train loss: 84.08207\n",
      "Step #190700, epoch #47675, avg. train loss: 83.40247\n",
      "Step #190800, epoch #47700, avg. train loss: 83.90591\n",
      "Step #190900, epoch #47725, avg. train loss: 83.15834\n",
      "Step #191000, epoch #47750, avg. train loss: 83.11613\n",
      "Step #191100, epoch #47775, avg. train loss: 83.73550\n",
      "Step #191200, epoch #47800, avg. train loss: 83.65619\n",
      "Step #191300, epoch #47825, avg. train loss: 84.18398\n",
      "Step #191400, epoch #47850, avg. train loss: 83.69213\n",
      "Step #191500, epoch #47875, avg. train loss: 83.56477\n",
      "Step #191600, epoch #47900, avg. train loss: 83.42450\n",
      "Step #191700, epoch #47925, avg. train loss: 83.87494\n",
      "Step #191800, epoch #47950, avg. train loss: 83.82167\n",
      "Step #191900, epoch #47975, avg. train loss: 84.28369\n",
      "Step #192000, epoch #48000, avg. train loss: 84.14722\n",
      "Step #192100, epoch #48025, avg. train loss: 83.48370\n",
      "Step #192200, epoch #48050, avg. train loss: 82.93760\n",
      "Step #192300, epoch #48075, avg. train loss: 83.98273\n",
      "Step #192400, epoch #48100, avg. train loss: 84.59355\n",
      "Step #192500, epoch #48125, avg. train loss: 84.10185\n",
      "Step #192600, epoch #48150, avg. train loss: 83.51333\n",
      "Step #192700, epoch #48175, avg. train loss: 84.55100\n",
      "Step #192800, epoch #48200, avg. train loss: 83.67825\n",
      "Step #192900, epoch #48225, avg. train loss: 83.45531\n",
      "Step #193000, epoch #48250, avg. train loss: 83.89778\n",
      "Step #193100, epoch #48275, avg. train loss: 83.00262\n",
      "Step #193200, epoch #48300, avg. train loss: 84.57935\n",
      "Step #193300, epoch #48325, avg. train loss: 84.37416\n",
      "Step #193400, epoch #48350, avg. train loss: 84.88022\n",
      "Step #193500, epoch #48375, avg. train loss: 83.35541\n",
      "Step #193600, epoch #48400, avg. train loss: 83.46024\n",
      "Step #193700, epoch #48425, avg. train loss: 84.48461\n",
      "Step #193800, epoch #48450, avg. train loss: 84.07050\n",
      "Step #193900, epoch #48475, avg. train loss: 84.00473\n",
      "Step #194000, epoch #48500, avg. train loss: 84.14604\n",
      "Step #194100, epoch #48525, avg. train loss: 83.84933\n",
      "Step #194200, epoch #48550, avg. train loss: 84.06046\n",
      "Step #194300, epoch #48575, avg. train loss: 84.34702\n",
      "Step #194400, epoch #48600, avg. train loss: 83.69974\n",
      "Step #194500, epoch #48625, avg. train loss: 84.08949\n",
      "Step #194600, epoch #48650, avg. train loss: 83.98437\n",
      "Step #194700, epoch #48675, avg. train loss: 84.02380\n",
      "Step #194800, epoch #48700, avg. train loss: 83.49586\n",
      "Step #194900, epoch #48725, avg. train loss: 84.05110\n",
      "Step #195000, epoch #48750, avg. train loss: 83.21910\n",
      "Step #195100, epoch #48775, avg. train loss: 84.64951\n",
      "Step #195200, epoch #48800, avg. train loss: 83.92125\n",
      "Step #195300, epoch #48825, avg. train loss: 83.43592\n",
      "Step #195400, epoch #48850, avg. train loss: 83.25816\n",
      "Step #195500, epoch #48875, avg. train loss: 83.89565\n",
      "Step #195600, epoch #48900, avg. train loss: 84.16424\n",
      "Step #195700, epoch #48925, avg. train loss: 83.81976\n",
      "Step #195800, epoch #48950, avg. train loss: 84.52521\n",
      "Step #195900, epoch #48975, avg. train loss: 82.69733\n",
      "Step #196000, epoch #49000, avg. train loss: 84.35668\n",
      "Step #196100, epoch #49025, avg. train loss: 84.11454\n",
      "Step #196200, epoch #49050, avg. train loss: 82.91567\n",
      "Step #196300, epoch #49075, avg. train loss: 83.55486\n",
      "Step #196400, epoch #49100, avg. train loss: 83.12713\n",
      "Step #196500, epoch #49125, avg. train loss: 83.75365\n",
      "Step #196600, epoch #49150, avg. train loss: 83.65366\n",
      "Step #196700, epoch #49175, avg. train loss: 83.39332\n",
      "Step #196800, epoch #49200, avg. train loss: 83.46323\n",
      "Step #196900, epoch #49225, avg. train loss: 83.25723\n",
      "Step #197000, epoch #49250, avg. train loss: 83.45193\n",
      "Step #197100, epoch #49275, avg. train loss: 83.95350\n",
      "Step #197200, epoch #49300, avg. train loss: 82.66438\n",
      "Step #197300, epoch #49325, avg. train loss: 83.51900\n",
      "Step #197400, epoch #49350, avg. train loss: 83.48016\n",
      "Step #197500, epoch #49375, avg. train loss: 83.54552\n",
      "Step #197600, epoch #49400, avg. train loss: 83.95757\n",
      "Step #197700, epoch #49425, avg. train loss: 83.33734\n",
      "Step #197800, epoch #49450, avg. train loss: 84.64153\n",
      "Step #197900, epoch #49475, avg. train loss: 83.48041\n",
      "Step #198000, epoch #49500, avg. train loss: 83.86433\n",
      "Step #198100, epoch #49525, avg. train loss: 82.65139\n",
      "Step #198200, epoch #49550, avg. train loss: 83.68247\n",
      "Step #198300, epoch #49575, avg. train loss: 83.06673\n",
      "Step #198400, epoch #49600, avg. train loss: 84.32744\n",
      "Step #198500, epoch #49625, avg. train loss: 84.72675\n",
      "Step #198600, epoch #49650, avg. train loss: 83.76943\n",
      "Step #198700, epoch #49675, avg. train loss: 83.45035\n",
      "Step #198800, epoch #49700, avg. train loss: 83.26841\n",
      "Step #198900, epoch #49725, avg. train loss: 84.85296\n",
      "Step #199000, epoch #49750, avg. train loss: 83.28947\n",
      "Step #199100, epoch #49775, avg. train loss: 82.45369\n",
      "Step #199200, epoch #49800, avg. train loss: 84.28157\n",
      "Step #199300, epoch #49825, avg. train loss: 83.25213\n",
      "Step #199400, epoch #49850, avg. train loss: 83.46122\n",
      "Step #199500, epoch #49875, avg. train loss: 84.65532\n",
      "Step #199600, epoch #49900, avg. train loss: 83.67792\n",
      "Step #199700, epoch #49925, avg. train loss: 83.93346\n",
      "Step #199800, epoch #49950, avg. train loss: 83.65295\n",
      "Step #199900, epoch #49975, avg. train loss: 82.89584\n",
      "Step #200000, epoch #50000, avg. train loss: 84.30370\n",
      "Step #200100, epoch #50025, avg. train loss: 84.39621\n",
      "Step #200200, epoch #50050, avg. train loss: 83.12673\n",
      "Step #200300, epoch #50075, avg. train loss: 83.51953\n",
      "Step #200400, epoch #50100, avg. train loss: 83.62202\n",
      "Step #200500, epoch #50125, avg. train loss: 83.50505\n",
      "Step #200600, epoch #50150, avg. train loss: 84.54917\n",
      "Step #200700, epoch #50175, avg. train loss: 82.65601\n",
      "Step #200800, epoch #50200, avg. train loss: 82.94039\n",
      "Step #200900, epoch #50225, avg. train loss: 83.85288\n",
      "Step #201000, epoch #50250, avg. train loss: 83.82516\n",
      "Step #201100, epoch #50275, avg. train loss: 84.20071\n",
      "Step #201200, epoch #50300, avg. train loss: 83.46467\n",
      "Step #201300, epoch #50325, avg. train loss: 83.55633\n",
      "Step #201400, epoch #50350, avg. train loss: 84.02560\n",
      "Step #201500, epoch #50375, avg. train loss: 83.93992\n",
      "Step #201600, epoch #50400, avg. train loss: 83.08730\n",
      "Step #201700, epoch #50425, avg. train loss: 83.69845\n",
      "Step #201800, epoch #50450, avg. train loss: 83.36790\n",
      "Step #201900, epoch #50475, avg. train loss: 82.75611\n",
      "Step #202000, epoch #50500, avg. train loss: 82.84267\n",
      "Step #202100, epoch #50525, avg. train loss: 83.42888\n",
      "Step #202200, epoch #50550, avg. train loss: 83.84661\n",
      "Step #202300, epoch #50575, avg. train loss: 83.67975\n",
      "Step #202400, epoch #50600, avg. train loss: 83.70647\n",
      "Step #202500, epoch #50625, avg. train loss: 83.83099\n",
      "Step #202600, epoch #50650, avg. train loss: 83.61948\n",
      "Step #202700, epoch #50675, avg. train loss: 84.31162\n",
      "Step #202800, epoch #50700, avg. train loss: 83.46903\n",
      "Step #202900, epoch #50725, avg. train loss: 83.08076\n",
      "Step #203000, epoch #50750, avg. train loss: 83.50083\n",
      "Step #203100, epoch #50775, avg. train loss: 82.80008\n",
      "Step #203200, epoch #50800, avg. train loss: 84.41888\n",
      "Step #203300, epoch #50825, avg. train loss: 84.06474\n",
      "Step #203400, epoch #50850, avg. train loss: 82.76745\n",
      "Step #203500, epoch #50875, avg. train loss: 83.45332\n",
      "Step #203600, epoch #50900, avg. train loss: 83.21669\n",
      "Step #203700, epoch #50925, avg. train loss: 83.64480\n",
      "Step #203800, epoch #50950, avg. train loss: 82.60588\n",
      "Step #203900, epoch #50975, avg. train loss: 83.64059\n",
      "Step #204000, epoch #51000, avg. train loss: 83.45972\n",
      "Step #204100, epoch #51025, avg. train loss: 83.14107\n",
      "Step #204200, epoch #51050, avg. train loss: 83.55001\n",
      "Step #204300, epoch #51075, avg. train loss: 82.73616\n",
      "Step #204400, epoch #51100, avg. train loss: 82.70323\n",
      "Step #204500, epoch #51125, avg. train loss: 82.95997\n",
      "Step #204600, epoch #51150, avg. train loss: 84.15718\n",
      "Step #204700, epoch #51175, avg. train loss: 82.87169\n",
      "Step #204800, epoch #51200, avg. train loss: 83.39362\n",
      "Step #204900, epoch #51225, avg. train loss: 82.75182\n",
      "Step #205000, epoch #51250, avg. train loss: 84.01578\n",
      "Step #205100, epoch #51275, avg. train loss: 83.37950\n",
      "Step #205200, epoch #51300, avg. train loss: 83.24487\n",
      "Step #205300, epoch #51325, avg. train loss: 83.16549\n",
      "Step #205400, epoch #51350, avg. train loss: 83.01639\n",
      "Step #205500, epoch #51375, avg. train loss: 82.79330\n",
      "Step #205600, epoch #51400, avg. train loss: 83.49110\n",
      "Step #205700, epoch #51425, avg. train loss: 82.90900\n",
      "Step #205800, epoch #51450, avg. train loss: 82.93210\n",
      "Step #205900, epoch #51475, avg. train loss: 82.69609\n",
      "Step #206000, epoch #51500, avg. train loss: 83.99522\n",
      "Step #206100, epoch #51525, avg. train loss: 82.67815\n",
      "Step #206200, epoch #51550, avg. train loss: 83.60417\n",
      "Step #206300, epoch #51575, avg. train loss: 83.92564\n",
      "Step #206400, epoch #51600, avg. train loss: 83.63627\n",
      "Step #206500, epoch #51625, avg. train loss: 82.81997\n",
      "Step #206600, epoch #51650, avg. train loss: 84.18962\n",
      "Step #206700, epoch #51675, avg. train loss: 82.98254\n",
      "Step #206800, epoch #51700, avg. train loss: 82.97905\n",
      "Step #206900, epoch #51725, avg. train loss: 83.23880\n",
      "Step #207000, epoch #51750, avg. train loss: 84.33076\n",
      "Step #207100, epoch #51775, avg. train loss: 83.89407\n",
      "Step #207200, epoch #51800, avg. train loss: 83.25173\n",
      "Step #207300, epoch #51825, avg. train loss: 83.85368\n",
      "Step #207400, epoch #51850, avg. train loss: 83.15537\n",
      "Step #207500, epoch #51875, avg. train loss: 83.01654\n",
      "Step #207600, epoch #51900, avg. train loss: 84.36419\n",
      "Step #207700, epoch #51925, avg. train loss: 83.83336\n",
      "Step #207800, epoch #51950, avg. train loss: 83.45224\n",
      "Step #207900, epoch #51975, avg. train loss: 83.63504\n",
      "Step #208000, epoch #52000, avg. train loss: 83.42435\n",
      "Step #208100, epoch #52025, avg. train loss: 83.09628\n",
      "Step #208200, epoch #52050, avg. train loss: 82.27805\n",
      "Step #208300, epoch #52075, avg. train loss: 83.22977\n",
      "Step #208400, epoch #52100, avg. train loss: 83.26305\n",
      "Step #208500, epoch #52125, avg. train loss: 84.13679\n",
      "Step #208600, epoch #52150, avg. train loss: 83.23405\n",
      "Step #208700, epoch #52175, avg. train loss: 82.77575\n",
      "Step #208800, epoch #52200, avg. train loss: 84.03826\n",
      "Step #208900, epoch #52225, avg. train loss: 82.09583\n",
      "Step #209000, epoch #52250, avg. train loss: 84.20066\n",
      "Step #209100, epoch #52275, avg. train loss: 83.05340\n",
      "Step #209200, epoch #52300, avg. train loss: 82.94365\n",
      "Step #209300, epoch #52325, avg. train loss: 82.97425\n",
      "Step #209400, epoch #52350, avg. train loss: 83.39198\n",
      "Step #209500, epoch #52375, avg. train loss: 83.55850\n",
      "Step #209600, epoch #52400, avg. train loss: 83.73689\n",
      "Step #209700, epoch #52425, avg. train loss: 83.42641\n",
      "Step #209800, epoch #52450, avg. train loss: 83.13860\n",
      "Step #209900, epoch #52475, avg. train loss: 82.42876\n",
      "Step #210000, epoch #52500, avg. train loss: 82.90048\n",
      "Step #210100, epoch #52525, avg. train loss: 83.44192\n",
      "Step #210200, epoch #52550, avg. train loss: 82.95735\n",
      "Step #210300, epoch #52575, avg. train loss: 83.02708\n",
      "Step #210400, epoch #52600, avg. train loss: 82.98939\n",
      "Step #210500, epoch #52625, avg. train loss: 83.33804\n",
      "Step #210600, epoch #52650, avg. train loss: 83.35815\n",
      "Step #210700, epoch #52675, avg. train loss: 83.49100\n",
      "Step #210800, epoch #52700, avg. train loss: 82.93243\n",
      "Step #210900, epoch #52725, avg. train loss: 83.75137\n",
      "Step #211000, epoch #52750, avg. train loss: 82.38350\n",
      "Step #211100, epoch #52775, avg. train loss: 83.26124\n",
      "Step #211200, epoch #52800, avg. train loss: 83.74609\n",
      "Step #211300, epoch #52825, avg. train loss: 83.33996\n",
      "Step #211400, epoch #52850, avg. train loss: 84.22080\n",
      "Step #211500, epoch #52875, avg. train loss: 83.26318\n",
      "Step #211600, epoch #52900, avg. train loss: 83.38206\n",
      "Step #211700, epoch #52925, avg. train loss: 82.69524\n",
      "Step #211800, epoch #52950, avg. train loss: 82.80871\n",
      "Step #211900, epoch #52975, avg. train loss: 83.42185\n",
      "Step #212000, epoch #53000, avg. train loss: 83.55251\n",
      "Step #212100, epoch #53025, avg. train loss: 83.26524\n",
      "Step #212200, epoch #53050, avg. train loss: 82.85456\n",
      "Step #212300, epoch #53075, avg. train loss: 82.87536\n",
      "Step #212400, epoch #53100, avg. train loss: 83.69897\n",
      "Step #212500, epoch #53125, avg. train loss: 83.67041\n",
      "Step #212600, epoch #53150, avg. train loss: 82.82459\n",
      "Step #212700, epoch #53175, avg. train loss: 83.07402\n",
      "Step #212800, epoch #53200, avg. train loss: 82.84861\n",
      "Step #212900, epoch #53225, avg. train loss: 83.07193\n",
      "Step #213000, epoch #53250, avg. train loss: 83.01747\n",
      "Step #213100, epoch #53275, avg. train loss: 84.05877\n",
      "Step #213200, epoch #53300, avg. train loss: 83.00866\n",
      "Step #213300, epoch #53325, avg. train loss: 83.17426\n",
      "Step #213400, epoch #53350, avg. train loss: 83.31885\n",
      "Step #213500, epoch #53375, avg. train loss: 82.96355\n",
      "Step #213600, epoch #53400, avg. train loss: 83.51054\n",
      "Step #213700, epoch #53425, avg. train loss: 83.58323\n",
      "Step #213800, epoch #53450, avg. train loss: 83.30593\n",
      "Step #213900, epoch #53475, avg. train loss: 83.34069\n",
      "Step #214000, epoch #53500, avg. train loss: 83.01787\n",
      "Step #214100, epoch #53525, avg. train loss: 82.87033\n",
      "Step #214200, epoch #53550, avg. train loss: 83.85578\n",
      "Step #214300, epoch #53575, avg. train loss: 83.24104\n",
      "Step #214400, epoch #53600, avg. train loss: 83.71107\n",
      "Step #214500, epoch #53625, avg. train loss: 82.94263\n",
      "Step #214600, epoch #53650, avg. train loss: 83.68027\n",
      "Step #214700, epoch #53675, avg. train loss: 83.09097\n",
      "Step #214800, epoch #53700, avg. train loss: 83.31953\n",
      "Step #214900, epoch #53725, avg. train loss: 83.48768\n",
      "Step #215000, epoch #53750, avg. train loss: 82.57904\n",
      "Step #215100, epoch #53775, avg. train loss: 83.98817\n",
      "Step #215200, epoch #53800, avg. train loss: 82.94472\n",
      "Step #215300, epoch #53825, avg. train loss: 83.60133\n",
      "Step #215400, epoch #53850, avg. train loss: 84.05569\n",
      "Step #215500, epoch #53875, avg. train loss: 82.38823\n",
      "Step #215600, epoch #53900, avg. train loss: 82.42254\n",
      "Step #215700, epoch #53925, avg. train loss: 82.84824\n",
      "Step #215800, epoch #53950, avg. train loss: 82.73055\n",
      "Step #215900, epoch #53975, avg. train loss: 83.77699\n",
      "Step #216000, epoch #54000, avg. train loss: 83.80876\n",
      "Step #216100, epoch #54025, avg. train loss: 83.46123\n",
      "Step #216200, epoch #54050, avg. train loss: 82.90339\n",
      "Step #216300, epoch #54075, avg. train loss: 83.11170\n",
      "Step #216400, epoch #54100, avg. train loss: 82.48184\n",
      "Step #216500, epoch #54125, avg. train loss: 83.38316\n",
      "Step #216600, epoch #54150, avg. train loss: 82.83825\n",
      "Step #216700, epoch #54175, avg. train loss: 83.62802\n",
      "Step #216800, epoch #54200, avg. train loss: 83.76862\n",
      "Step #216900, epoch #54225, avg. train loss: 83.58141\n",
      "Step #217000, epoch #54250, avg. train loss: 83.77566\n",
      "Step #217100, epoch #54275, avg. train loss: 83.41082\n",
      "Step #217200, epoch #54300, avg. train loss: 83.04613\n",
      "Step #217300, epoch #54325, avg. train loss: 82.90741\n",
      "Step #217400, epoch #54350, avg. train loss: 83.92815\n",
      "Step #217500, epoch #54375, avg. train loss: 82.62285\n",
      "Step #217600, epoch #54400, avg. train loss: 83.16617\n",
      "Step #217700, epoch #54425, avg. train loss: 83.02655\n",
      "Step #217800, epoch #54450, avg. train loss: 82.63712\n",
      "Step #217900, epoch #54475, avg. train loss: 83.04673\n",
      "Step #218000, epoch #54500, avg. train loss: 82.45905\n",
      "Step #218100, epoch #54525, avg. train loss: 83.94751\n",
      "Step #218200, epoch #54550, avg. train loss: 83.59027\n",
      "Step #218300, epoch #54575, avg. train loss: 81.98648\n",
      "Step #218400, epoch #54600, avg. train loss: 83.32941\n",
      "Step #218500, epoch #54625, avg. train loss: 83.44845\n",
      "Step #218600, epoch #54650, avg. train loss: 83.32973\n",
      "Step #218700, epoch #54675, avg. train loss: 82.72543\n",
      "Step #218800, epoch #54700, avg. train loss: 83.50576\n",
      "Step #218900, epoch #54725, avg. train loss: 84.05857\n",
      "Step #219000, epoch #54750, avg. train loss: 83.04350\n",
      "Step #219100, epoch #54775, avg. train loss: 82.93460\n",
      "Step #219200, epoch #54800, avg. train loss: 83.47684\n",
      "Step #219300, epoch #54825, avg. train loss: 82.59058\n",
      "Step #219400, epoch #54850, avg. train loss: 82.29830\n",
      "Step #219500, epoch #54875, avg. train loss: 83.12559\n",
      "Step #219600, epoch #54900, avg. train loss: 82.44389\n",
      "Step #219700, epoch #54925, avg. train loss: 83.07150\n",
      "Step #219800, epoch #54950, avg. train loss: 83.63245\n",
      "Step #219900, epoch #54975, avg. train loss: 82.92789\n",
      "Step #220000, epoch #55000, avg. train loss: 82.47301\n",
      "Step #220100, epoch #55025, avg. train loss: 83.36401\n",
      "Step #220200, epoch #55050, avg. train loss: 83.28736\n",
      "Step #220300, epoch #55075, avg. train loss: 82.76279\n",
      "Step #220400, epoch #55100, avg. train loss: 83.10152\n",
      "Step #220500, epoch #55125, avg. train loss: 82.49473\n",
      "Step #220600, epoch #55150, avg. train loss: 83.04893\n",
      "Step #220700, epoch #55175, avg. train loss: 83.19807\n",
      "Step #220800, epoch #55200, avg. train loss: 83.41148\n",
      "Step #220900, epoch #55225, avg. train loss: 83.23347\n",
      "Step #221000, epoch #55250, avg. train loss: 81.91672\n",
      "Step #221100, epoch #55275, avg. train loss: 82.83294\n",
      "Step #221200, epoch #55300, avg. train loss: 82.17080\n",
      "Step #221300, epoch #55325, avg. train loss: 82.49314\n",
      "Step #221400, epoch #55350, avg. train loss: 82.80588\n",
      "Step #221500, epoch #55375, avg. train loss: 82.57908\n",
      "Step #221600, epoch #55400, avg. train loss: 82.69957\n",
      "Step #221700, epoch #55425, avg. train loss: 82.09683\n",
      "Step #221800, epoch #55450, avg. train loss: 82.12766\n",
      "Step #221900, epoch #55475, avg. train loss: 82.39939\n",
      "Step #222000, epoch #55500, avg. train loss: 82.74677\n",
      "Step #222100, epoch #55525, avg. train loss: 82.26566\n",
      "Step #222200, epoch #55550, avg. train loss: 83.06808\n",
      "Step #222300, epoch #55575, avg. train loss: 83.37724\n",
      "Step #222400, epoch #55600, avg. train loss: 83.40968\n",
      "Step #222500, epoch #55625, avg. train loss: 82.79317\n",
      "Step #222600, epoch #55650, avg. train loss: 83.92461\n",
      "Step #222700, epoch #55675, avg. train loss: 83.30598\n",
      "Step #222800, epoch #55700, avg. train loss: 84.68182\n",
      "Step #222900, epoch #55725, avg. train loss: 83.34616\n",
      "Step #223000, epoch #55750, avg. train loss: 82.49612\n",
      "Step #223100, epoch #55775, avg. train loss: 83.96959\n",
      "Step #223200, epoch #55800, avg. train loss: 82.96523\n",
      "Step #223300, epoch #55825, avg. train loss: 82.97618\n",
      "Step #223400, epoch #55850, avg. train loss: 83.19072\n",
      "Step #223500, epoch #55875, avg. train loss: 83.58192\n",
      "Step #223600, epoch #55900, avg. train loss: 82.86729\n",
      "Step #223700, epoch #55925, avg. train loss: 82.69814\n",
      "Step #223800, epoch #55950, avg. train loss: 83.07153\n",
      "Step #223900, epoch #55975, avg. train loss: 82.70351\n",
      "Step #224000, epoch #56000, avg. train loss: 82.47546\n",
      "Step #224100, epoch #56025, avg. train loss: 82.35754\n",
      "Step #224200, epoch #56050, avg. train loss: 82.46928\n",
      "Step #224300, epoch #56075, avg. train loss: 83.04184\n",
      "Step #224400, epoch #56100, avg. train loss: 81.99298\n",
      "Step #224500, epoch #56125, avg. train loss: 83.79272\n",
      "Step #224600, epoch #56150, avg. train loss: 82.84710\n",
      "Step #224700, epoch #56175, avg. train loss: 83.44166\n",
      "Step #224800, epoch #56200, avg. train loss: 82.92188\n",
      "Step #224900, epoch #56225, avg. train loss: 82.59190\n",
      "Step #225000, epoch #56250, avg. train loss: 83.12238\n",
      "Step #225100, epoch #56275, avg. train loss: 82.54458\n",
      "Step #225200, epoch #56300, avg. train loss: 82.33662\n",
      "Step #225300, epoch #56325, avg. train loss: 82.25086\n",
      "Step #225400, epoch #56350, avg. train loss: 82.84252\n",
      "Step #225500, epoch #56375, avg. train loss: 82.95108\n",
      "Step #225600, epoch #56400, avg. train loss: 82.53893\n",
      "Step #225700, epoch #56425, avg. train loss: 82.95979\n",
      "Step #225800, epoch #56450, avg. train loss: 82.39798\n",
      "Step #225900, epoch #56475, avg. train loss: 82.44166\n",
      "Step #226000, epoch #56500, avg. train loss: 83.22342\n",
      "Step #226100, epoch #56525, avg. train loss: 83.33496\n",
      "Step #226200, epoch #56550, avg. train loss: 83.15786\n",
      "Step #226300, epoch #56575, avg. train loss: 83.81786\n",
      "Step #226400, epoch #56600, avg. train loss: 82.27814\n",
      "Step #226500, epoch #56625, avg. train loss: 83.08950\n",
      "Step #226600, epoch #56650, avg. train loss: 82.25124\n",
      "Step #226700, epoch #56675, avg. train loss: 83.09966\n",
      "Step #226800, epoch #56700, avg. train loss: 83.49202\n",
      "Step #226900, epoch #56725, avg. train loss: 82.91671\n",
      "Step #227000, epoch #56750, avg. train loss: 82.53118\n",
      "Step #227100, epoch #56775, avg. train loss: 82.85844\n",
      "Step #227200, epoch #56800, avg. train loss: 82.01963\n",
      "Step #227300, epoch #56825, avg. train loss: 83.11029\n",
      "Step #227400, epoch #56850, avg. train loss: 82.96024\n",
      "Step #227500, epoch #56875, avg. train loss: 82.68040\n",
      "Step #227600, epoch #56900, avg. train loss: 82.31504\n",
      "Step #227700, epoch #56925, avg. train loss: 83.00576\n",
      "Step #227800, epoch #56950, avg. train loss: 83.09591\n",
      "Step #227900, epoch #56975, avg. train loss: 82.73228\n",
      "Step #228300, epoch #57075, avg. train loss: 82.62398\n",
      "Step #228400, epoch #57100, avg. train loss: 83.41224\n",
      "Step #228500, epoch #57125, avg. train loss: 81.95725\n",
      "Step #228600, epoch #57150, avg. train loss: 83.64120\n",
      "Step #228700, epoch #57175, avg. train loss: 83.67738\n",
      "Step #228800, epoch #57200, avg. train loss: 83.08698\n",
      "Step #228900, epoch #57225, avg. train loss: 82.37965\n",
      "Step #229000, epoch #57250, avg. train loss: 83.50977\n",
      "Step #229100, epoch #57275, avg. train loss: 82.04391\n",
      "Step #229200, epoch #57300, avg. train loss: 82.82951\n",
      "Step #229300, epoch #57325, avg. train loss: 82.67564\n",
      "Step #229400, epoch #57350, avg. train loss: 82.63454\n",
      "Step #229500, epoch #57375, avg. train loss: 82.66002\n",
      "Step #229600, epoch #57400, avg. train loss: 82.32686\n",
      "Step #229700, epoch #57425, avg. train loss: 83.64584\n",
      "Step #229800, epoch #57450, avg. train loss: 82.41074\n",
      "Step #229900, epoch #57475, avg. train loss: 83.26581\n",
      "Step #230000, epoch #57500, avg. train loss: 82.90701\n",
      "Step #230100, epoch #57525, avg. train loss: 82.36463\n",
      "Step #230200, epoch #57550, avg. train loss: 82.85508\n",
      "Step #230300, epoch #57575, avg. train loss: 83.29842\n",
      "Step #230400, epoch #57600, avg. train loss: 82.62807\n",
      "Step #230500, epoch #57625, avg. train loss: 81.97778\n",
      "Step #230600, epoch #57650, avg. train loss: 82.86184\n",
      "Step #230700, epoch #57675, avg. train loss: 82.96918\n",
      "Step #230800, epoch #57700, avg. train loss: 82.99274\n",
      "Step #230900, epoch #57725, avg. train loss: 82.17947\n",
      "Step #231000, epoch #57750, avg. train loss: 82.27261\n",
      "Step #231100, epoch #57775, avg. train loss: 83.22514\n",
      "Step #231200, epoch #57800, avg. train loss: 83.00151\n",
      "Step #231300, epoch #57825, avg. train loss: 83.09632\n",
      "Step #231400, epoch #57850, avg. train loss: 82.43983\n",
      "Step #231500, epoch #57875, avg. train loss: 82.69038\n",
      "Step #231600, epoch #57900, avg. train loss: 82.86883\n",
      "Step #231700, epoch #57925, avg. train loss: 83.28169\n",
      "Step #231800, epoch #57950, avg. train loss: 82.05560\n",
      "Step #231900, epoch #57975, avg. train loss: 82.83010\n",
      "Step #232000, epoch #58000, avg. train loss: 81.85975\n",
      "Step #232100, epoch #58025, avg. train loss: 82.70860\n",
      "Step #232200, epoch #58050, avg. train loss: 82.48899\n",
      "Step #232300, epoch #58075, avg. train loss: 83.42258\n",
      "Step #232400, epoch #58100, avg. train loss: 83.00100\n",
      "Step #232500, epoch #58125, avg. train loss: 81.73831\n",
      "Step #232600, epoch #58150, avg. train loss: 82.34195\n",
      "Step #232700, epoch #58175, avg. train loss: 82.60711\n",
      "Step #232800, epoch #58200, avg. train loss: 83.16794\n",
      "Step #232900, epoch #58225, avg. train loss: 82.70059\n",
      "Step #233000, epoch #58250, avg. train loss: 82.67422\n",
      "Step #233100, epoch #58275, avg. train loss: 82.63745\n",
      "Step #233200, epoch #58300, avg. train loss: 83.31186\n",
      "Step #233300, epoch #58325, avg. train loss: 82.62629\n",
      "Step #233400, epoch #58350, avg. train loss: 83.08929\n",
      "Step #233500, epoch #58375, avg. train loss: 81.61729\n",
      "Step #233600, epoch #58400, avg. train loss: 82.11805\n",
      "Step #233700, epoch #58425, avg. train loss: 82.52428\n",
      "Step #233800, epoch #58450, avg. train loss: 82.33102\n",
      "Step #233900, epoch #58475, avg. train loss: 82.10862\n",
      "Step #234000, epoch #58500, avg. train loss: 83.20750\n",
      "Step #234100, epoch #58525, avg. train loss: 82.77630\n",
      "Step #234200, epoch #58550, avg. train loss: 82.43076\n",
      "Step #234300, epoch #58575, avg. train loss: 82.23061\n",
      "Step #234400, epoch #58600, avg. train loss: 81.97506\n",
      "Step #234500, epoch #58625, avg. train loss: 82.13215\n",
      "Step #234600, epoch #58650, avg. train loss: 82.79736\n",
      "Step #234700, epoch #58675, avg. train loss: 82.22570\n",
      "Step #234800, epoch #58700, avg. train loss: 82.88739\n",
      "Step #234900, epoch #58725, avg. train loss: 82.95559\n",
      "Step #235000, epoch #58750, avg. train loss: 82.31594\n",
      "Step #235100, epoch #58775, avg. train loss: 82.30293\n",
      "Step #235200, epoch #58800, avg. train loss: 82.76077\n",
      "Step #235300, epoch #58825, avg. train loss: 82.88033\n",
      "Step #235400, epoch #58850, avg. train loss: 82.28748\n",
      "Step #235500, epoch #58875, avg. train loss: 82.86923\n",
      "Step #235600, epoch #58900, avg. train loss: 83.15823\n",
      "Step #235700, epoch #58925, avg. train loss: 82.05269\n",
      "Step #235800, epoch #58950, avg. train loss: 82.61496\n",
      "Step #235900, epoch #58975, avg. train loss: 82.72748\n",
      "Step #236000, epoch #59000, avg. train loss: 82.30498\n",
      "Step #236100, epoch #59025, avg. train loss: 82.32098\n",
      "Step #236200, epoch #59050, avg. train loss: 82.72324\n",
      "Step #236300, epoch #59075, avg. train loss: 82.39492\n",
      "Step #236400, epoch #59100, avg. train loss: 82.35508\n",
      "Step #236500, epoch #59125, avg. train loss: 82.53157\n",
      "Step #236600, epoch #59150, avg. train loss: 82.29819\n",
      "Step #236700, epoch #59175, avg. train loss: 82.29999\n",
      "Step #236800, epoch #59200, avg. train loss: 83.19593\n",
      "Step #236900, epoch #59225, avg. train loss: 82.12208\n",
      "Step #237000, epoch #59250, avg. train loss: 82.16031\n",
      "Step #237100, epoch #59275, avg. train loss: 82.94333\n",
      "Step #237200, epoch #59300, avg. train loss: 82.91723\n",
      "Step #237300, epoch #59325, avg. train loss: 82.57114\n",
      "Step #237400, epoch #59350, avg. train loss: 82.42751\n",
      "Step #237500, epoch #59375, avg. train loss: 82.03620\n",
      "Step #237600, epoch #59400, avg. train loss: 81.92696\n",
      "Step #237700, epoch #59425, avg. train loss: 82.16512\n",
      "Step #237800, epoch #59450, avg. train loss: 83.05671\n",
      "Step #237900, epoch #59475, avg. train loss: 82.83758\n",
      "Step #238000, epoch #59500, avg. train loss: 82.43969\n",
      "Step #238100, epoch #59525, avg. train loss: 82.10651\n",
      "Step #238200, epoch #59550, avg. train loss: 82.98298\n",
      "Step #238300, epoch #59575, avg. train loss: 82.45476\n",
      "Step #238400, epoch #59600, avg. train loss: 81.36643\n",
      "Step #238500, epoch #59625, avg. train loss: 82.80019\n",
      "Step #238600, epoch #59650, avg. train loss: 82.44664\n",
      "Step #238700, epoch #59675, avg. train loss: 81.88197\n",
      "Step #238800, epoch #59700, avg. train loss: 82.58125\n",
      "Step #238900, epoch #59725, avg. train loss: 82.48336\n",
      "Step #239000, epoch #59750, avg. train loss: 82.04252\n",
      "Step #239100, epoch #59775, avg. train loss: 82.71638\n",
      "Step #239200, epoch #59800, avg. train loss: 82.47196\n",
      "Step #239300, epoch #59825, avg. train loss: 82.86703\n",
      "Step #239400, epoch #59850, avg. train loss: 82.47161\n",
      "Step #239500, epoch #59875, avg. train loss: 82.67659\n",
      "Step #239600, epoch #59900, avg. train loss: 83.04230\n",
      "Step #239700, epoch #59925, avg. train loss: 82.40889\n",
      "Step #239800, epoch #59950, avg. train loss: 82.73015\n",
      "Step #239900, epoch #59975, avg. train loss: 82.74234\n",
      "Step #240000, epoch #60000, avg. train loss: 82.05129\n",
      "Step #240100, epoch #60025, avg. train loss: 82.53118\n",
      "Step #240200, epoch #60050, avg. train loss: 81.99285\n",
      "Step #240300, epoch #60075, avg. train loss: 81.48912\n",
      "Step #240400, epoch #60100, avg. train loss: 82.74701\n",
      "Step #240500, epoch #60125, avg. train loss: 81.94281\n",
      "Step #240600, epoch #60150, avg. train loss: 82.84591\n",
      "Step #240700, epoch #60175, avg. train loss: 82.76620\n",
      "Step #240800, epoch #60200, avg. train loss: 82.47255\n",
      "Step #240900, epoch #60225, avg. train loss: 82.35786\n",
      "Step #241000, epoch #60250, avg. train loss: 82.24610\n",
      "Step #241100, epoch #60275, avg. train loss: 82.93137\n",
      "Step #241200, epoch #60300, avg. train loss: 82.18433\n",
      "Step #241300, epoch #60325, avg. train loss: 82.76099\n",
      "Step #241400, epoch #60350, avg. train loss: 83.00458\n",
      "Step #241500, epoch #60375, avg. train loss: 82.56770\n",
      "Step #241600, epoch #60400, avg. train loss: 82.05487\n",
      "Step #241700, epoch #60425, avg. train loss: 82.01645\n",
      "Step #241800, epoch #60450, avg. train loss: 82.53775\n",
      "Step #241900, epoch #60475, avg. train loss: 82.22182\n",
      "Step #242000, epoch #60500, avg. train loss: 81.95779\n",
      "Step #242100, epoch #60525, avg. train loss: 80.54907\n",
      "Step #242200, epoch #60550, avg. train loss: 82.28489\n",
      "Step #242300, epoch #60575, avg. train loss: 82.00628\n",
      "Step #242400, epoch #60600, avg. train loss: 81.51077\n",
      "Step #242500, epoch #60625, avg. train loss: 81.90116\n",
      "Step #242600, epoch #60650, avg. train loss: 83.06686\n",
      "Step #242700, epoch #60675, avg. train loss: 82.95493\n",
      "Step #242800, epoch #60700, avg. train loss: 83.23109\n",
      "Step #242900, epoch #60725, avg. train loss: 82.12956\n",
      "Step #243000, epoch #60750, avg. train loss: 82.82175\n",
      "Step #243100, epoch #60775, avg. train loss: 83.79296\n",
      "Step #243200, epoch #60800, avg. train loss: 81.73750\n",
      "Step #243300, epoch #60825, avg. train loss: 82.14297\n",
      "Step #243400, epoch #60850, avg. train loss: 82.40254\n",
      "Step #243500, epoch #60875, avg. train loss: 81.62857\n",
      "Step #243600, epoch #60900, avg. train loss: 82.21625\n",
      "Step #243700, epoch #60925, avg. train loss: 82.22210\n",
      "Step #243800, epoch #60950, avg. train loss: 82.97449\n",
      "Step #243900, epoch #60975, avg. train loss: 82.24931\n",
      "Step #244000, epoch #61000, avg. train loss: 82.60644\n",
      "Step #244100, epoch #61025, avg. train loss: 82.28188\n",
      "Step #244200, epoch #61050, avg. train loss: 81.82029\n",
      "Step #244300, epoch #61075, avg. train loss: 81.59598\n",
      "Step #244400, epoch #61100, avg. train loss: 82.14641\n",
      "Step #244500, epoch #61125, avg. train loss: 82.86526\n",
      "Step #244600, epoch #61150, avg. train loss: 82.43765\n",
      "Step #244700, epoch #61175, avg. train loss: 82.00851\n",
      "Step #244800, epoch #61200, avg. train loss: 81.99599\n",
      "Step #244900, epoch #61225, avg. train loss: 82.90876\n",
      "Step #245000, epoch #61250, avg. train loss: 81.91274\n",
      "Step #245100, epoch #61275, avg. train loss: 81.97601\n",
      "Step #245200, epoch #61300, avg. train loss: 82.58279\n",
      "Step #245300, epoch #61325, avg. train loss: 81.61326\n",
      "Step #245400, epoch #61350, avg. train loss: 82.32770\n",
      "Step #245500, epoch #61375, avg. train loss: 82.00579\n",
      "Step #245600, epoch #61400, avg. train loss: 82.17344\n",
      "Step #245700, epoch #61425, avg. train loss: 83.09194\n",
      "Step #245800, epoch #61450, avg. train loss: 82.38229\n",
      "Step #245900, epoch #61475, avg. train loss: 82.47881\n",
      "Step #246000, epoch #61500, avg. train loss: 82.14529\n",
      "Step #246100, epoch #61525, avg. train loss: 82.77010\n",
      "Step #246200, epoch #61550, avg. train loss: 82.03043\n",
      "Step #246300, epoch #61575, avg. train loss: 83.06366\n",
      "Step #246400, epoch #61600, avg. train loss: 82.74804\n",
      "Step #246500, epoch #61625, avg. train loss: 82.62145\n",
      "Step #246600, epoch #61650, avg. train loss: 81.99375\n",
      "Step #246700, epoch #61675, avg. train loss: 82.58658\n",
      "Step #246800, epoch #61700, avg. train loss: 81.38300\n",
      "Step #246900, epoch #61725, avg. train loss: 82.89830\n",
      "Step #247000, epoch #61750, avg. train loss: 82.05027\n",
      "Step #247100, epoch #61775, avg. train loss: 82.35320\n",
      "Step #247200, epoch #61800, avg. train loss: 82.56818\n",
      "Step #247300, epoch #61825, avg. train loss: 82.32040\n",
      "Step #247400, epoch #61850, avg. train loss: 82.29762\n",
      "Step #247500, epoch #61875, avg. train loss: 82.26524\n",
      "Step #247600, epoch #61900, avg. train loss: 82.38362\n",
      "Step #247700, epoch #61925, avg. train loss: 82.35782\n",
      "Step #247800, epoch #61950, avg. train loss: 82.52828\n",
      "Step #247900, epoch #61975, avg. train loss: 81.68423\n",
      "Step #248000, epoch #62000, avg. train loss: 82.20039\n",
      "Step #248100, epoch #62025, avg. train loss: 83.37597\n",
      "Step #248200, epoch #62050, avg. train loss: 82.83389\n",
      "Step #248300, epoch #62075, avg. train loss: 82.07089\n",
      "Step #248400, epoch #62100, avg. train loss: 81.84031\n",
      "Step #248500, epoch #62125, avg. train loss: 82.79736\n",
      "Step #248600, epoch #62150, avg. train loss: 82.44211\n",
      "Step #248700, epoch #62175, avg. train loss: 82.05285\n",
      "Step #248800, epoch #62200, avg. train loss: 83.36649\n",
      "Step #248900, epoch #62225, avg. train loss: 82.61453\n",
      "Step #249000, epoch #62250, avg. train loss: 82.79098\n",
      "Step #249100, epoch #62275, avg. train loss: 82.52171\n",
      "Step #249200, epoch #62300, avg. train loss: 82.35719\n",
      "Step #249300, epoch #62325, avg. train loss: 82.53904\n",
      "Step #249400, epoch #62350, avg. train loss: 81.94141\n",
      "Step #249500, epoch #62375, avg. train loss: 82.52461\n",
      "Step #249600, epoch #62400, avg. train loss: 82.39405\n",
      "Step #249700, epoch #62425, avg. train loss: 81.64206\n",
      "Step #249800, epoch #62450, avg. train loss: 82.44862\n",
      "Step #249900, epoch #62475, avg. train loss: 82.75475\n",
      "Step #250000, epoch #62500, avg. train loss: 82.22785\n",
      "Step #250100, epoch #62525, avg. train loss: 81.26311\n",
      "Step #250200, epoch #62550, avg. train loss: 82.47701\n",
      "Step #250300, epoch #62575, avg. train loss: 81.81553\n",
      "Step #250400, epoch #62600, avg. train loss: 82.47437\n",
      "Step #250500, epoch #62625, avg. train loss: 82.32252\n",
      "Step #250600, epoch #62650, avg. train loss: 82.67157\n",
      "Step #250700, epoch #62675, avg. train loss: 81.64119\n",
      "Step #250800, epoch #62700, avg. train loss: 81.96393\n",
      "Step #250900, epoch #62725, avg. train loss: 81.72049\n",
      "Step #251000, epoch #62750, avg. train loss: 81.82492\n",
      "Step #251100, epoch #62775, avg. train loss: 82.18314\n",
      "Step #251200, epoch #62800, avg. train loss: 81.55846\n",
      "Step #251300, epoch #62825, avg. train loss: 81.96895\n",
      "Step #251400, epoch #62850, avg. train loss: 82.03545\n",
      "Step #251500, epoch #62875, avg. train loss: 81.88405\n",
      "Step #251600, epoch #62900, avg. train loss: 82.43008\n",
      "Step #251700, epoch #62925, avg. train loss: 81.88100\n",
      "Step #251800, epoch #62950, avg. train loss: 83.42001\n",
      "Step #251900, epoch #62975, avg. train loss: 81.98684\n",
      "Step #252000, epoch #63000, avg. train loss: 82.04153\n",
      "Step #252100, epoch #63025, avg. train loss: 82.40563\n",
      "Step #252200, epoch #63050, avg. train loss: 82.67481\n",
      "Step #252300, epoch #63075, avg. train loss: 81.60295\n",
      "Step #252400, epoch #63100, avg. train loss: 82.53606\n",
      "Step #252500, epoch #63125, avg. train loss: 82.50572\n",
      "Step #252600, epoch #63150, avg. train loss: 82.08710\n",
      "Step #252700, epoch #63175, avg. train loss: 82.37482\n",
      "Step #252800, epoch #63200, avg. train loss: 83.13435\n",
      "Step #252900, epoch #63225, avg. train loss: 81.74354\n",
      "Step #253000, epoch #63250, avg. train loss: 82.17506\n",
      "Step #253100, epoch #63275, avg. train loss: 82.08936\n",
      "Step #253200, epoch #63300, avg. train loss: 82.20477\n",
      "Step #253300, epoch #63325, avg. train loss: 82.11079\n",
      "Step #253400, epoch #63350, avg. train loss: 82.40248\n",
      "Step #253500, epoch #63375, avg. train loss: 82.32419\n",
      "Step #253600, epoch #63400, avg. train loss: 82.41164\n",
      "Step #253700, epoch #63425, avg. train loss: 82.24959\n",
      "Step #253800, epoch #63450, avg. train loss: 82.99786\n",
      "Step #253900, epoch #63475, avg. train loss: 82.58555\n",
      "Step #254000, epoch #63500, avg. train loss: 81.86864\n",
      "Step #254100, epoch #63525, avg. train loss: 82.35773\n",
      "Step #254200, epoch #63550, avg. train loss: 83.12300\n",
      "Step #254300, epoch #63575, avg. train loss: 82.48917\n",
      "Step #254400, epoch #63600, avg. train loss: 82.53916\n",
      "Step #254500, epoch #63625, avg. train loss: 81.46916\n",
      "Step #254600, epoch #63650, avg. train loss: 81.26129\n",
      "Step #254700, epoch #63675, avg. train loss: 82.75421\n",
      "Step #254800, epoch #63700, avg. train loss: 82.29095\n",
      "Step #254900, epoch #63725, avg. train loss: 80.94632\n",
      "Step #255000, epoch #63750, avg. train loss: 81.91120\n",
      "Step #255100, epoch #63775, avg. train loss: 82.57442\n",
      "Step #255200, epoch #63800, avg. train loss: 82.29221\n",
      "Step #255300, epoch #63825, avg. train loss: 81.93592\n",
      "Step #255400, epoch #63850, avg. train loss: 81.98528\n",
      "Step #255500, epoch #63875, avg. train loss: 82.92159\n",
      "Step #255600, epoch #63900, avg. train loss: 82.70925\n",
      "Step #255700, epoch #63925, avg. train loss: 82.16232\n",
      "Step #255800, epoch #63950, avg. train loss: 82.40221\n",
      "Step #255900, epoch #63975, avg. train loss: 82.14107\n",
      "Step #256000, epoch #64000, avg. train loss: 82.00631\n",
      "Step #256100, epoch #64025, avg. train loss: 81.20872\n",
      "Step #256200, epoch #64050, avg. train loss: 82.25173\n",
      "Step #256300, epoch #64075, avg. train loss: 82.55612\n",
      "Step #256400, epoch #64100, avg. train loss: 81.96515\n",
      "Step #256500, epoch #64125, avg. train loss: 81.73298\n",
      "Step #256600, epoch #64150, avg. train loss: 82.27358\n",
      "Step #256700, epoch #64175, avg. train loss: 82.36124\n",
      "Step #256800, epoch #64200, avg. train loss: 82.37164\n",
      "Step #256900, epoch #64225, avg. train loss: 82.49417\n",
      "Step #257000, epoch #64250, avg. train loss: 82.02493\n",
      "Step #257100, epoch #64275, avg. train loss: 82.48644\n",
      "Step #257200, epoch #64300, avg. train loss: 82.02608\n",
      "Step #257300, epoch #64325, avg. train loss: 81.81863\n",
      "Step #257400, epoch #64350, avg. train loss: 82.50435\n",
      "Step #257500, epoch #64375, avg. train loss: 82.83246\n",
      "Step #257600, epoch #64400, avg. train loss: 82.21931\n",
      "Step #257700, epoch #64425, avg. train loss: 82.91444\n",
      "Step #257800, epoch #64450, avg. train loss: 82.21777\n",
      "Step #257900, epoch #64475, avg. train loss: 82.39337\n",
      "Step #258000, epoch #64500, avg. train loss: 81.90401\n",
      "Step #258100, epoch #64525, avg. train loss: 81.90110\n",
      "Step #258200, epoch #64550, avg. train loss: 82.56420\n",
      "Step #258300, epoch #64575, avg. train loss: 82.40757\n",
      "Step #258400, epoch #64600, avg. train loss: 81.31699\n",
      "Step #258500, epoch #64625, avg. train loss: 81.67818\n",
      "Step #258600, epoch #64650, avg. train loss: 83.11191\n",
      "Step #258700, epoch #64675, avg. train loss: 81.16651\n",
      "Step #258800, epoch #64700, avg. train loss: 82.64124\n",
      "Step #258900, epoch #64725, avg. train loss: 82.46893\n",
      "Step #259000, epoch #64750, avg. train loss: 81.95493\n",
      "Step #259100, epoch #64775, avg. train loss: 82.50198\n",
      "Step #259200, epoch #64800, avg. train loss: 82.61909\n",
      "Step #259300, epoch #64825, avg. train loss: 82.94232\n",
      "Step #259400, epoch #64850, avg. train loss: 82.52654\n",
      "Step #259500, epoch #64875, avg. train loss: 81.95251\n",
      "Step #259600, epoch #64900, avg. train loss: 82.56873\n",
      "Step #259700, epoch #64925, avg. train loss: 83.15559\n",
      "Step #259800, epoch #64950, avg. train loss: 81.98939\n",
      "Step #259900, epoch #64975, avg. train loss: 81.87434\n",
      "Step #260000, epoch #65000, avg. train loss: 82.38922\n",
      "Step #260100, epoch #65025, avg. train loss: 81.81998\n",
      "Step #260200, epoch #65050, avg. train loss: 82.27215\n",
      "Step #260300, epoch #65075, avg. train loss: 81.94065\n",
      "Step #260400, epoch #65100, avg. train loss: 82.08088\n",
      "Step #260500, epoch #65125, avg. train loss: 81.20811\n",
      "Step #260600, epoch #65150, avg. train loss: 82.22189\n",
      "Step #260700, epoch #65175, avg. train loss: 82.05479\n",
      "Step #260800, epoch #65200, avg. train loss: 82.19077\n",
      "Step #260900, epoch #65225, avg. train loss: 82.53120\n",
      "Step #261000, epoch #65250, avg. train loss: 81.18662\n",
      "Step #261100, epoch #65275, avg. train loss: 82.86140\n",
      "Step #261200, epoch #65300, avg. train loss: 81.54154\n",
      "Step #261300, epoch #65325, avg. train loss: 82.10877\n",
      "Step #261400, epoch #65350, avg. train loss: 82.43694\n",
      "Step #261500, epoch #65375, avg. train loss: 83.14457\n",
      "Step #261600, epoch #65400, avg. train loss: 82.54987\n",
      "Step #261700, epoch #65425, avg. train loss: 82.95132\n",
      "Step #261800, epoch #65450, avg. train loss: 81.66777\n",
      "Step #261900, epoch #65475, avg. train loss: 82.77243\n",
      "Step #262000, epoch #65500, avg. train loss: 82.16838\n",
      "Step #262100, epoch #65525, avg. train loss: 81.93191\n",
      "Step #262200, epoch #65550, avg. train loss: 82.09336\n",
      "Step #262300, epoch #65575, avg. train loss: 82.56624\n",
      "Step #262400, epoch #65600, avg. train loss: 81.84484\n",
      "Step #262500, epoch #65625, avg. train loss: 81.83499\n",
      "Step #262600, epoch #65650, avg. train loss: 82.01649\n",
      "Step #262700, epoch #65675, avg. train loss: 81.28458\n",
      "Step #262800, epoch #65700, avg. train loss: 82.56687\n",
      "Step #262900, epoch #65725, avg. train loss: 81.79153\n",
      "Step #263000, epoch #65750, avg. train loss: 82.29591\n",
      "Step #263100, epoch #65775, avg. train loss: 82.43029\n",
      "Step #263200, epoch #65800, avg. train loss: 82.32770\n",
      "Step #263300, epoch #65825, avg. train loss: 81.68382\n",
      "Step #263400, epoch #65850, avg. train loss: 82.80196\n",
      "Step #263500, epoch #65875, avg. train loss: 82.23715\n",
      "Step #263600, epoch #65900, avg. train loss: 82.14065\n",
      "Step #263700, epoch #65925, avg. train loss: 81.93857\n",
      "Step #263800, epoch #65950, avg. train loss: 82.07204\n",
      "Step #263900, epoch #65975, avg. train loss: 81.82122\n",
      "Step #264000, epoch #66000, avg. train loss: 81.86722\n",
      "Step #264100, epoch #66025, avg. train loss: 82.77354\n",
      "Step #264200, epoch #66050, avg. train loss: 81.58903\n",
      "Step #264300, epoch #66075, avg. train loss: 81.31133\n",
      "Step #264400, epoch #66100, avg. train loss: 81.73965\n",
      "Step #264500, epoch #66125, avg. train loss: 81.38029\n",
      "Step #264600, epoch #66150, avg. train loss: 81.93443\n",
      "Step #264700, epoch #66175, avg. train loss: 82.31216\n",
      "Step #264800, epoch #66200, avg. train loss: 81.53168\n",
      "Step #264900, epoch #66225, avg. train loss: 81.80253\n",
      "Step #265000, epoch #66250, avg. train loss: 81.86572\n",
      "Step #265100, epoch #66275, avg. train loss: 82.43542\n",
      "Step #265200, epoch #66300, avg. train loss: 81.64083\n",
      "Step #265300, epoch #66325, avg. train loss: 82.07944\n",
      "Step #265400, epoch #66350, avg. train loss: 80.99364\n",
      "Step #265500, epoch #66375, avg. train loss: 81.90569\n",
      "Step #265600, epoch #66400, avg. train loss: 82.84404\n",
      "Step #265700, epoch #66425, avg. train loss: 81.66002\n",
      "Step #265800, epoch #66450, avg. train loss: 82.23382\n",
      "Step #265900, epoch #66475, avg. train loss: 82.25253\n",
      "Step #266000, epoch #66500, avg. train loss: 82.21822\n",
      "Step #266100, epoch #66525, avg. train loss: 81.75452\n",
      "Step #266200, epoch #66550, avg. train loss: 82.36592\n",
      "Step #266300, epoch #66575, avg. train loss: 83.14600\n",
      "Step #266400, epoch #66600, avg. train loss: 81.08994\n",
      "Step #266500, epoch #66625, avg. train loss: 81.58733\n",
      "Step #266600, epoch #66650, avg. train loss: 81.73804\n",
      "Step #266700, epoch #66675, avg. train loss: 81.31115\n",
      "Step #266800, epoch #66700, avg. train loss: 81.62773\n",
      "Step #266900, epoch #66725, avg. train loss: 81.84435\n",
      "Step #267000, epoch #66750, avg. train loss: 81.65279\n",
      "Step #267100, epoch #66775, avg. train loss: 82.30606\n",
      "Step #267200, epoch #66800, avg. train loss: 82.34306\n",
      "Step #267300, epoch #66825, avg. train loss: 82.00885\n",
      "Step #267400, epoch #66850, avg. train loss: 81.93051\n",
      "Step #267500, epoch #66875, avg. train loss: 81.79192\n",
      "Step #267600, epoch #66900, avg. train loss: 81.61398\n",
      "Step #267700, epoch #66925, avg. train loss: 82.83890\n",
      "Step #267800, epoch #66950, avg. train loss: 81.90533\n",
      "Step #267900, epoch #66975, avg. train loss: 83.22879\n",
      "Step #268000, epoch #67000, avg. train loss: 81.13183\n",
      "Step #268100, epoch #67025, avg. train loss: 81.98193\n",
      "Step #268200, epoch #67050, avg. train loss: 81.12276\n",
      "Step #268300, epoch #67075, avg. train loss: 81.99780\n",
      "Step #268400, epoch #67100, avg. train loss: 81.77334\n",
      "Step #268500, epoch #67125, avg. train loss: 81.71252\n",
      "Step #268600, epoch #67150, avg. train loss: 82.15616\n",
      "Step #268700, epoch #67175, avg. train loss: 82.01023\n",
      "Step #268800, epoch #67200, avg. train loss: 81.08804\n",
      "Step #268900, epoch #67225, avg. train loss: 81.21874\n",
      "Step #269000, epoch #67250, avg. train loss: 82.54973\n",
      "Step #269100, epoch #67275, avg. train loss: 82.64599\n",
      "Step #269200, epoch #67300, avg. train loss: 82.55855\n",
      "Step #269300, epoch #67325, avg. train loss: 82.37029\n",
      "Step #269400, epoch #67350, avg. train loss: 82.11535\n",
      "Step #269500, epoch #67375, avg. train loss: 82.89771\n",
      "Step #269600, epoch #67400, avg. train loss: 82.42919\n",
      "Step #269700, epoch #67425, avg. train loss: 82.43387\n",
      "Step #269800, epoch #67450, avg. train loss: 82.17104\n",
      "Step #269900, epoch #67475, avg. train loss: 81.56526\n",
      "Step #270000, epoch #67500, avg. train loss: 81.80659\n",
      "Step #270100, epoch #67525, avg. train loss: 83.26353\n",
      "Step #270200, epoch #67550, avg. train loss: 82.04314\n",
      "Step #270300, epoch #67575, avg. train loss: 81.02798\n",
      "Step #270400, epoch #67600, avg. train loss: 81.93792\n",
      "Step #270500, epoch #67625, avg. train loss: 82.39555\n",
      "Step #270600, epoch #67650, avg. train loss: 81.38804\n",
      "Step #270700, epoch #67675, avg. train loss: 81.81150\n",
      "Step #270800, epoch #67700, avg. train loss: 81.26375\n",
      "Step #270900, epoch #67725, avg. train loss: 82.54967\n",
      "Step #271000, epoch #67750, avg. train loss: 81.90171\n",
      "Step #271100, epoch #67775, avg. train loss: 82.47063\n",
      "Step #271200, epoch #67800, avg. train loss: 82.51246\n",
      "Step #271300, epoch #67825, avg. train loss: 81.71404\n",
      "Step #271400, epoch #67850, avg. train loss: 81.49636\n",
      "Step #271500, epoch #67875, avg. train loss: 82.46143\n",
      "Step #271600, epoch #67900, avg. train loss: 81.54623\n",
      "Step #271700, epoch #67925, avg. train loss: 82.47045\n",
      "Step #271800, epoch #67950, avg. train loss: 82.45451\n",
      "Step #271900, epoch #67975, avg. train loss: 81.90868\n",
      "Step #272000, epoch #68000, avg. train loss: 82.17854\n",
      "Step #272100, epoch #68025, avg. train loss: 82.47380\n",
      "Step #272200, epoch #68050, avg. train loss: 82.38525\n",
      "Step #272300, epoch #68075, avg. train loss: 81.93923\n",
      "Step #272400, epoch #68100, avg. train loss: 81.67176\n",
      "Step #272500, epoch #68125, avg. train loss: 81.18828\n",
      "Step #272600, epoch #68150, avg. train loss: 82.64725\n",
      "Step #272700, epoch #68175, avg. train loss: 81.13034\n",
      "Step #272800, epoch #68200, avg. train loss: 81.80586\n",
      "Step #272900, epoch #68225, avg. train loss: 81.03596\n",
      "Step #273000, epoch #68250, avg. train loss: 82.17799\n",
      "Step #273100, epoch #68275, avg. train loss: 82.02404\n",
      "Step #273200, epoch #68300, avg. train loss: 81.49068\n",
      "Step #273300, epoch #68325, avg. train loss: 81.68196\n",
      "Step #273400, epoch #68350, avg. train loss: 81.21694\n",
      "Step #273500, epoch #68375, avg. train loss: 81.59406\n",
      "Step #273600, epoch #68400, avg. train loss: 81.82175\n",
      "Step #273700, epoch #68425, avg. train loss: 81.79678\n",
      "Step #273800, epoch #68450, avg. train loss: 82.15856\n",
      "Step #273900, epoch #68475, avg. train loss: 81.45300\n",
      "Step #274000, epoch #68500, avg. train loss: 81.88877\n",
      "Step #274100, epoch #68525, avg. train loss: 81.97003\n",
      "Step #274200, epoch #68550, avg. train loss: 82.27679\n",
      "Step #274300, epoch #68575, avg. train loss: 81.39144\n",
      "Step #274400, epoch #68600, avg. train loss: 81.66765\n",
      "Step #274500, epoch #68625, avg. train loss: 82.20824\n",
      "Step #274600, epoch #68650, avg. train loss: 81.41795\n",
      "Step #274700, epoch #68675, avg. train loss: 82.19234\n",
      "Step #274800, epoch #68700, avg. train loss: 81.84662\n",
      "Step #274900, epoch #68725, avg. train loss: 82.11589\n",
      "Step #275000, epoch #68750, avg. train loss: 81.65294\n",
      "Step #275100, epoch #68775, avg. train loss: 82.14485\n",
      "Step #275200, epoch #68800, avg. train loss: 81.25518\n",
      "Step #275300, epoch #68825, avg. train loss: 81.47189\n",
      "Step #275400, epoch #68850, avg. train loss: 82.07719\n",
      "Step #275500, epoch #68875, avg. train loss: 81.39037\n",
      "Step #275600, epoch #68900, avg. train loss: 81.67063\n",
      "Step #275700, epoch #68925, avg. train loss: 82.12698\n",
      "Step #275800, epoch #68950, avg. train loss: 81.81198\n",
      "Step #275900, epoch #68975, avg. train loss: 81.41788\n",
      "Step #276000, epoch #69000, avg. train loss: 82.10626\n",
      "Step #276100, epoch #69025, avg. train loss: 82.30298\n",
      "Step #276200, epoch #69050, avg. train loss: 81.89595\n",
      "Step #276300, epoch #69075, avg. train loss: 81.78267\n",
      "Step #276400, epoch #69100, avg. train loss: 81.64162\n",
      "Step #276500, epoch #69125, avg. train loss: 81.43176\n",
      "Step #276600, epoch #69150, avg. train loss: 81.84300\n",
      "Step #276700, epoch #69175, avg. train loss: 82.23708\n",
      "Step #276800, epoch #69200, avg. train loss: 82.01919\n",
      "Step #276900, epoch #69225, avg. train loss: 81.30594\n",
      "Step #277000, epoch #69250, avg. train loss: 82.15001\n",
      "Step #277100, epoch #69275, avg. train loss: 81.23251\n",
      "Step #277200, epoch #69300, avg. train loss: 81.82265\n",
      "Step #277300, epoch #69325, avg. train loss: 81.55815\n",
      "Step #277400, epoch #69350, avg. train loss: 81.55673\n",
      "Step #277500, epoch #69375, avg. train loss: 81.71051\n",
      "Step #277600, epoch #69400, avg. train loss: 81.79729\n",
      "Step #277700, epoch #69425, avg. train loss: 82.26877\n",
      "Step #277800, epoch #69450, avg. train loss: 82.02151\n",
      "Step #277900, epoch #69475, avg. train loss: 81.63358\n",
      "Step #278000, epoch #69500, avg. train loss: 80.98139\n",
      "Step #278100, epoch #69525, avg. train loss: 81.60373\n",
      "Step #278200, epoch #69550, avg. train loss: 82.30008\n",
      "Step #278300, epoch #69575, avg. train loss: 81.68542\n",
      "Step #278400, epoch #69600, avg. train loss: 82.53496\n",
      "Step #278500, epoch #69625, avg. train loss: 80.69105\n",
      "Step #278600, epoch #69650, avg. train loss: 81.12638\n",
      "Step #278700, epoch #69675, avg. train loss: 82.15283\n",
      "Step #278800, epoch #69700, avg. train loss: 82.10049\n",
      "Step #278900, epoch #69725, avg. train loss: 81.26276\n",
      "Step #279000, epoch #69750, avg. train loss: 81.60016\n",
      "Step #279100, epoch #69775, avg. train loss: 82.07684\n",
      "Step #279200, epoch #69800, avg. train loss: 81.50958\n",
      "Step #279300, epoch #69825, avg. train loss: 81.85365\n",
      "Step #279400, epoch #69850, avg. train loss: 82.10403\n",
      "Step #279500, epoch #69875, avg. train loss: 82.12465\n",
      "Step #279600, epoch #69900, avg. train loss: 81.27041\n",
      "Step #279700, epoch #69925, avg. train loss: 82.00562\n",
      "Step #279800, epoch #69950, avg. train loss: 81.22454\n",
      "Step #279900, epoch #69975, avg. train loss: 82.89470\n",
      "Step #280000, epoch #70000, avg. train loss: 81.53514\n",
      "Step #280100, epoch #70025, avg. train loss: 82.19852\n",
      "Step #280200, epoch #70050, avg. train loss: 80.85176\n",
      "Step #280300, epoch #70075, avg. train loss: 82.62755\n",
      "Step #280400, epoch #70100, avg. train loss: 82.04115\n",
      "Step #280500, epoch #70125, avg. train loss: 81.58710\n",
      "Step #280600, epoch #70150, avg. train loss: 81.12532\n",
      "Step #280700, epoch #70175, avg. train loss: 81.93462\n",
      "Step #280800, epoch #70200, avg. train loss: 81.81087\n",
      "Step #280900, epoch #70225, avg. train loss: 82.36922\n",
      "Step #281000, epoch #70250, avg. train loss: 81.37533\n",
      "Step #281100, epoch #70275, avg. train loss: 81.61949\n",
      "Step #281200, epoch #70300, avg. train loss: 81.17889\n",
      "Step #281300, epoch #70325, avg. train loss: 81.60512\n",
      "Step #281400, epoch #70350, avg. train loss: 81.63720\n",
      "Step #281500, epoch #70375, avg. train loss: 81.17049\n",
      "Step #281600, epoch #70400, avg. train loss: 81.77967\n",
      "Step #281700, epoch #70425, avg. train loss: 82.04213\n",
      "Step #281800, epoch #70450, avg. train loss: 81.43172\n",
      "Step #281900, epoch #70475, avg. train loss: 82.43198\n",
      "Step #282000, epoch #70500, avg. train loss: 82.38869\n",
      "Step #282100, epoch #70525, avg. train loss: 81.98521\n",
      "Step #282200, epoch #70550, avg. train loss: 81.38123\n",
      "Step #282300, epoch #70575, avg. train loss: 80.85239\n",
      "Step #282400, epoch #70600, avg. train loss: 81.21066\n",
      "Step #282500, epoch #70625, avg. train loss: 81.80466\n",
      "Step #282600, epoch #70650, avg. train loss: 82.67159\n",
      "Step #282700, epoch #70675, avg. train loss: 82.02827\n",
      "Step #282800, epoch #70700, avg. train loss: 81.26378\n",
      "Step #282900, epoch #70725, avg. train loss: 82.71038\n",
      "Step #283000, epoch #70750, avg. train loss: 81.59486\n",
      "Step #283100, epoch #70775, avg. train loss: 81.76849\n",
      "Step #283200, epoch #70800, avg. train loss: 82.14378\n",
      "Step #283300, epoch #70825, avg. train loss: 82.62033\n",
      "Step #283400, epoch #70850, avg. train loss: 82.30606\n",
      "Step #283500, epoch #70875, avg. train loss: 81.61864\n",
      "Step #283600, epoch #70900, avg. train loss: 81.31012\n",
      "Step #283700, epoch #70925, avg. train loss: 80.78456\n",
      "Step #283800, epoch #70950, avg. train loss: 81.80013\n",
      "Step #283900, epoch #70975, avg. train loss: 81.66592\n",
      "Step #284000, epoch #71000, avg. train loss: 81.92832\n",
      "Step #284100, epoch #71025, avg. train loss: 81.93049\n",
      "Step #284200, epoch #71050, avg. train loss: 80.67950\n",
      "Step #284300, epoch #71075, avg. train loss: 81.72797\n",
      "Step #284400, epoch #71100, avg. train loss: 81.59771\n",
      "Step #284500, epoch #71125, avg. train loss: 82.39017\n",
      "Step #284600, epoch #71150, avg. train loss: 81.90173\n",
      "Step #284700, epoch #71175, avg. train loss: 81.84174\n",
      "Step #284800, epoch #71200, avg. train loss: 81.33012\n",
      "Step #284900, epoch #71225, avg. train loss: 81.41165\n",
      "Step #285000, epoch #71250, avg. train loss: 81.50693\n",
      "Step #285100, epoch #71275, avg. train loss: 81.50283\n",
      "Step #285200, epoch #71300, avg. train loss: 81.36946\n",
      "Step #285300, epoch #71325, avg. train loss: 81.25797\n",
      "Step #285400, epoch #71350, avg. train loss: 81.79852\n",
      "Step #285500, epoch #71375, avg. train loss: 81.30240\n",
      "Step #285600, epoch #71400, avg. train loss: 80.84845\n",
      "Step #285700, epoch #71425, avg. train loss: 80.47937\n",
      "Step #285800, epoch #71450, avg. train loss: 81.03480\n",
      "Step #285900, epoch #71475, avg. train loss: 81.99895\n",
      "Step #286000, epoch #71500, avg. train loss: 82.03474\n",
      "Step #286100, epoch #71525, avg. train loss: 81.27034\n",
      "Step #286200, epoch #71550, avg. train loss: 80.98344\n",
      "Step #286300, epoch #71575, avg. train loss: 81.28817\n",
      "Step #286400, epoch #71600, avg. train loss: 81.97330\n",
      "Step #286500, epoch #71625, avg. train loss: 81.97269\n",
      "Step #286600, epoch #71650, avg. train loss: 82.51458\n",
      "Step #286700, epoch #71675, avg. train loss: 81.36862\n",
      "Step #286800, epoch #71700, avg. train loss: 81.67069\n",
      "Step #286900, epoch #71725, avg. train loss: 82.52720\n",
      "Step #287000, epoch #71750, avg. train loss: 82.26438\n",
      "Step #287100, epoch #71775, avg. train loss: 81.27852\n",
      "Step #287200, epoch #71800, avg. train loss: 81.61704\n",
      "Step #287300, epoch #71825, avg. train loss: 81.70538\n",
      "Step #287400, epoch #71850, avg. train loss: 81.34357\n",
      "Step #287500, epoch #71875, avg. train loss: 81.40466\n",
      "Step #287600, epoch #71900, avg. train loss: 80.15518\n",
      "Step #287700, epoch #71925, avg. train loss: 81.62904\n",
      "Step #287800, epoch #71950, avg. train loss: 81.17326\n",
      "Step #287900, epoch #71975, avg. train loss: 82.38081\n",
      "Step #288000, epoch #72000, avg. train loss: 80.94836\n",
      "Step #288100, epoch #72025, avg. train loss: 81.81842\n",
      "Step #288200, epoch #72050, avg. train loss: 80.88467\n",
      "Step #288300, epoch #72075, avg. train loss: 81.08394\n",
      "Step #288400, epoch #72100, avg. train loss: 81.72232\n",
      "Step #288500, epoch #72125, avg. train loss: 82.01228\n",
      "Step #288600, epoch #72150, avg. train loss: 81.73251\n",
      "Step #288700, epoch #72175, avg. train loss: 81.05722\n",
      "Step #288800, epoch #72200, avg. train loss: 81.56157\n",
      "Step #288900, epoch #72225, avg. train loss: 81.01221\n",
      "Step #289000, epoch #72250, avg. train loss: 82.80531\n",
      "Step #289100, epoch #72275, avg. train loss: 80.69082\n",
      "Step #289200, epoch #72300, avg. train loss: 82.47623\n",
      "Step #289300, epoch #72325, avg. train loss: 82.32475\n",
      "Step #289400, epoch #72350, avg. train loss: 81.21767\n",
      "Step #289500, epoch #72375, avg. train loss: 81.44760\n",
      "Step #289600, epoch #72400, avg. train loss: 82.35281\n",
      "Step #289700, epoch #72425, avg. train loss: 81.39069\n",
      "Step #289800, epoch #72450, avg. train loss: 82.00625\n",
      "Step #289900, epoch #72475, avg. train loss: 80.89693\n",
      "Step #290000, epoch #72500, avg. train loss: 81.83661\n",
      "Step #290100, epoch #72525, avg. train loss: 81.51474\n",
      "Step #290200, epoch #72550, avg. train loss: 82.18083\n",
      "Step #290300, epoch #72575, avg. train loss: 82.34825\n",
      "Step #290400, epoch #72600, avg. train loss: 81.77873\n",
      "Step #290500, epoch #72625, avg. train loss: 81.14194\n",
      "Step #290600, epoch #72650, avg. train loss: 81.32104\n",
      "Step #290700, epoch #72675, avg. train loss: 82.11348\n",
      "Step #290800, epoch #72700, avg. train loss: 81.70956\n",
      "Step #290900, epoch #72725, avg. train loss: 82.00130\n",
      "Step #291000, epoch #72750, avg. train loss: 81.33356\n",
      "Step #291100, epoch #72775, avg. train loss: 81.76476\n",
      "Step #291200, epoch #72800, avg. train loss: 81.42694\n",
      "Step #291300, epoch #72825, avg. train loss: 80.97521\n",
      "Step #291400, epoch #72850, avg. train loss: 81.87283\n",
      "Step #291500, epoch #72875, avg. train loss: 81.73791\n",
      "Step #291600, epoch #72900, avg. train loss: 81.99302\n",
      "Step #291700, epoch #72925, avg. train loss: 81.66567\n",
      "Step #291800, epoch #72950, avg. train loss: 80.82127\n",
      "Step #291900, epoch #72975, avg. train loss: 82.05479\n",
      "Step #292000, epoch #73000, avg. train loss: 81.08567\n",
      "Step #292100, epoch #73025, avg. train loss: 82.45763\n",
      "Step #292200, epoch #73050, avg. train loss: 82.15660\n",
      "Step #292300, epoch #73075, avg. train loss: 81.32809\n",
      "Step #292400, epoch #73100, avg. train loss: 81.24462\n",
      "Step #292500, epoch #73125, avg. train loss: 81.66970\n",
      "Step #292600, epoch #73150, avg. train loss: 80.80997\n",
      "Step #292700, epoch #73175, avg. train loss: 81.21153\n",
      "Step #292800, epoch #73200, avg. train loss: 81.65306\n",
      "Step #292900, epoch #73225, avg. train loss: 81.50278\n",
      "Step #293000, epoch #73250, avg. train loss: 81.57870\n",
      "Step #293100, epoch #73275, avg. train loss: 82.02841\n",
      "Step #293200, epoch #73300, avg. train loss: 81.40379\n",
      "Step #293300, epoch #73325, avg. train loss: 82.24272\n",
      "Step #293400, epoch #73350, avg. train loss: 81.28915\n",
      "Step #293500, epoch #73375, avg. train loss: 80.22737\n",
      "Step #293600, epoch #73400, avg. train loss: 81.05521\n",
      "Step #293700, epoch #73425, avg. train loss: 81.20222\n",
      "Step #293800, epoch #73450, avg. train loss: 80.90606\n",
      "Step #293900, epoch #73475, avg. train loss: 81.34643\n",
      "Step #294000, epoch #73500, avg. train loss: 80.89881\n",
      "Step #294100, epoch #73525, avg. train loss: 81.75021\n",
      "Step #294200, epoch #73550, avg. train loss: 80.65664\n",
      "Step #294300, epoch #73575, avg. train loss: 81.48456\n",
      "Step #294400, epoch #73600, avg. train loss: 81.33163\n",
      "Step #294500, epoch #73625, avg. train loss: 81.31096\n",
      "Step #294600, epoch #73650, avg. train loss: 81.56212\n",
      "Step #294700, epoch #73675, avg. train loss: 81.35194\n",
      "Step #294800, epoch #73700, avg. train loss: 80.82258\n",
      "Step #294900, epoch #73725, avg. train loss: 82.12643\n",
      "Step #295000, epoch #73750, avg. train loss: 81.23450\n",
      "Step #295100, epoch #73775, avg. train loss: 81.98355\n",
      "Step #295200, epoch #73800, avg. train loss: 81.79528\n",
      "Step #295300, epoch #73825, avg. train loss: 81.16335\n",
      "Step #295400, epoch #73850, avg. train loss: 80.74354\n",
      "Step #295500, epoch #73875, avg. train loss: 80.64295\n",
      "Step #295600, epoch #73900, avg. train loss: 81.88613\n",
      "Step #295700, epoch #73925, avg. train loss: 82.37402\n",
      "Step #295800, epoch #73950, avg. train loss: 80.74409\n",
      "Step #295900, epoch #73975, avg. train loss: 81.30775\n",
      "Step #296000, epoch #74000, avg. train loss: 81.56546\n",
      "Step #296100, epoch #74025, avg. train loss: 81.81644\n",
      "Step #296200, epoch #74050, avg. train loss: 81.40825\n",
      "Step #296300, epoch #74075, avg. train loss: 81.57802\n",
      "Step #296400, epoch #74100, avg. train loss: 82.56271\n",
      "Step #296500, epoch #74125, avg. train loss: 81.06610\n",
      "Step #296600, epoch #74150, avg. train loss: 80.87451\n",
      "Step #296700, epoch #74175, avg. train loss: 81.45281\n",
      "Step #296800, epoch #74200, avg. train loss: 81.14645\n",
      "Step #296900, epoch #74225, avg. train loss: 80.86128\n",
      "Step #297000, epoch #74250, avg. train loss: 81.09090\n",
      "Step #297100, epoch #74275, avg. train loss: 80.53520\n",
      "Step #297200, epoch #74300, avg. train loss: 82.57864\n",
      "Step #297300, epoch #74325, avg. train loss: 81.52689\n",
      "Step #297400, epoch #74350, avg. train loss: 81.38336\n",
      "Step #297500, epoch #74375, avg. train loss: 81.40083\n",
      "Step #297600, epoch #74400, avg. train loss: 81.31266\n",
      "Step #297700, epoch #74425, avg. train loss: 81.35596\n",
      "Step #297800, epoch #74450, avg. train loss: 82.34517\n",
      "Step #297900, epoch #74475, avg. train loss: 81.38045\n",
      "Step #298000, epoch #74500, avg. train loss: 81.12930\n",
      "Step #298100, epoch #74525, avg. train loss: 81.71455\n",
      "Step #298200, epoch #74550, avg. train loss: 82.79408\n",
      "Step #298300, epoch #74575, avg. train loss: 81.46931\n",
      "Step #298400, epoch #74600, avg. train loss: 81.60206\n",
      "Step #298500, epoch #74625, avg. train loss: 81.59718\n",
      "Step #298600, epoch #74650, avg. train loss: 81.57714\n",
      "Step #298700, epoch #74675, avg. train loss: 81.42300\n",
      "Step #298800, epoch #74700, avg. train loss: 81.41348\n",
      "Step #298900, epoch #74725, avg. train loss: 81.90736\n",
      "Step #299000, epoch #74750, avg. train loss: 81.02139\n",
      "Step #299100, epoch #74775, avg. train loss: 81.17518\n",
      "Step #299200, epoch #74800, avg. train loss: 81.22745\n",
      "Step #299300, epoch #74825, avg. train loss: 80.42084\n",
      "Step #299400, epoch #74850, avg. train loss: 81.59566\n",
      "Step #299500, epoch #74875, avg. train loss: 81.06145\n",
      "Step #299600, epoch #74900, avg. train loss: 81.09778\n",
      "Step #299700, epoch #74925, avg. train loss: 80.85955\n",
      "Step #299800, epoch #74950, avg. train loss: 81.42062\n",
      "Step #299900, epoch #74975, avg. train loss: 82.39983\n",
      "Step #300000, epoch #75000, avg. train loss: 81.06033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNRegressor(batch_size=32, clip_gradients=5.0, config=None,\n",
       "            continue_training=False, dropout=None,\n",
       "            hidden_units=[85, 40, 20, 10], learning_rate=0.01, n_classes=0,\n",
       "            optimizer='Adagrad', steps=300000, verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit/train neural network\n",
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 148.8486328125\n"
     ]
    }
   ],
   "source": [
    "#Measure accuracy\n",
    "score = np.sqrt(metrics.mean_squared_error(regressor.predict(x_test), y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254.960403</td>\n",
       "      <td>246.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231.180801</td>\n",
       "      <td>223.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229.887314</td>\n",
       "      <td>293.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183.837448</td>\n",
       "      <td>235.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195.853485</td>\n",
       "      <td>314.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>309.261749</td>\n",
       "      <td>214.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>516.272095</td>\n",
       "      <td>198.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>229.286972</td>\n",
       "      <td>221.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>170.185654</td>\n",
       "      <td>219.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>234.212799</td>\n",
       "      <td>289.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>309.967682</td>\n",
       "      <td>237.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>445.900421</td>\n",
       "      <td>338.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>219.405411</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>172.927505</td>\n",
       "      <td>199.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>209.062073</td>\n",
       "      <td>182.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>194.343246</td>\n",
       "      <td>276.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>202.361160</td>\n",
       "      <td>248.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>278.455994</td>\n",
       "      <td>249.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>214.124252</td>\n",
       "      <td>182.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>118.299248</td>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>321.295166</td>\n",
       "      <td>263.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>179.103043</td>\n",
       "      <td>272.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>253.123367</td>\n",
       "      <td>244.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>226.671173</td>\n",
       "      <td>145.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>652.219055</td>\n",
       "      <td>236.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>193.852951</td>\n",
       "      <td>250.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>834.208740</td>\n",
       "      <td>197.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>157.816330</td>\n",
       "      <td>190.800003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>366.878021</td>\n",
       "      <td>206.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>194.218948</td>\n",
       "      <td>238.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>228.004303</td>\n",
       "      <td>234.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>195.569427</td>\n",
       "      <td>215.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>264.847412</td>\n",
       "      <td>183.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>203.223770</td>\n",
       "      <td>195.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>445.268951</td>\n",
       "      <td>174.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>198.955872</td>\n",
       "      <td>259.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>226.144150</td>\n",
       "      <td>202.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>421.919556</td>\n",
       "      <td>291.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>193.219376</td>\n",
       "      <td>158.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>205.446655</td>\n",
       "      <td>165.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>250.437561</td>\n",
       "      <td>300.200012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted      Actual\n",
       "0   254.960403  246.399994\n",
       "1   231.180801  223.199997\n",
       "2   229.887314  293.000000\n",
       "3   183.837448  235.899994\n",
       "4   195.853485  314.899994\n",
       "5   309.261749  214.300003\n",
       "6   516.272095  198.800003\n",
       "7   229.286972  221.899994\n",
       "8   170.185654  219.100006\n",
       "9   234.212799  289.200012\n",
       "10  309.967682  237.899994\n",
       "11  445.900421  338.899994\n",
       "12  219.405411  250.000000\n",
       "13  172.927505  199.899994\n",
       "14  209.062073  182.300003\n",
       "15  194.343246  276.799988\n",
       "16  202.361160  248.199997\n",
       "17  278.455994  249.300003\n",
       "18  214.124252  182.600006\n",
       "19  118.299248  198.000000\n",
       "20  321.295166  263.600006\n",
       "21  179.103043  272.399994\n",
       "22  253.123367  244.699997\n",
       "23  226.671173  145.100006\n",
       "24  652.219055  236.300003\n",
       "25  193.852951  250.699997\n",
       "26  834.208740  197.699997\n",
       "27  157.816330  190.800003\n",
       "28  366.878021  206.199997\n",
       "29  194.218948  238.300003\n",
       "30  228.004303  234.699997\n",
       "31  195.569427  215.300003\n",
       "32  264.847412  183.899994\n",
       "33  203.223770  195.500000\n",
       "34  445.268951  174.199997\n",
       "35  198.955872  259.700012\n",
       "36  226.144150  202.899994\n",
       "37  421.919556  291.500000\n",
       "38  193.219376  158.300003\n",
       "39  205.446655  165.899994\n",
       "40  250.437561  300.200012"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make predictions \n",
    "pred = regressor.predict(x_test)\n",
    "predDF = pd.DataFrame(pred)\n",
    "dfytest=pd.DataFrame(y_test)\n",
    "dfytest.reset_index(inplace=True, drop=True)\n",
    "predDF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df2 = pd.concat([predDF, dfytest], axis=1,ignore_index=True)\n",
    "df2.columns=['Predicted', 'Actual']\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
