{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37664 entries, 132975 to 132698\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 38.2+ MB\n",
      "None\n",
      "Skew in numeric features:\n",
      "id        0.001300\n",
      "cont1     0.510914\n",
      "cont2    -0.297998\n",
      "cont3    -0.005130\n",
      "cont4     0.419437\n",
      "cont5     0.683353\n",
      "cont6     0.467183\n",
      "cont7     0.816377\n",
      "cont8     0.673707\n",
      "cont9     1.068756\n",
      "cont10    0.355044\n",
      "cont11    0.288620\n",
      "cont12    0.300982\n",
      "cont13    0.380568\n",
      "cont14    0.249562\n",
      "loss      3.360506\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.39it/s]\n",
      "100%|██████████| 116/116 [00:07<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
      "\n",
      "Will train until test-mae hasn't improved in 50 rounds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baysian hyperparameter optimization [https://github.com/fmfn/BayesianOptimization]\n",
    "for Mean Absoulte Error objective\n",
    "on default features for https://www.kaggle.com/c/allstate-claims-severity\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Vladimir Iglovikov\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             feval=evalerror,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    path = \"./data/allstate\"\n",
    "    inputFilePath = os.path.join(path, \"train.csv.zip\")\n",
    "    train = pd.read_csv(inputFilePath, compression=\"zip\", header=0, na_values=['NULL'])\n",
    "    train = train.sample(frac=0.2)\n",
    "\n",
    "    y = np.log(train['loss'] + shift)\n",
    "\n",
    "    print(train.info())\n",
    "    numerical_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "    # compute skew and do Box-Cox transformation\n",
    "    skewed_feats = train[numerical_feats].apply(lambda x: skew(x.dropna()))\n",
    "    print('Skew in numeric features:')\n",
    "    print(skewed_feats)\n",
    "    # transform features with skew > 0.25 (this can be varied to find optimal value)\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    for feats in tqdm(skewed_feats):\n",
    "        train[feats] = train[feats] + 1\n",
    "        train[feats], lam = boxcox(train[feats])\n",
    "\n",
    "    categorical_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for column in tqdm(categorical_columns):\n",
    "        le = LabelEncoder()\n",
    "        train[column] = le.fit_transform(train[column])\n",
    "\n",
    "    X = train.drop(['loss', 'id'], 1)\n",
    "    xgtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "    return xgtrain\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_rounds = 100000\n",
    "    random_state = 2016\n",
    "    num_iter = 250\n",
    "    init_points = 5\n",
    "    shift = 200\n",
    "\n",
    "    xgtrain = prepare_data()\n",
    "\n",
    "    params = {\n",
    "        'eta': 0.1,\n",
    "        'silent': 1,\n",
    "        'eval_metric': 'mae',\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state\n",
    "    }\n",
    "\n",
    "    xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                                'colsample_bytree': (0, 0.7),\n",
    "                                                'max_depth': (5, 15),\n",
    "                                                'subsample': (0.3, 1),\n",
    "                                                'gamma': (0, 10),\n",
    "                                                'alpha': (0, 10),\n",
    "                                                })\n",
    "\n",
    "    xgbBO.maximize(init_points=init_points, n_iter=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
